{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1cb0d77",
   "metadata": {},
   "source": [
    "# Streaming Data From A File To Power BI Streaming Dataset Using Python\n",
    "> Create streaming dataset based on an existing file using Python. Super handy for POCs.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [powerbi, pandas, python, api, streaming, dataset]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9368d9e8",
   "metadata": {},
   "source": [
    "## Streaming Dataset\n",
    "\n",
    "It's easy to create a streaming dataset in Power BI for real-time applications. You do not need a premium capacity. To keep the blog short, I am going to assume you already know what is a streaming dataset and how it differs from import/DQ. If you are not familiar, [this documentation](https://learn.microsoft.com/en-us/power-bi/connect-data/service-real-time-streaming) will help. \n",
    "\n",
    "Imagine you are creating a POC and would like to develop the real-time dashboard, report or just want to show to your stakeholders what the solution would loook like, you will need data to stream to Power BI service. You can create Azure Event Hub, Streaming Analytics. But in this blog, I will show you how you can stream data from an existing file. I will also include a link below in the resources for a few sample datasets you can use depending on the use case. \n",
    "\n",
    "You will need a Python IDE with pandas installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc23793",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/pawarbi/blog/master/images/msedge_umtOfdekx5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21956d42",
   "metadata": {},
   "source": [
    "##### Create A Streaming dataset\n",
    "\n",
    "First create a streaming dataset in Power BI service using the API method.\n",
    "![](https://raw.githubusercontent.com/pawarbi/blog/master/images/KP9NMM8b4g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c79dc",
   "metadata": {},
   "source": [
    "I will be using [this dataset](https://github.com/reisanar/datasets/blob/master/eBayAuctions.csv) for my example. Note the columns and column types from your data and create the dataset in Power BI service accordingly. \n",
    "![](https://raw.githubusercontent.com/pawarbi/blog/master/images/msedge_dyzzbjo5kF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5765d2",
   "metadata": {},
   "source": [
    "Note the API endpoint:\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/pawarbi/blog/master/images/msedge_imlnDaqdcW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d4587",
   "metadata": {},
   "source": [
    "That's it. We are now ready to stream the data to Power BI service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73306e9d",
   "metadata": {},
   "source": [
    "## Python Script\n",
    "\n",
    "I will import the data using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d0dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20cb0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "\n",
    "\n",
    "class StreamToPowerBI:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Class to stream data to Power BI Streaming dataset\n",
    "    --------------------------------------------------\n",
    "    Example:\n",
    "\n",
    "     - endpoint :  API ednpoint, e.g. \"https://api.powerbi.com/beta/<ws_id>/datasets/<dataset_id>/rows?key=<___key___>\"\n",
    "     - data     :  Data in dictionary e.g. {\"id\":1, \"Value\":2.33, \"Color\":\"Red\"} \n",
    "                   Number of columns, column names and column type must match with Power BI.\n",
    "     - delay    :  Delay in seconds in sending the data to Power BI. e.g. if delay is 3, data will be sent every 3 seconds.\n",
    "                   default value is 2 seconds. To overwrite, use the wait() method\n",
    "                   \n",
    "                   \n",
    "     example:      To stream data to the specified endpoint with 5 s delay\n",
    "                   StreamToPowerBI(endpoint,data).wait(5).post_data()              \n",
    " \n",
    "    \"\"\"\n",
    "    endpoint: str\n",
    "    data: Dict\n",
    "    delay: int=2   \n",
    "        \n",
    "    def wait(self, other=None):\n",
    "        ''' Delay in seconds to stream the data '''\n",
    "        if other==None:\n",
    "            # default value is 2 seconds\n",
    "            other = 2 \n",
    "        sleep(other)    \n",
    "        return self\n",
    "    \n",
    "    def post_data(self):\n",
    "        ''' Use post method send the data to Power BI'''\n",
    "        if isinstance(self.data, dict):\n",
    "            endpoint  = self.endpoint\n",
    "            payload = [self.data]\n",
    "\n",
    "            x = requests.post(endpoint, json = payload)\n",
    "            if x.status_code==200:\n",
    "                x=x\n",
    "            else:\n",
    "                print(\"Failed ! Check endpoint, data\")\n",
    "        else:\n",
    "            print(\"Data passed is not a dictionary. It should be in the format {}\".format('{}'))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35762d76",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "Import the data using pandas or any other library (spark, polars etc.). Be sure to make the numeric columns type `float` otherwise json serialization will fail. I am also removing null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf33537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.read_csv(\"https://raw.githubusercontent.com/reisanar/datasets/master/eBayAuctions.csv\", \n",
    "                  index_col=None, \n",
    "                  dtype={'Category':str,\n",
    "                         'currency':str,\n",
    "                         'sellerRating':float,\n",
    "                         'Duration':float,\n",
    "                         'ClosePrice':float,\n",
    "                         'OpenPrice':float,\n",
    "                         'Competitive?':float},\n",
    "                  usecols=['Category','currency','sellerRating','Duration','ClosePrice','OpenPrice','Competitive?'])\n",
    "     ).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae6558",
   "metadata": {},
   "source": [
    "#### Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd160c",
   "metadata": {},
   "source": [
    "Before streaming data from the whole file, let's first test using one data point. Note that in the orginal data, there is no timestamp, so I am adding the timestamp , i.e `DateTime` column, to the dictionary. In your case, you may not want to do that. comment it out if you want to stream the data as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509b9b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Category': 'Music/Movie/Game',\n",
       " 'currency': 'US',\n",
       " 'sellerRating': 3249.0,\n",
       " 'Duration': 5.0,\n",
       " 'ClosePrice': 0.01,\n",
       " 'OpenPrice': 0.01,\n",
       " 'Competitive?': 0.0,\n",
       " 'DateTime': '2023-01-13T13:44:05.740998'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict(df.iloc[0])\n",
    "d['DateTime']=datetime.now().isoformat()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e28067bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint = \"https://api.powerbi.com/beta/xxxxxxx/datasets/xxxxxxxxxx\"\n",
    "StreamToPowerBI(endpoint,d).wait(1).post_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8538a4",
   "metadata": {},
   "source": [
    ">note: Note that the API endpoint Power BI provides includes the key so never share the endpoint with anyone. If you do, they will be able to stream the data without any authentication !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcda8f",
   "metadata": {},
   "source": [
    "If the reponse returned is `<Response [200]>`, it was successful. We can now stream more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21b905",
   "metadata": {},
   "source": [
    "## Streaming Rows\n",
    "\n",
    "To stream the rows, we will just iterate over the dataframe and send one row at a time with the specified delay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c90ef0",
   "metadata": {},
   "source": [
    "#### To stream the entire file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c437a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in range(0,len(df)):\n",
    "    data = dict(df.iloc[record])\n",
    "    \n",
    "    #including datetime is optional. If you do want it, make sure it's included in the streaming dataset defintion in Power BI service\n",
    "    data['DateTime']=datetime.now().isoformat()\n",
    "    \n",
    "    StreamToPowerBI(endpoint,data).wait(1).post_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd380d6",
   "metadata": {},
   "source": [
    "### To randomly stream x% of the data\n",
    "\n",
    "Remember the purpose here is POC so don't stream all the rows (read the documentation for limtations, especially the limitations on number of datapoints, no of requests etc.). You can use below to randomly sample x % of the rows from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = 0.1\n",
    "\n",
    "for record in range(0,len(df.sample(frac=x))):\n",
    "    data = dict(df.iloc[record])\n",
    "    \n",
    "    #including datetime is optional. If you do want it, make sure it's included in the streaming dataset defintion in Power BI service\n",
    "    data['DateTime']=datetime.now().isoformat()\n",
    "    \n",
    "    StreamToPowerBI(endpoint,data).wait(1).post_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de7d37",
   "metadata": {},
   "source": [
    "### Resources \n",
    "\n",
    "1. Documentation : https://learn.microsoft.com/en-us/power-bi/connect-data/service-real-time-streaming\n",
    "2. Example datasets: https://github.com/reisanar/datasets\n",
    "3. Azure Open Datasets : https://github.com/Azure/OpenDatasetsNotebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fa59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
