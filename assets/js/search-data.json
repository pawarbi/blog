{
  
    
        "post0": {
            "title": "Time series Forecasting in Python & R, Part 2 (Forecasting )",
            "content": "Overview . In Part 1, I did exploratory data analysis of sales time series of a French retailer. In this blog I will apply various time series models in Python and R to forecast sales for the next 4 quarters. The forecasting models I will cover are: . Seasomal Naive | Holt Winter&#39;s (i.e Triple Exponential Smoothing), ETS | SARIMA | Facebook Prohet | . For each of these models, I will provide a short description for intuitive understanding of these models and give refrences for more academic explanation. For any forecasting model, the general steps are as below. . Forecasting Steps . EDA | Forecast on test set | Evaluate the forecast Use appropriate evaluation metric (%MAPE, RMSE, AIC) | Plot the forecast against train and test data set | . | Check residuals. . Plot residuals, plot ACF/PACF and Q/Q plots | Conditions A, B below are essential and C,D are useful. Residuals should be: . Uncorrelated | Have zero (or close to zero) mean | Constant variance | Normally distributed | | First two ensure that there is no more information that can be extracted from the data, while the bottom two keep the variability in the point forecast narrow . | . | Select model(s) Forecast future series | Prediction Interval | . | Evaluation Metric . We evaluate the forecasting model by comparing the fitted &amp; predicted values against the actual values in training and test sets. Note that residuals are the difference between training data and fitted values, while forecast error is the difference between test data and predicted values. We use residuals to check performance of the model while errors for checking accuracy/uncertanty of the future forecast. . As a general rule, if the data has no outliers RMSE is a good metric to use. %MAPE provides a more inutitive understanding as it is expressed in percentage. We do not use %MAPE if the series is intermittent to avoid division by zero. . . Importing libraries . #collapse-hide #Author: Sandeep Pawar #Version: 1.0 #Date Mar 27, 2020 import pandas as pd import numpy as np import itertools #Plotting libraries import matplotlib.pyplot as plt import seaborn as sns import altair as alt plt.style.use(&#39;seaborn-white&#39;) pd.plotting.register_matplotlib_converters() %matplotlib inline #statistics libraries import statsmodels.api as sm import scipy from scipy.stats import anderson from statsmodels.tools.eval_measures import rmse from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import month_plot, seasonal_plot, plot_acf, plot_pacf, quarter_plot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing from statsmodels.stats.diagnostic import acorr_ljungbox as ljung from statsmodels.tsa.statespace.tools import diff as diff from statsmodels.tsa.statespace.sarimax import SARIMAX import pmdarima as pm from pmdarima import ARIMA, auto_arima from scipy import signal from scipy.stats import shapiro from scipy.stats import boxcox from scipy.special import inv_boxcox from sklearn.preprocessing import StandardScaler from scipy.stats import jarque_bera as jb import fbprophet as Prophet #library to use R in Python import rpy2 from rpy2.robjects import pandas2ri pandas2ri.activate() import warnings warnings.filterwarnings(&quot;ignore&quot;) np.random.seed(786) . . Library versions . #Printing library versions print(&#39;Pandas:&#39;, pd.__version__) print(&#39;Statsmodels:&#39;, sm.__version__) print(&#39;Scipy:&#39;, scipy.__version__) print(&#39;Rpy2:&#39;, rpy2.__version__) print(&#39;Numpy:&#39;, np.__version__) . Pandas: 0.25.0 Statsmodels: 0.11.0 Scipy: 1.4.1 Rpy2: 2.9.4 Numpy: 1.18.2 . Various functions used . Below are some of the custom functions I wrote for forecast accuracy, gridsearching, residual diagnostics. . def MAPE(y_true, y_pred): &quot;&quot;&quot; %Error compares true value with predicted value. Lower the better. Use this along with rmse(). If the series has outliers, compare/select model using MAPE instead of rmse() &quot;&quot;&quot; y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 def HWGrid(train, test, seasonal_periods): &quot;&quot;&quot; Author: Sandeep Pawar twitter: @PawarBI Functions returns a dataframe with parameters of the Holt-Winter&#39;s method and corresponding train &amp; test evaluation scores. It also does a quick check of the residuals using Ljung-Box test and Shapiro test for normality. Residuals must be uncorrelated. train: (pandas series) - Training data test: (pandas series) - Test data Seasonal_periods: int - No of seasonas in the time period. e.g. 4 for Quarterly, 12 for Monthly, 52 for Weekly data &quot;&quot;&quot; trend = [&#39;add&#39;,&#39;mul&#39;] seasonal = [&#39;add&#39;,&#39;mul&#39;] damped = [False, True] use_boxcox = [False, True, &#39;log&#39;] params = itertools.product(trend,seasonal,damped,use_boxcox) result_df = pd.DataFrame(columns=[&#39;Trend&#39;, &#39;Seasonal&#39;, &#39;Damped&#39;, &#39;BoxCox&#39;,&#39;AICc Train&#39;, &#39;%MAPE_Train&#39;, &#39;RMSE_Train&#39;, &#39;%MAPE_Test&#39;, &#39;RMSE_Test&#39;, &quot;Resid_LJ&quot;, &quot;Resid_Norm&quot;,&quot;Resid_mean&quot; ]) for trend,seasonal,damped,use_boxcox in params: model = ExponentialSmoothing(train, trend=trend, damped=damped, seasonal=seasonal, seasonal_periods=seasonal_periods).fit(use_boxcox=use_boxcox) mape1=MAPE(train,model.fittedvalues) rmse1=rmse(train,model.fittedvalues) mape2=MAPE(test,model.forecast(len(test))) rmse2=rmse(test,model.forecast(len(test))) aicc1 = model.aicc.round(1) lj_p_val = np.mean(ljung(x=model.resid, lags=10)[1]) norm_p_val = jb(model.resid)[1]#shapiro(model.resid)[1] lj = &quot;Uncorrelated&quot; if lj_p_val &gt; 0.05 else &quot;Correlated&quot; norm = &quot;Normal&quot; if norm_p_val &gt; 0.05 else &quot;Non-Normal&quot; result_df = result_df.append({&#39;Trend&#39;:trend , &#39;Seasonal&#39;: seasonal , &#39;Damped&#39;:damped , &#39;BoxCox&#39;:use_boxcox , &#39;%MAPE_Train&#39;:np.round(mape1,2) , &#39;RMSE_Train&#39;:np.round(rmse1,1) , &#39;AICc Train&#39;:aicc1 , &#39;%MAPE_Test&#39;:np.round(mape2,2) , &#39;RMSE_Test&#39;:np.round(rmse2,1) , &#39;Resid_LJ&#39; :lj , &#39;Resid_Norm&#39;:norm , &#39;Resid_mean&#39;:np.round(model.resid.mean(),1)} , ignore_index=True, sort=False) return result_df.sort_values(by=[&quot;RMSE_Test&quot;, &quot;%MAPE_Test&quot;,&quot;RMSE_Train&quot;,&quot;%MAPE_Train&quot;]).style.format({&quot;%MAPE_Train&quot;: &quot;{:20,.2f}%&quot;, &quot;%MAPE_Test&quot;: &quot;{:20,.2f}%&quot;}).highlight_min(color=&#39;lightgreen&#39;) . Calculating cross-validation score for Holt-Winter&#39;s method in Python . def hw_cv(series, seasonal_periods, initial_train_window, test_window): from statsmodels.tools.eval_measures import rmse import warnings warnings.filterwarnings(&quot;ignore&quot;) &quot;&quot;&quot; Author: Sandeep Pawar Date: 4/15/2020 Ver: 1.0 Returns Rolling and Expanding cross-validation scores (avg rmse), along with model paramters for Triple Exponential Smoothing method. Expanding expands the training set each time by adding one observation, while rolling slides the training and test by one observation each time. Output shows parameters used and Rolling &amp; Expanding cv scores. Output is in below order: 1. Trend 2. Seasonal 3. Damped 4. use_boxcox 5. Rolling cv 6. Expanding cv Requirements: Pandas, Numpy, Statsmodels, itertools, rmse series: Pandas Series Time series seasonal_periods: int No of seasonal periods in a full cycle (e.g. 4 in quarter, 12 in monthly, 52 in weekly data) initial_train_window: int Minimum training set length. Recommended to use minimum 2 * seasonal_periods test_window: int Test set length. Recommended to use equal to forecast horizon e.g. hw_cv(ts[&quot;Sales&quot;], 4, 12, 6 ) Output: add add False False R: 41.3 ,E: 39.9 Note: This function can take anywhere from 5-15 min to run full output &quot;&quot;&quot; def expanding_tscv(series,trend,seasonal,seasonal_periods,damped,boxcox,initial_train_window, test_window): i = 0 x = initial_train_window t = test_window errors_roll=[] while (i+x+t) &lt;len(series): train_ts=series[:(i+x)].values test_ts= series[(i+x):(i+x+t)].values model_roll = ExponentialSmoothing(train_ts, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods, damped=damped).fit(use_boxcox=boxcox) fcast = model_roll.forecast(t) error_roll = rmse(test_ts, fcast) errors_roll.append(error_roll) i=i+1 return np.mean(errors_roll).round(1) def rolling_tscv(series,trend,seasonal,seasonal_periods,damped,boxcox,initial_train_window, test_window): i = 0 x = initial_train_window t = test_window errors_roll=[] while (i+x+t) &lt;len(series): train_ts=series[(i):(i+x)].values test_ts= series[(i+x):(i+x+t)].values model_roll = ExponentialSmoothing(train_ts, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods, damped=damped).fit(use_boxcox=boxcox) fcast = model_roll.forecast(t) error_roll = rmse(test_ts, fcast) errors_roll.append(error_roll) i=i+1 return np.mean(errors_roll).round(1) trend = [&#39;add&#39;,&#39;mul&#39;] seasonal = [&#39;add&#39;,&#39;mul&#39;] damped = [False, True] use_boxcox = [False, True, &#39;log&#39;] params = itertools.product(trend,seasonal,damped,use_boxcox) for trend,seasonal,damped,use_boxcox in params: r=rolling_tscv(data[&quot;Sales&quot;], trend, seasonal, 4, damped, use_boxcox, 12,4) e=expanding_tscv(data[&quot;Sales&quot;], trend, seasonal, 4, damped, use_boxcox, 12,4) result = print(trend, seasonal, damped, use_boxcox,&quot; R:&quot;, r,&quot; ,E:&quot;, e) return result . Function for residual diagnostics . def residcheck(residuals, lags): &quot;&quot;&quot; Function to check if the residuals are white noise. Ideally the residuals should be uncorrelated, zero mean, constant variance and normally distributed. First two are must, while last two are good to have. If the first two are not met, we have not fully captured the information from the data for prediction. Consider different model and/or add exogenous variable. If Ljung Box test shows p&gt; 0.05, the residuals as a group are white noise. Some lags might still be significant. Lags should be min(2*seasonal_period, T/5) plots from: https://tomaugspurger.github.io/modern-7-timeseries.html &quot;&quot;&quot; resid_mean = np.mean(residuals) lj_p_val = np.mean(ljung(x=residuals, lags=lags)[1]) norm_p_val = jb(residuals)[1] adfuller_p = adfuller(residuals)[1] fig = plt.figure(figsize=(10,8)) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2); acf_ax = plt.subplot2grid(layout, (1, 0)); kde_ax = plt.subplot2grid(layout, (1, 1)); residuals.plot(ax=ts_ax) plot_acf(residuals, lags=lags, ax=acf_ax); sns.kdeplot(residuals); #[ax.set_xlim(1.5) for ax in [acf_ax, kde_ax]] sns.despine() plt.tight_layout(); print(&quot;** Mean of the residuals: &quot;, np.around(resid_mean,2)) print(&quot; n** Ljung Box Test, p-value:&quot;, np.around(lj_p_val,3), &quot;(&gt;0.05, Uncorrelated)&quot; if (lj_p_val &gt; 0.05) else &quot;(&lt;0.05, Correlated)&quot;) print(&quot; n** Jarque Bera Normality Test, p_value:&quot;, np.around(norm_p_val,3), &quot;(&gt;0.05, Normal)&quot; if (norm_p_val&gt;0.05) else &quot;(&lt;0.05, Not-normal)&quot;) print(&quot; n** AD Fuller, p_value:&quot;, np.around(adfuller_p,3), &quot;(&gt;0.05, Non-stationary)&quot; if (adfuller_p &gt; 0.05) else &quot;(&lt;0.05, Stationary)&quot;) return ts_ax, acf_ax, kde_ax . Function for calculating RMSE &amp; %MAPE . def accuracy(y1,y2): accuracy_df=pd.DataFrame() rms_error = np.round(rmse(y1, y2),1) map_error = np.round(np.mean(np.abs((np.array(y1) - np.array(y2)) / np.array(y1))) * 100,1) accuracy_df=accuracy_df.append({&quot;RMSE&quot;:rms_error, &quot;%MAPE&quot;: map_error}, ignore_index=True) return accuracy_df . Importing Data . path = &#39;https://raw.githubusercontent.com/pawarbi/datasets/master/timeseries/ts_frenchretail.csv&#39; #Sales numbers are in thousands, so I am dividing by 1000 to make it easier to work with numbers, especially squared errors data = pd.read_csv(path, parse_dates=True, index_col=&quot;Date&quot;).div(1_000) data.index.freq=&#39;Q&#39; data.head() . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . data.index . DatetimeIndex([&#39;2012-03-31&#39;, &#39;2012-06-30&#39;, &#39;2012-09-30&#39;, &#39;2012-12-31&#39;, &#39;2013-03-31&#39;, &#39;2013-06-30&#39;, &#39;2013-09-30&#39;, &#39;2013-12-31&#39;, &#39;2014-03-31&#39;, &#39;2014-06-30&#39;, &#39;2014-09-30&#39;, &#39;2014-12-31&#39;, &#39;2015-03-31&#39;, &#39;2015-06-30&#39;, &#39;2015-09-30&#39;, &#39;2015-12-31&#39;, &#39;2016-03-31&#39;, &#39;2016-06-30&#39;, &#39;2016-09-30&#39;, &#39;2016-12-31&#39;, &#39;2017-03-31&#39;, &#39;2017-06-30&#39;, &#39;2017-09-30&#39;, &#39;2017-12-31&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, freq=&#39;Q-DEC&#39;) . Train Test Split: . Part 1 on EDA covers this in detail. I will be using both typical train/test split and cross-validation for training &amp; evaluation. . #Split into train and test train = data.iloc[:-6] test = data.iloc[-6:] #forecast horizon h = 6 train_length = len(train) print(&#39;train_length:&#39;,train_length, &#39; ntest_length:&#39;, len(test) ) #Creating BxCox transformed train &amp; test to be used later train_bcox, bcox_lam = boxcox(train[&quot;Sales&quot;]) print(&quot;BoxCox parameter to linearize the series:&quot;, bcox_lam) test_bcox = boxcox(test[&quot;Sales&quot;], lmbda=bcox_lam) . train_length: 18 test_length: 6 BoxCox parameter to linearize the series: -0.20815057529661993 . #collapse-hide #Create line chart for Training data. index is reset to use Date column train_chart=alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;Date&#39;, y=&#39;Sales&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]) #Create Rolling mean. This centered rolling mean rolling_mean = alt.Chart(train.reset_index()).mark_trail( color=&#39;orange&#39;, size=1 ).transform_window( rolling_mean=&#39;mean(Sales)&#39;, frame=[-4,4] ).encode( x=&#39;Date:T&#39;, y=&#39;rolling_mean:Q&#39;, size=&#39;Sales&#39; ) #Add data labels text = train_chart.mark_text( align=&#39;left&#39;, baseline=&#39;top&#39;, dx=5 # Moves text to right so it doesn&#39;t appear on top of the bar ).encode( text=&#39;Sales:Q&#39; ) #Add zoom-in/out scales = alt.selection_interval(bind=&#39;scales&#39;) #Combine everything (train_chart + rolling_mean +text).properties( width=600, title=&quot;French Retail Sales &amp; 4Q Rolling mean ( in &#39;000)&quot;).add_selection( scales ) . . Seasonal Naive . Seasonal naive method just uses the observations from the corresponding season from last period. For example, forecast Q3 would be sales from Q3 last year. It does not take any trend or previous history into account. This method, as expected, is not the most accurate but helps create a baseline. Hopefully we can identify forecast methods that can perform much better than this method. . . This method is not available in statsmodels library so I wrote a function for it. . def pysnaive(train_series,seasonal_periods,forecast_horizon): &#39;&#39;&#39; Python implementation of Seasonal Naive Forecast. This should work similar to https://otexts.com/fpp2/simple-methods.html Returns two arrays &gt; fitted: Values fitted to the training dataset &gt; fcast: seasonal naive forecast Author: Sandeep Pawar Date: Apr 9, 2020 Ver: 1.0 train_series: Pandas Series Training Series to be used for forecasting. This should be a valid Pandas Series. Length of the Training set should be greater than or equal to number of seasonal periods Seasonal_periods: int No of seasonal periods Yearly=1 Quarterly=4 Monthly=12 Weekly=52 Forecast_horizon: int Number of values to forecast into the future e.g. fitted_values = pysnaive(train,12,12)[0] fcast_values = pysnaive(train,12,12)[1] &#39;&#39;&#39; if len(train_series)&gt;= seasonal_periods: #checking if there are enough observations in the training data last_season=train_series.iloc[-seasonal_periods:] reps=np.int(np.ceil(forecast_horizon/seasonal_periods)) fcarray=np.tile(last_season,reps) fcast=pd.Series(fcarray[:forecast_horizon]) fitted = train_series.shift(seasonal_periods) else: fcast=print(&quot;Length of the trainining set must be greater than number of seasonal periods&quot;) return fitted, fcast . #Before I create the model, I am going to create a dataframe to store all out-of=sample forecasts and the test set predictions = test.copy() . Seasonal Naive Forecast model . #Fitted values py_snaive_fit = pysnaive(train[&quot;Sales&quot;], seasonal_periods=4, forecast_horizon=6)[0] #forecast py_snaive = pysnaive(train[&quot;Sales&quot;], seasonal_periods=4, forecast_horizon=6)[1] #Residuals py_snaive_resid = (train[&quot;Sales&quot;] - py_snaive_fit).dropna() predictions[&quot;py_snaive&quot;] = py_snaive.values predictions . Sales py_snaive . Date . 2016-09-30 773.0 | 681.0 | . 2016-12-31 592.0 | 557.0 | . 2017-03-31 627.0 | 628.0 | . 2017-06-30 725.0 | 707.0 | . 2017-09-30 854.0 | 681.0 | . 2017-12-31 661.0 | 557.0 | . Plot the Forecast . train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) py_snaive_fit.plot(color=&quot;b&quot;, legend=True, label=&quot;SNaive_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) predictions[&quot;py_snaive&quot;].plot(color=&quot;b&quot;, legend=True, label=&quot;Snaive_fc&quot;); . Model Evaluation: . #Training score accuracy(train[&quot;Sales&quot;].iloc[-len(py_snaive_fit.dropna()):], py_snaive_fit.dropna()) . %MAPE RMSE . 0 13.9 | 80.3 | . #Test score accuracy(predictions[&quot;Sales&quot;], predictions[&quot;py_snaive&quot;]) . %MAPE RMSE . 0 9.4 | 92.0 | . Residual Check . residcheck(py_snaive_resid.dropna(),12); . ** Mean of the residuals: 75.21 ** Ljung Box Test, p-value: 0.339 (&gt;0.05, Uncorrelated) ** Jarque Bera Normality Test, p_value: 0.721 (&gt;0.05, Normal) ** AD Fuller, p_value: 0.071 (&gt;0.05, Non-stationary) . observations: . Seasonal naive forecast actually performs quite well considering it&#39;s just a logical forecasting method and there is no statistical procedure involved. | Model captures seasonality and general trend quite well but under forecasts (underfitting) | Training RMSE is 80.3 and test RMSE is 92 which is less than the standard deviation of the training set (111). | Residual analysis shows residuls are not stationary and have non-zero mean. Residual plot clearly shows that the model hasn&#39;t extracted the trend and seasonal behaviour as well as we would like. Though visually the model seems to perform well, it&#39;s not a useful model on its own | Non-zero mean can be fixed by adding the mean back to the forecasts as explained here but in this case the mean is significantly away from zero. | We could perhaps fit an AR model to the residuals to get more out of it. If you look at the ACF plot, it shows no lags are significant but the PACF plot (see below) shows 4,9,10,11,12 are significant. This is an AR process signature. Thus, if we want to capture information and make this model useful, we can fit an AR model to the residuals to create a 2 layer model | This shows importance of always checking the residuals after fitting the model | #PACF of Seasonal Snaive model residuals plot_acf(py_snaive_resid); plot_pacf(py_snaive_resid); . Triple Exponential Smoothing (Holt-Winter&#39;s Mthod) . Triple Exponential Smoothing (Holt Winter&#39;s method) decomposes the series into level, trend, seasonality. Future values are predicted by combining these systematic factors based on recent history. The intuitive idea here is that the future will behave very similar to recent past, we just have to find how much of the past is relevant. The three systematic components are: . Level, (alpha): Average value around which the series varies. For a seasonal time series, level is obtained by first de-seasonalizing the series and then averaging. Alpha value determines how much of the past to consider and is between [0,1]. alpha=1 means give importance only to the last data point (naive forecast) | Trend, (beta): Trend is how the level changes over time. Similar to alpha, a beta value closer to 1 indicates the model is considering only the recent trend. Trend also has a damping factor (phi) which determines how much of the recent trend to &#39;forget&#39;. Consider it as a de-rating factor on trend. | Seasonality (gamma): This factor models how the series behaves in each time period for full season. Recall that in the previous blog about EDA (Part 1), I calculated the seasonal factors. Gamma is the same thing. | . This method is called &quot;Exponential&quot; because each of the above factors give exponential weightage to the past values. . Additive model = (Level + Trend) + Seasonality . Multiplicative Model = (Level Trend) Seasonality . The Exponentialsmoothing() method in statsmodels finds the optimal alpha, beta, gamma and phi by minizing the errors. . . In addition to the Holt Winter&#39;s algorithm hw(), R also has ets() method in the forecast() library. ETS standard for Error, Trend &amp; Seasonality. Prof.Hyndman and his colleagues created a state space representation of exponential smoothing method. Holt-Winter&#39;s method and certain ARIMA models are subset of ETS class of models. In general, ets() performs better than Holt Winter&#39;s method because it can obtain global minima faster. Pyhton has ETS(AAA) model in its tsa.statespace() package which is still in development as of publishing this blog(04/2020). . Grid Searching Triple Exponential Smoothing (Holt-Winter&#39;s Method) . Instead of fitting each model individually, I wrote a custom function HWGrid() to perform uniform, gridsearch using the model parameters. This function also returns statistics on residuals (Ljung Box test, Normality test and mean). You get model evaluation metric and residual metric on 24 models. This may take a while (5-15 min) on your computer. . m=HWGrid(train[&quot;Sales&quot;], test[&quot;Sales&quot;], seasonal_periods=4) . m . Trend Seasonal Damped BoxCox AICc Train %MAPE_Train RMSE_Train %MAPE_Test RMSE_Test Resid_LJ Resid_Norm Resid_mean . 3 add | add | True | False | 170.6 | 3.55% | 20.4 | 10.79% | 81.3 | Uncorrelated | Normal | 4.4 | . 0 add | add | False | False | 151.8 | 2.41% | 18.2 | 10.98% | 82.6 | Uncorrelated | Normal | 4.9 | . 8 add | mul | False | log | 139.7 | 2.29% | 13 | 11.49% | 84 | Uncorrelated | Normal | 0.8 | . 11 add | mul | True | log | 154.3 | 2.29% | 13 | 11.49% | 84 | Uncorrelated | Normal | 0.8 | . 2 add | add | False | log | 138.9 | 2.20% | 12.7 | 11.69% | 84.8 | Uncorrelated | Normal | 0.8 | . 5 add | add | True | log | 153.4 | 2.20% | 12.7 | 11.69% | 84.8 | Uncorrelated | Normal | 0.8 | . 18 mul | mul | False | False | 137.6 | 2.31% | 12.2 | 11.88% | 86.7 | Uncorrelated | Normal | -0.6 | . 12 mul | add | False | False | 154.7 | 3.09% | 19.7 | 12.09% | 91.1 | Uncorrelated | Normal | 5.8 | . 15 mul | add | True | False | 170.5 | 3.56% | 20.4 | 12.15% | 91.4 | Uncorrelated | Normal | 3.2 | . 23 mul | mul | True | log | 153.4 | 2.32% | 12.7 | 12.52% | 92.1 | Uncorrelated | Normal | 0.8 | . 20 mul | mul | False | log | 139 | 2.24% | 12.7 | 12.61% | 92.4 | Uncorrelated | Normal | 0.6 | . 14 mul | add | False | log | 138.1 | 2.16% | 12.4 | 12.76% | 92.9 | Uncorrelated | Normal | 0.6 | . 17 mul | add | True | log | 152.7 | 2.16% | 12.4 | 12.76% | 92.9 | Uncorrelated | Normal | 0.6 | . 1 add | add | False | True | 139.6 | 2.26% | 13 | 12.85% | 94.6 | Uncorrelated | Normal | 0.8 | . 4 add | add | True | True | 154.2 | 2.26% | 13 | 12.85% | 94.6 | Uncorrelated | Normal | 0.8 | . 10 add | mul | True | True | 154.8 | 2.32% | 13.2 | 12.77% | 94.7 | Uncorrelated | Normal | 0.8 | . 7 add | mul | False | True | 140.3 | 2.32% | 13.2 | 12.78% | 94.7 | Uncorrelated | Normal | 0.8 | . 6 add | mul | False | False | 142 | 1.78% | 13.8 | 13.52% | 98 | Uncorrelated | Normal | 3.4 | . 16 mul | add | True | True | 154.1 | 2.22% | 12.9 | 13.34% | 98.1 | Uncorrelated | Normal | 1.4 | . 9 add | mul | True | False | 156.5 | 1.78% | 13.8 | 13.53% | 98.1 | Uncorrelated | Normal | 3.4 | . 19 mul | mul | False | True | 139.7 | 2.32% | 13 | 13.28% | 98.7 | Uncorrelated | Normal | 0.6 | . 13 mul | add | False | True | 139.4 | 2.25% | 12.9 | 13.45% | 99.2 | Uncorrelated | Normal | 0.7 | . 22 mul | mul | True | True | 154.6 | 2.31% | 13.1 | 13.40% | 99.5 | Uncorrelated | Normal | 0.7 | . 21 mul | mul | True | False | nan | nan% | nan | nan% | nan | Correlated | Non-Normal | 469.6 | . Above resulting dataframe is sorted in ascending order showing model with lowest test RMSE and %MAPE at the top. Cells highlighted in green are the lowest numbers in their respective columns. Some observations from this result: . Train RMSE is much smaller than test RMSE, showing all the models perform far better on training set than test set | Models with additive trend and BoxCox=log are at the top. This confirms the finding from the EDA that the trend was more than linear. By taking the &#39;log&#39;, trend is linearized and thus &quot;additive&quot; model can be used. | The top model [&quot;add&quot;, &quot;add&quot;, True, False] performed the worst on the train set. AICc is also the highest. | Top 5 models have used &#39;log&#39; transformation and generally have very similar performance on the test set. | All models except one at the bottom as Uncorrelated residuals. Recall that model with uncorrelated residuals has captured as much information as it can from the available data. | All models are biased (non-zero mean). Ideally we want the model to have zero mean but in this case the means are small and should be added to the forecast to correct the bias. | All models have residuals that are normal. This is a useful but not necessary condition. Having a model with normal residuals can make prediction interval calculations easier. | Model selection should always be done by comparing test evaluation metric and not by comparing residual diagnostic metrics. | Top and fourth model has high AICc. Third and fifth have almost same performance. We want to select a parsimonious and simple model. I will select the model with additive seasonality and trend as it has the lowest AICc in the top 5 models. | hw_model = ExponentialSmoothing(train[&quot;Sales&quot;], trend =&quot;add&quot;, seasonal = &quot;add&quot;, seasonal_periods=4, damped=False).fit(use_boxcox=&#39;log&#39;) hw_fitted = hw_model.fittedvalues hw_resid = hw_model.resid #Adding the mean of the residuals to correct the bias. py_hw = hw_model.forecast(len(test[&quot;Sales&quot;])) + np.mean(hw_resid) predictions[&quot;py_hw&quot;] = py_hw #Holt-Winter Parameters hw_model.params . {&#39;smoothing_level&#39;: 0.7538528963779774, &#39;smoothing_slope&#39;: 3.5680065678712873e-09, &#39;smoothing_seasonal&#39;: 0.0, &#39;damping_slope&#39;: nan, &#39;initial_level&#39;: 6.094619616027986, &#39;initial_slope&#39;: 0.036756035002857526, &#39;initial_seasons&#39;: array([-0.24634722, -0.20368509, -0.09372556, -0.35422446]), &#39;use_boxcox&#39;: &#39;log&#39;, &#39;lamda&#39;: 0.0, &#39;remove_bias&#39;: False} . #Plotting train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) hw_fitted.plot(color=&quot;b&quot;, legend=True, label=&quot;HW_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) predictions[&quot;py_hw&quot;].plot(color=&quot;b&quot;, legend=True, label=&quot;HW_Forecast&quot;); . Cross-Validation . In the above gridsearch, the training set size was fix and we evaluated the model performance by comparing train AICc, RMSE, %MAPE and test RMSE &amp; %MAPE. Test metrics provides true forecast accuracy and should always be used for model selection. This is the preferred approach when the data size is large. But when the time series is short, cross-validation should be used to make sure the model does not overfit the data. The two common appraches are: . Expanding window cross-validation: We start with some initial training set and test set and with each iteration we add one observation to the training set. Forecast errors are calculated with each iteration and averaged to compare model performance. This simulates the performance of the model as we add more observations.For the final forecast we will using all the available history, so using expanding window gives us a good estimate of the forecast uncertainty. . | Rolling Window cross-validation: Similar to Expanding but the training size remains same, instead it moves by one observation each time. Training and test lengths remain same. . | Note that AICc, theoretically, provides the same information because it penalizes complex models that overfit. . . #I would like to perform 5 fold cross validation, want the training size to be at #least 12 and test window = forecast horizon 24 - 4 - 5 = 15. Initial training size should be min 12, max 15. #I will choose 15 hw_cv(data[&quot;Sales&quot;], seasonal_periods=4, initial_train_window=15, test_window=4) . add add False False R: 39.9 ,E: 41.3 add add False True R: 43.4 ,E: 51.0 add add False log R: 40.9 ,E: 36.8 add add True False R: 40.7 ,E: 45.9 add add True True R: 38.2 ,E: 45.4 add add True log R: 33.9 ,E: 39.6 add mul False False R: 35.4 ,E: 39.5 add mul False True R: 42.6 ,E: 50.4 add mul False log R: 44.1 ,E: 40.7 add mul True False R: 38.9 ,E: 40.8 add mul True True R: 37.1 ,E: 45.6 add mul True log R: 37.4 ,E: 41.1 mul add False False R: 44.7 ,E: 43.8 mul add False True R: 47.6 ,E: 49.7 mul add False log R: 46.0 ,E: 39.1 mul add True False R: 163.9 ,E: 90.7 mul add True True R: 292.8 ,E: 70.0 mul add True log R: 487.5 ,E: 39.1 mul mul False False R: 42.6 ,E: 38.9 mul mul False True R: 48.9 ,E: 52.3 mul mul False log R: 43.0 ,E: 39.2 mul mul True False R: 78.4 ,E: nan mul mul True True R: nan ,E: 71.2 mul mul True log R: 3186.8 ,E: 39.9 . Residual Check . residcheck(hw_resid, 12); . ** Mean of the residuals: 0.84 ** Ljung Box Test, p-value: 0.3 (&gt;0.05, Uncorrelated) ** Jarque Bera Normality Test, p_value: 0.419 (&gt;0.05, Normal) ** AD Fuller, p_value: 0.0 (&lt;0.05, Stationary) . Accuracy . accuracy(predictions.Sales,predictions[&quot;py_hw&quot;] ) . %MAPE RMSE . 0 11.8 | 85.7 | . Observations: . In general, cross-validation shows that models with high AICc found in gridsearch have higher cv scores. | Multiplicative trend models have high cv scores | The model I chose from the gridsearch (additive trend, seasonality and logged observations) has the lowest expanding cross-validation score. | Plot shows that the model captures the trend, seasonality very well | This model predicts more sales than the actual (in test). From the gridsearch table, we see that the %MAPE is ~12%. Thus, if we use the this model, we should let the stakeholders know that the forecast error is +12% | Residuals are uncorrelated, normal, stationary with a bias (which we already corrected in the forecast) | SARIMA (Seasonal, Auto-Regressive, Integrated, Moving Average Model) . SARIMA (Seasonal ARIMA) is a classical, statistical forecasting method that predicts the forecast values based on past values, i.e lagged values (AR) and lagged errors (MA). Unlike Holt-Winter&#39;s (or ETS), it needs the time series to be stationary before it can be used. That&#39;s where the &quot;Integrated&quot; part comes from. &quot;Integration&quot; means differecning the series to remove trend and make it stationary. You can learn amore about the fundamentals by watching below video. Prof. Shmueli also has an excellent book on Forecasting that I highly recommend. . I also recommend this free Coursera course Practical Time Series Analysis if you want to gain practical and intutitive understanding of ARIMA models. . I will share my main take-aways from these and other resources I have used. . . Auto-regression, AR(p): . As the name suggests, it&#39;s the linear regression with its past values | AR (p) =&gt; Current value = mean + fraction (phi) of yesterday&#39;s value + fraction (phi) of day before yesterday&#39;s value +......+ fraction of pth day&#39;s value + noise | If phi is negaitive =&gt; mean inversion, i.e today&#39;s value will likely go down after yesterday&#39;s peak. | If phi is positive =&gt; Momentum | If phi = 0 =&gt; white noise | If phi = 1 =&gt; random walk | phi has to be between [-1,1] for process to be stationary | If the PACF plot cuts off sharply at lag k, while there is a gradual decay in ACF, it&#39;s a AR(p) process. [Note: I keep PACF and AR(P) mnemonic in mind to know which plot to use for identifying AR process) | An AR(1) model is equivalent to MA(infinity) model, (practially q&gt;&gt;50) | Below video explain AR process really well . . Moving Average ,MA(q): . MA process is not the same as taking moving average of a series | MA process is made up of white noise at different times. In MA(q), q tells us how far back along the sequence of white noise we have to loo for weighted average | For example, in our case if the series is an MA(q) process, the forecast is not affected by the previous sales but rather errors in past forecast. | MA processes are not common but when combined with AR, can produce very accurate forecasts | For an MA(q) model, the forecast beyond 1 period will be the same for rest of the forecast horizon | To identify MA(q) process, plot the ACF. If it sharply cuts off at qth lag, it&#39;s an MA(q) process | Thus, ARIMA (p,d,q) = constant + (weighted sum of last p values) + (weighted sum of last q values of forecast errors) after d differencing . Below are the simulated MA and AR processes. If you run this cell a few times and observe the plots, you will not that it&#39;s not possible to distinguish an AR from MA by just looking at the plots. You need to study the ACF &amp; PACF to know the difference. . #Simulating AR process from statsmodels.tsa.arima_process import ArmaProcess ar = np.array([1,-0.9]) ma = np.array([1, 0.9]) AR = ArmaProcess(ar=ar, ma=None) MA = ArmaProcess(ar=None, ma=ma) simulated_AR= AR.generate_sample(125) simulated_MA= MA.generate_sample(125) fig, (ax1, ax2) = plt.subplots(1, 2) # fig = plt.figure(figsize=(10,8)) fig.suptitle(&#39;Simulated AR &amp; MA Processes&#39;) ax1.plot(simulated_AR); ax2.plot(simulated_MA); . Finding the parameters of the ARIMA process (p,d,q) is an art and science. Generally, p+q &lt;=3. Similar to ARIMA, Seasonal ARIMA (SARIMA) has (P,D,Q) parameters, so SARIMA is (p,d,q)(P,D,Q). p+d+q+P+D+Q &lt;=6 (generally) . Instead of finding the above parameters manually by studying the ACF, PACF, we usually use grid searching just like HW method above. pmdarima is a great library for SARIMA forecasting in Python. It returns the parameters that minimizes AICc and also has cross-validation tools.statsmodels has arma_order_select_ic() for identifying order of the ARMA model but not for SARIMA. . Let&#39;s first take a look at the ACF and PACF to identify potential order of the SARIMA model . ACF plot shows that 1st lag is significant (outside the blue band), the ACFs and PACF both decrease gradually. We will need at least 1 differencing to make the series stationary. When ACF and PACF plots do not have sharp cut offs and significant lags at higher orders, its a indication of ARMA process with seasonality. . plot_acf(train[&quot;Sales&quot;]); plot_pacf(train[&quot;Sales&quot;]); . Using pmdarima() to find the SARIMA order with lowest aicc. This may take a few minutes to run. . (auto_arima(train[&quot;Sales&quot;], seasonal=True, m=4, #seasonality_order 4 d=1, #ACF plot showed we need at least 1 differencing information_criterion=&#39;aicc&#39;). #You can choose AIC, BIC. AICc is corrected AIC summary()) . SARIMAX Results Dep. Variable: y | No. Observations: 18 | . Model: SARIMAX(0, 1, 1)x(1, 1, [], 4) | Log Likelihood -55.434 | . Date: Thu, 16 Apr 2020 | AIC 118.867 | . Time: 15:07:50 | BIC 121.127 | . Sample: 0 | HQIC 118.403 | . - 18 | | . Covariance Type: opg | | . | coef std err z P&gt;|z| [0.025 0.975] . intercept 7.0286 | 1.992 | 3.529 | 0.000 | 3.125 | 10.932 | . ma.L1 -0.9992 | 158.481 | -0.006 | 0.995 | -311.617 | 309.618 | . ar.S.L4 -0.7492 | 0.305 | -2.457 | 0.014 | -1.347 | -0.152 | . sigma2 176.2645 | 2.78e+04 | 0.006 | 0.995 | -5.43e+04 | 5.47e+04 | . Ljung-Box (Q): 8.66 | Jarque-Bera (JB): 1.55 | . Prob(Q): 0.73 | Prob(JB): 0.46 | . Heteroskedasticity (H): 1.17 | Skew: -0.84 | . Prob(H) (two-sided): 0.88 | Kurtosis: 3.24 | . Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step). Observations . pmdarima() has identified the training set as (0, 1, 1)x(1, 1, 0, 4) process. It&#39;s a seasonal AR(1) with d=D=1. Summary also shows that Ljung Box p value (Prob(Q) and JB p value (Prob(JB) are &gt; 0.05 thus residuals are uncorrelated and normally distributed. Summary also shows MA is significant at lag 1, seasonal AR is significant at lag 4. . #Creating SARIMA model in Python using statsmodels sarima_model=(SARIMAX(endog=train[&quot;Sales&quot;], order=(0,1,1), seasonal_order=(1,1,0,4), trend=&#39;c&#39;, enforce_invertibility=False)) sarima_fit=sarima_model.fit() start = len(train) end = len(train) +len(test) -1 sarima_fitted = sarima_fit.fittedvalues sarima_resid = sarima_fit.resid py_sarima = sarima_fit.predict(start, end, dynamic=False) predictions[&quot;py_sarima&quot;] = py_sarima sarima_fit.plot_diagnostics(); . Residual Check . SARIMAX() has its own residual diagnostics (shown above). It shows the residuals are normally distributed, uncorrelated. Q-Q plot shows an outlier in the lower left but otherwise everyting looks good. . np.mean(ljung(sarima_fit.resid,13)[1]) . 0.7227747659281314 . Accuracy . accuracy(predictions.Sales,py_sarima) . %MAPE RMSE . 0 12.6 | 94.4 | . Observations: . SARIMA did worse than Holt-Winter&#39;s method. RMSE for HW was 85.7 | SARIMA2 - (Using Logged value) . Recall that one of the observations from the HW method was log models performed better, so I will try log of the trainining set. The forecast will be logged values so I will inverse it with np.exp() . #Fitting model to log of train (auto_arima(np.log(train[&quot;Sales&quot;]), seasonal=True, m=4, #seasonality_order 4 # d=1, #ACF plot showed we need at least 1 differencing information_criterion=&#39;aicc&#39;). #You can choose AIC, BIC. AICc is corrected AIC summary()) . SARIMAX Results Dep. Variable: y | No. Observations: 18 | . Model: SARIMAX(1, 1, 0, 4) | Log Likelihood 26.993 | . Date: Thu, 16 Apr 2020 | AIC -47.986 | . Time: 15:48:30 | BIC -46.068 | . Sample: 0 | HQIC -48.163 | . - 18 | | . Covariance Type: opg | | . | coef std err z P&gt;|z| [0.025 0.975] . intercept 0.2848 | 0.032 | 8.979 | 0.000 | 0.223 | 0.347 | . ar.S.L4 -0.8106 | 0.163 | -4.966 | 0.000 | -1.131 | -0.491 | . sigma2 0.0009 | 0.001 | 1.357 | 0.175 | -0.000 | 0.002 | . Ljung-Box (Q): 20.24 | Jarque-Bera (JB): 0.47 | . Prob(Q): 0.09 | Prob(JB): 0.79 | . Heteroskedasticity (H): 0.40 | Skew: -0.24 | . Prob(H) (two-sided): 0.34 | Kurtosis: 2.23 | . Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step). It&#39;s a SARIMA model with no (p,d,q). (P,D,Q,m) = (1,1,0,4) . Prob(Q) &gt; 0.05 and Prob(JB) &gt; 0.05 Thus, residuals are uncorrelated and normal ! . sarima_logmodel=(SARIMAX(np.log(train[&quot;Sales&quot;]), order=(0,0,0), seasonal_order=(1,1,0,4), trend=&#39;c&#39;, enforce_invertibility=False)).fit() sarima_log = np.exp(sarima_logmodel.predict(start, end)) predictions[&quot;sarima_log&quot;] = sarima_log slog_fitted = np.exp(sarima_logmodel.fittedvalues) . Residual Check . sarima_logmodel.plot_diagnostics(); . Observations . Residuals are not stationary but are normally distributed and are uncorrelated | There is an outlier in the left tail in the Q-Q plot. This is expected, since we took 1 seasonal differencing so model did not fit well to the early data. | Accuracy . accuracy(predictions.Sales,sarima_log ) . %MAPE RMSE . 0 11.1 | 81.7 | . RMSE &amp; %MAPE are now slightly better than the HW model !!! Plot below shows model did not fit well in the early data but performs well on validation set. . #Plotting train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) slog_fitted.plot(color=&quot;b&quot;, legend=True, label=&quot;HW_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) predictions[&quot;sarima_log&quot;].plot(color=&quot;b&quot;, legend=True, label=&quot;LogSARIMA_forecast&quot;); . . Note: There is a misconception that ARIMA is a more accurate method that ETS/Holt-Winters. That&#8217;s not accurate. In this example, ARIMA worked better but that may not always be the case and you won&#8217;t know until you experiment. . Facebook Prophet . Facebook described Prophet library as below in their documentation: . &quot; Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.&quot; . Video below gives great overview of this package. . . My main take aways are: . Prophet was built for high frequency data like daily, hourly, minute etc.. It may not work very well on monthly, quarterly data, but you won&#39;t know until you try. | In addition to forecasting, it also provide changepoints, anomalies which are great for detecting sudden changes in the time series | Prof. Kourentzes tested Prophet along with other methods (ETS, SARIMA) on M3 competition data and found that Prophet performed poorly. | ETS/HW &amp; SARIMA cannot work with multiple seasonalities &amp; high frequency data. Prophet can also include effect of holidays. | Prophet requires the data to be in specific format. Dataframe must have time column ds and time series observations in column y | Though Prophet is designed mainly for high frequency data, it can be used for monthly/quarterly/yearly data with some tweaks. | from fbprophet import Prophet data_fb = data.reset_index() data_fb.columns=[&#39;ds&#39;,&#39;y&#39;] #create new df with columns ds &amp; y train_fb, test_fb = data_fb.iloc[:-len(test)], data_fb.iloc[-len(test):] #create train &amp; test df . #Fit the model to train fb1_model=Prophet(weekly_seasonality=False, daily_seasonality=False, n_changepoints=10, seasonality_mode=&quot;multiplicative&quot;).fit(train_fb) #I tried &quot;additive too&quot;, it was slightly worse #Prophet results are saved to a dataframe using make_future_dataframe() fb1_df=fb1_model.make_future_dataframe(6, freq=&#39;Q&#39;) #set the freq argument to &#39;Q&#39; for quarterly data #We only need &quot;ds&quot; and &quot;yhat&quot; columns.. &quot;ds&quot; is the date column and &quot;yhat&quot; are predictions fb1_fc_df=fb1_model.predict(fb1_df)[[&quot;ds&quot;,&quot;yhat&quot;]] #Residuals fb1_resid = train[&quot;Sales&quot;].values - fb1_fc_df[&#39;yhat&#39;].iloc[:len(train)] fb1_fc = fb1_fc_df.iloc[-len(test):] predictions[&quot;fb1&quot;] = fb1_fc[&quot;yhat&quot;].values . fb1_fc_df.head() . ds yhat . 0 2012-03-31 | 362.0 | . 1 2012-06-30 | 385.0 | . 2 2012-09-30 | 432.0 | . 3 2012-12-31 | 341.0 | . 4 2013-03-31 | 382.0 | . Accuracy . accuracy(test[&quot;Sales&quot;],fb1_fc[&quot;yhat&quot;].values) . %MAPE RMSE . 0 9.1 | 69.4 | . WHOAAA !!! Prophet worked significantly better than HW &amp; SARIMA, that too on quarterly data ! I didn&#39;t expect that given how extensively it&#39;s been proven that Prophet does not work well on low frequency data. . train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) fb1_fc_df.set_index(&#39;ds&#39;)[&quot;yhat&quot;].iloc[:-len(test)].plot(color=&quot;b&quot;, legend=True, label=&quot;Prophet_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) fb1_fc_df.set_index(&#39;ds&#39;)[&quot;yhat&quot;].iloc[-len(test):].plot(color=&quot;b&quot;, legend=True, label=&quot;Prophet_forecast&quot;); . residcheck(fb1_resid,12); . ** Mean of the residuals: -0.0 ** Ljung Box Test, p-value: 0.402 (&gt;0.05, Uncorrelated) ** Jarque Bera Normality Test, p_value: 0.003 (&lt;0.05, Not-normal) ** AD Fuller, p_value: 0.969 (&gt;0.05, Non-stationary) . Observations: . Prophet under forecasts while other methods were over the actual values in the validation set | Prophet captured the overall trend and seasonality well | Above plot also shows that Prophet overfitted the training set. I am sure there are hyperparameters that could be tuned but most the parameters are for high frequency data. Also, Prophet has a cross-validation method built-in, but it only accepts daily or sub-daily data. That&#39;s a limitation. | Residuals are uncorrelated but not-normal, which is ok. | Forecasts . We evaluated four different forecasting methods: . Seasonal Naive | Holt-Winter&#39;s (Triple Exponential Smoothing) | SARIMA | Prophet | . Prophet gave the most accurate forecast, followed by SARIMA (log), HW and Seasonal Naive. Here&#39;s how the point forecasts compare with each other and the test set: . forecasts = predictions[[&quot;Sales&quot;,&quot;py_snaive&quot;,&quot;py_hw&quot;,&quot;sarima_log&quot;,&quot;fb1&quot;]] forecasts.round(0) . Sales py_snaive py_hw sarima_log fb1 . Date . 2016-09-30 773.0 | 681.0 | 813.0 | 797.0 | 717.0 | . 2016-12-31 592.0 | 557.0 | 650.0 | 650.0 | 518.0 | . 2017-03-31 627.0 | 628.0 | 751.0 | 743.0 | 654.0 | . 2017-06-30 725.0 | 707.0 | 813.0 | 803.0 | 716.0 | . 2017-09-30 854.0 | 681.0 | 941.0 | 933.0 | 762.0 | . 2017-12-31 661.0 | 557.0 | 753.0 | 762.0 | 572.0 | . fc_melt=pd.melt(forecasts.reset_index(), id_vars=&#39;Date&#39;, value_vars=[&#39;Sales&#39;, &#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;], var_name=&quot;Model&quot;, value_name=&quot;Forecasts&quot;).round(0) . fc_melt.head() . Date Model Forecasts . 0 2016-09-30 | Sales | 773.0 | . 1 2016-12-31 | Sales | 592.0 | . 2 2017-03-31 | Sales | 627.0 | . 3 2017-06-30 | Sales | 725.0 | . 4 2017-09-30 | Sales | 854.0 | . #collapse-hide #Based on https://altair-viz.github.io/gallery/multiline_tooltip.html # Create a selection that chooses the nearest point &amp; selects based on x-value nearest = alt.selection(type=&#39;single&#39;, nearest=True, on=&#39;mouseover&#39;, fields=[&#39;Date&#39;], empty=&#39;none&#39;) # The basic line line = alt.Chart(fc_melt).mark_line(point=True).encode( x=&#39;Date&#39;, y=alt.Y(&#39;Forecasts:Q&#39;,scale=alt.Scale(domain=[500,1000], clamp=True)), color=&#39;Model:N&#39;, tooltip=[&#39;Date&#39;,&#39;Forecasts&#39;,&#39;Model&#39;] ) # Transparent selectors across the chart. This is what tells us # the x-value of the cursor selectors = alt.Chart(fc_melt).mark_point().encode( x=&#39;Date&#39;, opacity=alt.value(0), ).add_selection( nearest ) # Draw points on the line, and highlight based on selection points = line.mark_point().encode( opacity=alt.condition(nearest, alt.value(1), alt.value(0)) ) # Draw text labels near the points, and highlight based on selection text = line.mark_text(align=&#39;left&#39;, baseline=&#39;top&#39;, dx=5, dy=-5).encode( text=alt.condition(nearest, &#39;Forecasts:Q&#39;, alt.value(&#39; &#39;)) ) text2 = line.mark_text(align=&#39;left&#39;, baseline=&#39;bottom&#39;, dx=5, dy=-5).encode( text=alt.condition(nearest, &#39;Model:N&#39;, alt.value(&#39; &#39;)) ) # Draw a rule at the location of the selection rules = alt.Chart(fc_melt).mark_rule(color=&#39;gray&#39;).encode( x=&#39;Date&#39;, ).transform_filter( nearest ) # Put the five layers into a chart and bind the data alt.layer( line, selectors, points, rules, text, text2 ).properties( width=800, height=500, title=&quot;Comaprison of various Forecasting Models&quot; ).interactive() . . In the above chart, blue line is the test set and rest are forecasts. You can zoom-in/out, pan to inspect the fit over the test set. . SNaive &amp; Prophet are below the test while SARIMA and HW are above. | HW &amp; SARIMA are almost identical | If we had to pick a single best model, we would use Prophet since it has the lowest RMSE value BUT it overfit the training set and relative to that performed poorly on the test. Unfortunately, Prophet doesn&#39;t have cross-validation method for low frequency data. | SARIMA did well on the test set but not so good on the early part of the training set. That&#39;s acceptable but the point is that all these methods did well in training or test. Thus, a better approach is to create an ensemble forecast that combines all these forecasts together. | Many research studies have shown that forecast combination often provides a more robust, accurate forecast that&#39;s less susceptible to overfitting. You can read more here | Ensemble Forecasts . Although there are many different ways to combine forecasts, simple averaging often works as good as a more complex method and is easier to implement/monitor/debug. As we saw above, some forecasts are above the test and some are below. So hopefully averaging will bring it closer to the actual values. We will try some combinations. . forecasts . Sales py_snaive py_hw sarima_log fb1 . Date . 2016-09-30 773.0 | 681.0 | 812.646500 | 797.175115 | 716.954330 | . 2016-12-31 592.0 | 557.0 | 649.896932 | 649.770583 | 517.539078 | . 2017-03-31 627.0 | 628.0 | 750.899727 | 743.222703 | 654.222708 | . 2017-06-30 725.0 | 707.0 | 812.897039 | 802.853193 | 715.815974 | . 2017-09-30 854.0 | 681.0 | 941.221318 | 932.851318 | 761.781268 | . 2017-12-31 661.0 | 557.0 | 752.695160 | 762.493309 | 571.860845 | . forecasts[&quot;avg_all&quot;] = forecasts.iloc[:,1:4].mean(axis=1) forecasts[&quot;snaive_hw&quot;] = forecasts[[&quot;py_snaive&quot;,&quot;py_hw&quot;]].mean(axis=1) forecasts[&quot;snaive_sarima&quot;] = forecasts[[&quot;py_snaive&quot;,&quot;sarima_log&quot;]].mean(axis=1) forecasts[&quot;hw_fb1&quot;] = forecasts[[&quot;fb1&quot;,&quot;py_hw&quot;]].mean(axis=1) forecasts[&quot;sarima_fb1&quot;] = forecasts[[&quot;sarima_log&quot;,&quot;fb1&quot;]].mean(axis=1) forecasts[&quot;hw_sarima_fb1&quot;] = forecasts[[&quot;py_hw&quot;,&quot;sarima_log&quot;,&quot;fb1&quot;]].mean(axis=1) forecasts . Sales py_snaive py_hw sarima_log fb1 avg_all snaive_hw snaive_sarima hw_fb1 sarima_fb1 hw_sarima_fb1 . Date . 2016-09-30 773.0 | 681.0 | 812.646500 | 797.175115 | 716.954330 | 763.607205 | 746.823250 | 739.087557 | 764.800415 | 757.064722 | 775.591982 | . 2016-12-31 592.0 | 557.0 | 649.896932 | 649.770583 | 517.539078 | 618.889172 | 603.448466 | 603.385291 | 583.718005 | 583.654830 | 605.735531 | . 2017-03-31 627.0 | 628.0 | 750.899727 | 743.222703 | 654.222708 | 707.374143 | 689.449863 | 685.611351 | 702.561218 | 698.722706 | 716.115046 | . 2017-06-30 725.0 | 707.0 | 812.897039 | 802.853193 | 715.815974 | 774.250078 | 759.948520 | 754.926597 | 764.356507 | 759.334584 | 777.188736 | . 2017-09-30 854.0 | 681.0 | 941.221318 | 932.851318 | 761.781268 | 851.690879 | 811.110659 | 806.925659 | 851.501293 | 847.316293 | 878.617968 | . 2017-12-31 661.0 | 557.0 | 752.695160 | 762.493309 | 571.860845 | 690.729490 | 654.847580 | 659.746655 | 662.278003 | 667.177077 | 695.683105 | . for col in forecasts.iloc[:,1:]: print(col,&quot;--&gt;&quot;,&quot;RMSE:&quot;,rmse(forecasts[&quot;Sales&quot;],forecasts[col]).round(1),&quot; MAPE:&quot;, MAPE(forecasts[&quot;Sales&quot;], forecasts[col]).round(1)) . py_snaive --&gt; RMSE: 92.0 MAPE: 9.4 py_hw --&gt; RMSE: 85.7 MAPE: 11.8 sarima_log --&gt; RMSE: 81.7 MAPE: 11.1 fb1 --&gt; RMSE: 65.8 MAPE: 8.3 avg_all --&gt; RMSE: 42.0 MAPE: 5.0 snaive_hw --&gt; RMSE: 36.1 MAPE: 4.3 snaive_sarima --&gt; RMSE: 36.1 MAPE: 4.2 hw_fb1 --&gt; RMSE: 35.1 MAPE: 3.4 sarima_fb1 --&gt; RMSE: 33.5 MAPE: 3.6 hw_sarima_fb1 --&gt; RMSE: 46.0 MAPE: 5.4 . Our best point forecasts (Prophet) had an RMSE of 66, we can cut that in half by combining it with SARIMA !!! Let&#39;s plot that . forecasts[[&quot;Sales&quot;, &quot;sarima_fb1&quot;,&quot;sarima_log&quot;,&quot;fb1&quot;]].plot(figsize=(12,8)); . That pretty much matches the actual values in the test set accurately, plus by combining we can make the forecast more robust . When combining multiple forecasts, the correct approach is to create the distribution of the estimate by bootstrapping the forecast errors and then using that obtain prediction interval, average forecast. In this case, since we are using just two methods, we can use the variance of the forecast errors to create prediction interval. . . Note: We want to use forecast errors and not residuals for prediction interval. Errors are difference between test set and the forecast on test, whereas residuals are difference between training set and fitted values. Residuals are obtained by optimizing the model to minimize the residuals, thus not a true measure of forecast uncertainty. . Final Model . #SARIMA Forecast sarima_fc = np.exp(sarima_logmodel.predict(end+1, end+4, dynamic=False)) #dynamic = False to use all the data #SARIMA Error Variance sarima_var = np.var(forecasts[&quot;Sales&quot;]-forecasts[&quot;sarima_log&quot;]) . #Prophet Forecast m=Prophet(weekly_seasonality=False, daily_seasonality=False, n_changepoints=10, seasonality_mode=&quot;multiplicative&quot;).fit(data_fb) #I tried &quot;additive too&quot;, it was slightly worse #Prophet results are saved to a dataframe using make_future_dataframe() future=m.make_future_dataframe(4, freq=&#39;Q&#39;) #set the freq argument to &#39;Q&#39; for quarterly data #We only need &quot;ds&quot; and &quot;yhat&quot; columns.. &quot;ds&quot; is the date column and &quot;yhat&quot; are predictions fb_df=fb1_model.predict(future)[[&quot;ds&quot;,&quot;yhat&quot;]] prophet_fc = prophet_fc[-4:] prophet_var = np.var(forecasts[&quot;Sales&quot;]-forecasts[&quot;fb1&quot;]) #calculating coriance between SARIMA and Prophet cov = np.cov((forecasts[&quot;Sales&quot;]-forecasts[&quot;sarima_log&quot;]), forecasts[&quot;Sales&quot;]-forecasts[&quot;fb1&quot;])[0][1] . w=0.5 #weight forecast = (w*sarima_fc) + ((1-w)*prophet_fc) var = sarima_var+prophet_var std = np.sqrt(var) forecast_df=pd.DataFrame(forecast, columns=[&quot;forecast&quot;]) forecast_df[&quot;hi&quot;] = forecast_df[&quot;forecast&quot;] + (1.96*std) forecast_df[&quot;lo&quot;] = forecast_df[&quot;forecast&quot;] - (1.96*std) . forecast_df.round(0) . forecast hi lo . 2018-03-31 770.0 | 873.0 | 666.0 | . 2018-06-30 843.0 | 946.0 | 739.0 | . 2018-09-30 949.0 | 1053.0 | 845.0 | . 2018-12-31 760.0 | 864.0 | 656.0 | . data[&quot;Sales&quot;].plot(legend=True, label=&quot;Data&quot;, figsize=(15,10)) forecast_df[&quot;forecast&quot;].plot(legend=True, label=&quot;Forecast&quot;) forecast_df[&quot;hi&quot;].plot(legend=True, label=&quot;Hi95%&quot;, color=&#39;gray&#39;, style=&quot;--&quot;) forecast_df[&quot;lo&quot;].plot(legend=True, label=&quot;Lo95%&quot;, color=&#39;gray&#39;, style=&quot;--&quot;); . Forecast Using R . Import rpy2 . Use pandas2ri.activate() to convert pandas to R dataframe . %load_ext rpy2.ipython to use %%R magic command to run R in a cell . import rpy2 import warnings warnings.filterwarnings(&#39;ignore&#39;) from rpy2.robjects import pandas2ri import rpy2.rinterface as rinterface pandas2ri.activate() %load_ext rpy2.ipython . The rpy2.ipython extension is already loaded. To reload it, use: %reload_ext rpy2.ipython . R-ETS . ets() package in R uses a state space approach rather than Holt-Winter&#39;s (which can be modeled in ETS form). You can read more here . %%R -i data -o fets,fc_ets library(fpp2) r_train &lt;- ts(data$Sales, start=c(2012,01), frequency=4) fets &lt;- r_train %&gt;% ets() fc_ets&lt;-r_train %&gt;% ets() %&gt;% forecast(h=4) %&gt;% summary() r_train %&gt;% ets() %&gt;% forecast(h=4) %&gt;% autoplot() . print(fets,end=&quot;&quot;) . ETS(M,A,M) Call: ets(y = .) Smoothing parameters: alpha = 0.1564 beta = 1e-04 gamma = 2e-04 Initial states: l = 339.3571 b = 16.5403 s = 0.8781 1.1272 1.0301 0.9646 sigma: 0.05 AIC AICc BIC 241.8271 254.6843 252.4296 . ets() returned ETS(M,A,M) as the best model based on AICc, i.e &quot;multiplicative&quot; error, &quot;additive&quot; trend and &quot;multiplicative&quot; error. We also obtained same results in Python. But because we used a log transform the multiplicative term became additive. ets() is 10x faster and more accurate than creating gridsearch and finding a model . fc_ets . Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 . 0 737.458963 | 690.235833 | 784.682093 | 665.237417 | 809.680508 | . 1 804.569111 | 752.445770 | 856.692452 | 724.853340 | 884.284882 | . 2 899.154431 | 840.263229 | 958.045633 | 809.088110 | 989.220753 | . 3 714.953180 | 667.641498 | 762.264862 | 642.596206 | 787.310153 | . R-SARIMA . %%R -i train -o fsarima,fc_sarima r_train &lt;- ts(train$Sales, start=c(2012,01), frequency=4) fsarima &lt;- r_train %&gt;% auto.arima(stepwise=FALSE) . %%R -i data -o fc_arima r_data &lt;- ts(data$Sales, start=c(2012,01), frequency=4) fc_arima&lt;- r_data %&gt;% auto.arima(stepwise=FALSE) %&gt;% forecast(h=4) %&gt;% summary() r_data %&gt;% auto.arima(stepwise=FALSE) %&gt;% forecast(h=4) %&gt;% autoplot() . print(fsarima, end=&quot;&quot;) . Series: . ARIMA(0,1,0)(1,1,0)[4] Coefficients: sar1 -0.6495 s.e. 0.2501 sigma^2 estimated as 433: log likelihood=-58.48 AIC=120.96 AICc=122.16 BIC=122.09 . We obtained the same SARIMA order as Python ARIMA(0,1,0)(1,1,0)[4] . fc_sarima . Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 . 0 676.847258 | 644.770943 | 708.923573 | 627.790767 | 725.903749 | . 1 791.750583 | 748.385675 | 835.115490 | 725.429680 | 858.071486 | . 2 920.750583 | 877.385675 | 964.115490 | 854.429680 | 987.071486 | . 3 727.750583 | 684.385675 | 771.115490 | 661.429680 | 794.071486 | . Conclusion . We explored various time series forecasting methods, model selection procedures and created an ensemble forecast to significantly improve the forecast accuracy. I did not include any deep-learning methods that they generally do well on a large dataset with multiple features. Classical methods such as HW/ETS &amp; ARIMA also provide interpretable results that can prove useful for understanding the behaviour of the time time series. In the next blog I will cover deploying this model in PowerBI. . References: . Forecasting: Principles and Practice, by Prof. Hyndman | Time Series Analysis and its Applications, by Robert Shumway | Time Series Analysis and Forecasting, by Montgomery &amp; Jennings | Introduction to Time Series and Analysis, by Brockwell | Practial Time Series Forecasting with R, by Galit Shmueli |",
            "url": "https://pawarbi.github.io/blog/forecasting/r/python/rpy2/altair/fbprophet/2020/04/17/timeseries-part2.html",
            "relUrl": "/forecasting/r/python/rpy2/altair/fbprophet/2020/04/17/timeseries-part2.html",
            "date": "  Apr 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Test Test Test",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote. . 2. This is the other footnote. You can even have a link! .",
            "url": "https://pawarbi.github.io/blog/jupyter/2020/04/16/test.html",
            "relUrl": "/jupyter/2020/04/16/test.html",
            "date": "  Apr 16, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Time series Forecasting in Python & R, Part 1 (EDA)",
            "content": "Project Goals . This is a quarterly sales data of a French retail company from Prof. Rob Hyndman&#39;s &quot;Forecasting Methods &amp; Applications&quot; book. I have uploaded the data to my github.The goals for this first part are: . Exploratory data analysis of the time series | Explain the time series behaviour in qualitative and quantitative terms to build intution for model selection | Identify the models that can be used based on the findings in the EDA | Importing libraries . #collapse-hide #Author: Sandeep Pawar #Version: 1.0 #Date Mar 27, 2020 import pandas as pd import numpy as np import itertools #Plotting libraries import matplotlib.pyplot as plt import seaborn as sns import altair as alt plt.style.use(&#39;seaborn-white&#39;) %matplotlib inline #statistics libraries import statsmodels.api as sm import scipy from scipy.stats import anderson from statsmodels.tools.eval_measures import rmse from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import month_plot, seasonal_plot, plot_acf, plot_pacf, quarter_plot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing from statsmodels.stats.diagnostic import acorr_ljungbox as ljung #from nimbusml.timeseries import SsaForecaster from statsmodels.tsa.statespace.tools import diff as diff import pmdarima as pm from pmdarima import ARIMA, auto_arima from scipy import signal from scipy.stats import shapiro from scipy.stats import boxcox from sklearn.preprocessing import StandardScaler #library to use R in Python import rpy2 from rpy2.robjects import pandas2ri pandas2ri.activate() import warnings warnings.filterwarnings(&quot;ignore&quot;) np.random.seed(786) . . . Note: I have found that results could be significanlty different if you use different versions of the libraries, especially with statsmodels. If you want to reproduce these results, be sure to use the same versions of these libraries. For this project, I created a conda virtual environment as rpy2 requires specific versions of Pandas &amp; certain R libraries . #Printing library versions print(&#39;Pandas:&#39;, pd.__version__) print(&#39;Statsmodels:&#39;, sm.__version__) print(&#39;Scipy:&#39;, scipy.__version__) print(&#39;Rpy2:&#39;, rpy2.__version__) . Pandas: 0.25.0 Statsmodels: 0.11.0 Scipy: 1.4.1 Rpy2: 2.9.4 . #collapse-hide # Define some custom functions to help the analysis def MAPE(y_true, y_pred): &quot;&quot;&quot; %Error compares true value with predicted value. Lower the better. Use this along with rmse(). If the series has outliers, compare/select model using MAPE instead of rmse() &quot;&quot;&quot; y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 def residcheck(residuals, lags): &quot;&quot;&quot; Function to check if the residuals are white noise. Ideally the residuals should be uncorrelated, zero mean, constant variance and normally distributed. First two are must, while last two are good to have. If the first two are not met, we have not fully captured the information from the data for prediction. Consider different model and/or add exogenous variable. If Ljung Box test shows p&gt; 0.05, the residuals as a group are white noise. Some lags might still be significant. Lags should be min(2*seasonal_period, T/5) plots from: https://tomaugspurger.github.io/modern-7-timeseries.html &quot;&quot;&quot; resid_mean = np.mean(residuals) lj_p_val = np.mean(ljung(x=residuals, lags=lags)[1]) norm_p_val = jb(residuals)[1] adfuller_p = adfuller(residuals)[1] fig = plt.figure(figsize=(10,8)) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2); acf_ax = plt.subplot2grid(layout, (1, 0)); kde_ax = plt.subplot2grid(layout, (1, 1)); residuals.plot(ax=ts_ax) plot_acf(residuals, lags=lags, ax=acf_ax); sns.kdeplot(residuals); #[ax.set_xlim(1.5) for ax in [acf_ax, kde_ax]] sns.despine() plt.tight_layout(); print(&quot;** Mean of the residuals: &quot;, np.around(resid_mean,2)) print(&quot; n** Ljung Box Test, p-value:&quot;, np.around(lj_p_val,3), &quot;(&gt;0.05, Uncorrelated)&quot; if (lj_p_val &gt; 0.05) else &quot;(&lt;0.05, Correlated)&quot;) print(&quot; n** Jarque Bera Normality Test, p_value:&quot;, np.around(norm_p_val,3), &quot;(&gt;0.05, Normal)&quot; if (norm_p_val&gt;0.05) else &quot;(&lt;0.05, Not-normal)&quot;) print(&quot; n** AD Fuller, p_value:&quot;, np.around(adfuller_p,3), &quot;(&gt;0.05, Non-stationary)&quot; if (adfuller_p &gt; 0.05) else &quot;(&lt;0.05, Stationary)&quot;) return ts_ax, acf_ax, kde_ax def accuracy(y1,y2): accuracy_df=pd.DataFrame() rms_error = np.round(rmse(y1, y2),1) map_error = np.round(np.mean(np.abs((np.array(y1) - np.array(y2)) / np.array(y1))) * 100,1) accuracy_df=accuracy_df.append({&quot;RMSE&quot;:rms_error, &quot;%MAPE&quot;: map_error}, ignore_index=True) return accuracy_df def plot_pgram(series,diff_order): &quot;&quot;&quot; This function plots thd Power Spectral Density of a de-trended series. PSD should also be calculated for a de-trended time series. Enter the order of differencing needed Output is a plot with PSD on Y and Time period on X axis Series: Pandas time series or np array differencing_order: int. Typically 1 &quot;&quot;&quot; #from scipy import signal de_trended = series.diff(diff_order).dropna() f, fx = signal.periodogram(de_trended) freq=f.reshape(len(f),1) #reshape the array to a column psd = fx.reshape(len(f),1) # plt.figure(figsize=(5, 4) plt.plot(1/freq, psd ) plt.title(&quot;Periodogram&quot;) plt.xlabel(&quot;Time Period&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.tight_layout() . . Importing Data . path = &#39;https://raw.githubusercontent.com/pawarbi/datasets/master/timeseries/ts_frenchretail.csv&#39; #Sales numbers are in thousands, so I am dividing by 1000 to make it easier to work with numbers, especially squared errors data = pd.read_csv(path, parse_dates=True, index_col=&quot;Date&quot;).div(1_000) data.index.freq=&#39;Q&#39; data.head() . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . . Note: I have explicitly set the index frequency to quarterly. This makes plotting and analyzing data with pandas and statsmodels easier. Many methods in Statsmodels have freq argument. Setting the frequency explicitly will pass the value automatically. More date offsets can be found in Pandas documentation here. freq=&amp;#8217;Q-DEC&amp;#8217; in the index.freq below shows quarterly data ending in December. Other advantage of setting the .freq value is that if the dates are not continous, Pandas will throw an error, which can be used to fix the data quality error and make the series continuos. Other common date offsets are: - Monthly Start: &#39;MS&#39; . Quarterly Start: &#39;QS&#39; | Weekly: &#39;W&#39; | Bi Weekly: &#39;2W&#39; | Business/ Weekday: &#39;B&#39; | Hourly: &#39;H&#39; | . data.index . DatetimeIndex([&#39;2012-03-31&#39;, &#39;2012-06-30&#39;, &#39;2012-09-30&#39;, &#39;2012-12-31&#39;, &#39;2013-03-31&#39;, &#39;2013-06-30&#39;, &#39;2013-09-30&#39;, &#39;2013-12-31&#39;, &#39;2014-03-31&#39;, &#39;2014-06-30&#39;, &#39;2014-09-30&#39;, &#39;2014-12-31&#39;, &#39;2015-03-31&#39;, &#39;2015-06-30&#39;, &#39;2015-09-30&#39;, &#39;2015-12-31&#39;, &#39;2016-03-31&#39;, &#39;2016-06-30&#39;, &#39;2016-09-30&#39;, &#39;2016-12-31&#39;, &#39;2017-03-31&#39;, &#39;2017-06-30&#39;, &#39;2017-09-30&#39;, &#39;2017-12-31&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, freq=&#39;Q-DEC&#39;) . Train Test Split: . Before analyzing the data, first split it into train and test (hold-out) for model evaluation. All the EDA and model fitting/selection should be done first using train data. Never look at the test sample until later to avoid any bias. Typically we want at least 3-4 full seasonal cycles for training, and test set length should be no less than the forecast horizon. . In this example, we have 24 observations of the quarterly data, which means 6 full cycles (24/4). Our forecast horizon is 4 quarters. So training set should be more than 16 and less than 20. I will use first 18 observations for training and keep last 6 for validation.Note that I am always selecting the last 6 values for test by using .iloc[:-6]. As we get more data, this will ensure that last 6 values are always for validation. Unlike typical train/test split, we can not shuffle the data before splitting to retain the temporal structure. . Cross-validation: . Data can be split using the above method or using cross-validation where the series is split into number of successive segments and the model is tested using one-step ahead forecast. Model accuracy in that case is based on the mean of the cross-validation errors over the number of splits used. This minimizes chances of overfitting. Be sure to include at least 1-2 seasonal periods to capture the seasonality. e.g. in this case, the first training set of the CV should be min 8 values so the model has captured seasonal behaviour from 2 years. This is the preferred method when the time series is short. . Our series has 24 obervations so I can use last 6-8 for validation. When the typical train/test split is used, always check the sensisitivity of the model performance and model parameters to train/test size. If AIC or AICc is used for model evaluation, it approximatley approaches cross-validation error asymptotically. I will cover this in Part 2 with the code and example. . . #Split into train and test train = data.iloc[:-6] test = data.iloc[-6:] #forecast horizon h = 6 train_length = len(train) print(&#39;train_length:&#39;,train_length, &#39; n test_length:&#39;, len(test) ) . train_length: 18 test_length: 6 . train . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . 2013-06-30 409.0 | . 2013-09-30 498.0 | . 2013-12-31 387.0 | . 2014-03-31 473.0 | . 2014-06-30 513.0 | . 2014-09-30 582.0 | . 2014-12-31 474.0 | . 2015-03-31 544.0 | . 2015-06-30 582.0 | . 2015-09-30 681.0 | . 2015-12-31 557.0 | . 2016-03-31 628.0 | . 2016-06-30 707.0 | . Exploratory Data Analysis &amp; Modeling Implications . These are some of the questions I ask at various stages of model building. . Are there any null values? how many? best way to impute the null data? If null/NaNs are present, first identify why the data is missing and if NaNs mean anything. Missing values can be filled by interpolation, forward-fill or backward-fill depending on the data and context. Also make sure null doesnt mean 0, which is acceptable but has modeling implications. | It&#39;s important to understand how the data was generated (manual entry, ERP system), any transformations, assumptions were made before providing the data. | . | Are the data/dates continuous? In this exmaple I am only looking at continous time-series. There other methods that deal with non-continuous data. ETS &amp; ARIMA require the data to be continuous. If the series is not continuous, we can add dummy data or use interpolation. | . | Are there any duplicate dates, data? Remove the duplicates or aggregate the data (e.g. average or mean) to treat duplicates | . | Any &#39;potential&#39; outliers? . Outliers are defined as observations that differ significantly from the general observations. Identify if the data is susceptible to outliers/spikes, if outliers mean anything and how to define outliers. While &#39;Outlier Detection&#39; is a topic in itself, in forecasting context we want to treat outliers before the data is used for fitting the model. Both ETS and ARIMA class of models (especially ARIMA) are not robust to outliers and can provide erroneous forecasts. Data should be analyzed while keeping seasonality in mind. e.g. a sudden spike could be because of the seasonal behaviour and not be outlier. Do not confuse outlier with &#39;influential data&#39;. . | Few ways to treat outliers: . Winsorization: Use Box and whiskers and clip the values tha exceed 1 &amp; 99th percentile (not preferred) | Use residual standard deviation and compare against observed values (preferred but can&#39;t do a priori) | Use moving average to check spikes/troughs (iterative and not robust) | . | Another important reason to pay close attention to outliers is that we will choose the appropriate error metric based on that. There are many error metrics used to assess accuracy of forecasts, viz. MAE, MSE, RMSE, %MAPE, %sMAPE. If outliers are present, don&#39;t use RMSE because the squaring the error at the outlier value can inflate the RMSE. In that case model should be selected/assessed using %MAPE or %sMAPE. More on that in part 2. . | . | Visually any trend, seasonality, cyclic behaviour? This will help us choose the appropriate model (Single, Double, Triple Exponential Smoothing, ARIMA/SARIMA) | If cyclic behiour is present (seasonality is short-order variation e.g. month/quarter, cyclicity occurs over 10-20 years e.g. recession) we will need to use different type of decomposition (X11, STL). Depending on the context and purpose of analysis, seasoanlity adjustment may also be needed. | If multiple seasonalities are present, ETS or ARIMA cannot be used. SSA, TBATS, harmonic regression are more appropriate in that case. FB Prophet can also help with multiple seasonalities. | Frequency of seasonality is important. ETS &amp; SARIMAX are not appropriate for high frequency data such as hourly, daily, sub-daily and even weekly. Consider using SSA,TBTAS, FB Prophet, deep learning models. | . | How does the data change from season to season for each period and period to period and compared to the level? Does it increas/decrease with the trend? Changes slowly, rapidly or remains constant. This is an important observation to be made, especially for ETS model, as it can determine the parametrs to be used &amp; if any preprocessing will be needed. | De-compose the series into level, trend, seasonal components and residual error. Observe the patterns in the decomposed series. | Is the trend constant, growing/slowing linearly or exponentially or some other non-linear function? | Is the seasonal pattern repetitive? | How is the seasonal pattern changing relative to level? If it is constant relative to level, it shows &quot;additive&quot; seasonality, whereas if it is growing, it&#39;s &quot;multiplicative&quot; | . | Distribution of the data? will we need any transformations? While normally distributed data is not a requirement for forecasting and doesnt necessarily improve point forecast accuracy, it can help stablize the variance and narrow the prediction interval. | Plot the histogram/KDE for each time period (e.g. each year and each seasona) to get gauge peakedness, spread in the data. It can also help compare different periods and track trends over time. | If the data is severely skewed, consider normalizing the data before training the model. Be sure to apply inverse transformation on the forecasts. Use the same transformation parameters on the train and test sets. Stabilizing the variance by using Box Cox transformation (special case being log &amp; inverse transform), power law etc can help more than normalizing the data. | Watch out for outliers before transformation as it will affect the transformation | Plottng distribution also helps track &quot;concept-drift&quot; in the data, i.e. does the underlying temporal structure / assumption change over time. If the drift is significant, refit the model or at least re-evaluate. This can be tricky in time series analysis. | Uncertainty in the training data will lead to higher uncertainty in the forecast. If the data is highly volatile/uncertain (seen by spread in the distribution, standard deviation, non-constant variance etc), ETS and ARIMA models will not be suitable. Consider GARCH and other methods. | . | Is the data stationary? Is this a white noise, random walk process? . Perhaps the most important concept to keep in mind when doing time series analysis and forecasting is that, time series is a probabilistic / stochastic process, and the time series we are analyzing is a &#39;realization of a stochastic process&#39;. A time signal could be deterministic or stochastic/probabilistic. In a deterministic process, the future values can be predicted exactly with a mathematical function e.g. y = sin(2$ pi$ft). In our case, the future values can only be expressed in terms of probability distribution. The point estimates are mean/median of the distribution. By definition, the mean has a distribution around it and as such the stakeholders should be made aware of the probabilistic nature of the forecast through uncertainty estimates. . | Stationarity: Statistical stationarity means the time series has constant mean, variance and autocorrelation is insignificant at all lags. Autocorrelation is a mouthful, all it means is the correlation with its past self. e.g. to check if two variables are linearly correlated with each other, we calculate their coeff of correlation (Pearson correlation). Similarly, autocorrelation does the same thing but with its past values (i.e lags). More on that later. For a stationary time series, the properties are the same no matter which part of the series (w.r.t time) we look at. This is a core concept of the ARIMA methods, as only stationary processes can be modeled using ARIMA. ETS can handle non-stationary processes. . | White Noise: If a time series has zero mean and a constant variance , i.e. N(0,$ sigma^2$), it&#39;s a white noise. The variables in this case are independent and identically distributed (i.i.d) and are uncorrelated. We want the residuals left after fitting the model to be a white noise. White noise can be identified by using ADFuller test and plotting autocorrelation function (ACF) plots. In an ACF plot, the autocorrelation should be insignificant (inside the 95% CI band) at all lags. | Random Walk: Random walks are non-stationary. It&#39;s mean or variance or both change over time. Random walk cannot be forecast because we have more unknowns than the data so we will end up having way too many parameters in the model. In essence, random walk has no pattern to it, it&#39;s last data point plus some random signal (drift). Thus, if the first difference of the time series results in a white noise, it&#39;s an indication of a Random Walk. e.g. most equity stocks are random walk but by looking at percent difference (%growth over time) we can study the white noise. `Next Data point = Previous Data Point + Random Noise` `Random Noise = Next Data Point - Previous Data Point` . | . | . . Note: It&#8217;s easy to mistake randomness for seasonality. In the random walk chart below, it can appear that the data has some seasonality but does not! . #collapse-hide #create white noise with N(0,1.5), 500 points np.random.seed(578) steps = np.random.normal(0,1,500) noise = pd.DataFrame({&quot;x&quot;:steps}) wnoise_chart = alt.Chart(noise.reset_index()).mark_line().encode( x=&#39;index&#39;, y=&#39;x&#39;).properties( title=&quot;White Noise&quot;) #Create random walk with N(0,1.5), 500 points steps[0]=0 rwalk = pd.DataFrame({&quot;x&quot;:100 + np.cumsum(steps)}).reset_index() rwalk_chart = alt.Chart(rwalk).mark_line().encode( x=&#39;index&#39;, y=alt.Y(&#39;x&#39;, scale=alt.Scale(domain=(80,150)))).properties( title=&quot;Random Walk&quot;) wnoise_chart | rwalk_chart . . Auto-correlation? at what lag? Study the second order properties (autocorrelation and power spectral density) of the time series along with mean, standard deviation, distribution. More details below. | . | If trend is present, momentum or mean-reversing? . Time series with momentum indicates the value tends to keep going up or down (relative to trend) depending on the immediate past. Series with mean-reversion indicates it will go up (or down) if it has gone down (or up) in the immediate past. This can be found by examining the coefficients of the ARIMA model. This provides more insight into the process and builds intuition. This doesnt not directly help with forecasting. | . | Break-points in the series? . Are there any structural breaks (shifts) in the series. Structural breaks are abrupt changes in the trend. Gather more information about the sudden changes. If the breaks are valid, ETS/ARIMA models wont work. FB Prophet, dynamic regression, deep learning models, adding more features might help. Identify the possible reasons for change, e.g. change in macros, price change, change in customer preferenaces etc. Note structural change persists for some time, while outliers do not. Break points are different from non-stationarity. Read more here for examples &amp; explanations. In case of structural break-points, consider modeling the segments of the series separately. | . | Intermittent demand? Time series is said to be intermittent when there are several 0 and small values (not nulls) in the series. ETS and ARIMA are not appropriate for this type of time series. It&#39;s a common pattern with inventory time series, especially for new items. Croston&#39;s method is one approach to use for forecasting intermittent demand. | When demand is intermittent, use RMSE rather than %MAPE as the evaluation metric. With %MAPE, the denominator would be 0 leading to erroneous results. | . | #collapse-hide #creating intermittent demand plot demand = [10, 12, 0, 3,50,0,0,18,0,4, 12,0,0,8,0,3] demanddf = pd.DataFrame({&#39;y&#39;: demand, &#39;x&#39;: np.arange(2000, 2016) } ) alt.Chart(demanddf).mark_bar().encode( x=&#39;x&#39;, y=&#39;y&#39;).properties( title=&quot;Example: Intermittent Demand&quot;, width = 700) . . Do we need any exogenous variables/external regressors? It may be necessary to include additional features/variables to accurately capture the time series behaviour. For example, the sales for a retailer might be higher on weekends, holidays, when it&#39;s warmer etc. This is different from seasonal pattern. In such cases, using the &#39;day of the week&#39; or &#39;is_holiday&#39; feature might provide better forecast. ETS models cannot use exogenous variable. SARIMAX (X is for exogenous), deep learning, XGB models are more suited. | Always inspect the residuals after fitting the model. If the residuals are correlated (use ACF/PACF plots, Ljung Box test on residuals), it&#39;s an indication that we are not capturing the time series behaviour accurately and could try adding exogenous behaviour. | . | Are the stakeholders interested in forecast for invidiuals periods or hierarchical forecast? Typically forecasts are made for individual periods, e.g in this example, we are interested in the forecasts for next 4 quarters. But it&#39;s possible that the business leaders might be more interested in the 1 year forecast rather than 4 quarters. We could combine the forecasts from 4 quarters to calculate the forecast for 1 year, but that would be incorrect. As mentioned above, time series is a statistical process with probability distribution. We can get reasonable value by summing the mean forecasts but the uncertainty around those forecasts cannot be added. Also, if the individual forecasts are based on the median (rather than mean), forecasts cannot be added. We will need to calculate &quot;hierarchical forecast* by simulating the future paths and then adding the distributions to get prediction interval. | . | Are forecast explainability &amp; interpretability important? Many traditional and statistical forecasting methods such as ETS, SARIMA are easy to apply, interprete and model parameters/results can help explain the time series behaviour. This can be important in scenarios where such insights can help make business decisions. e.g if an ETS model with damped trend fits better, it can be an indication of slowing growth etc. However, many other models such deep learning, RNN, S2S, LSTM etc are blackbox approaches that may lead to higher accuracy but provide little-to-no explainability. | . | EDA in Pyhton . Data Integrity / Quality . #Any missing data? print(&quot;missing_data:&quot;, train.isna().sum()) print(&quot;unique dates:&quot;, train.index.nunique()) . missing_data: Sales 0 dtype: int64 unique dates: 18 . #Counting number of values for each quarter and Year. Columsn are quarters. #Here each qquarter and year has 1 value, thus no duplicates pd.crosstab(index=train.index.year, columns=train.index.quarter) . col_0 1 2 3 4 . row_0 . 2012 1 | 1 | 1 | 1 | . 2013 1 | 1 | 1 | 1 | . 2014 1 | 1 | 1 | 1 | . 2015 1 | 1 | 1 | 1 | . 2016 1 | 1 | 0 | 0 | . Observations: . No null values | Length of the train set is 18 and we have 12 unique dates/quarters so no duplicate dates | Each year and quarter has 1 observation, so no duplicates and data is continuous | Time Series . Plotting the time series and the 4 quarter rolling mean using Altair. . . Tip: Matplotlib and Seaborn create static charts, whereas plots created with Altair are interactive. You can hover over the data points to read tooltips. The most useful feature is the ability to zoom-in and out. Time series data can be dense and it&#8217;s important to check each time period to get insights. With zoom-in/out, it can be done iteractively without slicing the time series. Altair&#8217;s documentation and example library is great. . #collapse-hide #Create line chart for Training data. index is reset to use Date column train_chart=alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;Date&#39;, y=&#39;Sales&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]) #Create Rolling mean. This centered rolling mean rolling_mean = alt.Chart(train.reset_index()).mark_trail( color=&#39;orange&#39;, size=1 ).transform_window( rolling_mean=&#39;mean(Sales)&#39;, frame=[-4,4] ).encode( x=&#39;Date:T&#39;, y=&#39;rolling_mean:Q&#39;, size=&#39;Sales&#39; ) #Add data labels text = train_chart.mark_text( align=&#39;left&#39;, baseline=&#39;top&#39;, dx=5 # Moves text to right so it doesn&#39;t appear on top of the bar ).encode( text=&#39;Sales:Q&#39; ) #Add zoom-in/out scales = alt.selection_interval(bind=&#39;scales&#39;) #Combine everything (train_chart + rolling_mean +text).properties( width=600, title=&quot;French Retail Sales &amp; 4Q Rolling mean ( in &#39;000)&quot;).add_selection( scales ) . . Sub-series plot . Sub-series plot to show how the series behaves each year in all seasons (quarterly or monthly) . #collapse-hide alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;quarter(Date)&#39;, y=&#39;Sales&#39;, column=&#39;year(Date)&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]).properties( title=&quot;Sales: Yearly Subseries plot&quot;, width=100).configure_header( titleColor=&#39;black&#39;, titleFontSize=14, labelColor=&#39;blue&#39;, labelFontSize=14 ) . . #box plot to see distribution of sales in each year fig, ax = plt.subplots(figsize = (12,8)) sns.boxplot(data=train, x=train.index.year, y = &#39;Sales&#39;, ax = ax, boxprops=dict(alpha=.3)); sns.swarmplot(data=train, x=train.index.year, y = &#39;Sales&#39;); . #%Growth each year. Excluding 2016 since we have only 2 quarters growth = train[:&#39;2015&#39;].groupby(train[:&#39;2015&#39;].index.year)[&quot;Sales&quot;].sum().pct_change() growth . Date 2012 NaN 2013 0.102632 2014 0.218377 2015 0.157689 Name: Sales, dtype: float64 . Observations: . Sales has gone up each year from 2012-2015 =&gt;Positive Trend present. | Typically, Sales goes up from Q1 to Q3, peaks in Q3, drops in Q4. Definitely a seasoanl pattern. =&gt; Model should capture seasonality and trend. | Just comparing Q4 peaks, sales has gone up from $432K to $582K =&gt; Trend exists, Model should capture trend. No cyclic behaviour | Overall data looks clean, no observations outside of IQR =&gt; Clean data, no outliers | No structural breaks, intermittent pattern =&gt; ETS and SARIMA may be used | Notice that the length of the bar in box plot increases from 2012-2015. =&gt; Mean &amp; variance increasing, we will need to stabilize the variance by taking log or using Box Cox transform | Quarterly trends &amp; distrbution . Quarterly sub-series plot to see how the series behaves in each quarter across alll years. . #collapse-hide alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;year(Date)&#39;, y=&#39;Sales&#39;, column=&#39;quarter(Date)&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]).properties( title=&quot;Sales: Quarterly Subseries plot&quot;, width=100).configure_header( titleColor=&#39;black&#39;, titleFontSize=14, labelColor=&#39;blue&#39;, labelFontSize=14 ) . . . Tip: Statsmodels has a quarter_plot() method that can be used to create similar chart easily. . #Quarterly plot: Shows trend for Q1-Q4 for each of the years. Red line shows mean quarter_plot(train); . Distribution of Sales in each year . #collapse-hide #Distribution plot of each year compared with overall distribution sns.distplot(train, label=&#39;Train&#39;, hist=False, kde_kws={&quot;color&quot;: &quot;g&quot;, &quot;lw&quot;: 3, &quot;label&quot;: &quot;Train&quot;,&quot;shade&quot;:True}) sns.distplot(train[&#39;2012&#39;], label=&#39;2012&#39;, hist=False) sns.distplot(train[&#39;2013&#39;], label=&#39;2013&#39;, hist=False) sns.distplot(train[&#39;2014&#39;], label=&#39;2014&#39;, hist=False) sns.distplot(train[&#39;2015&#39;], label=&#39;2015&#39;, hist=False); . . In this case the heatmap feels redundant but when the series is long, heatmap can reveal more patterns . #collapse-hide sns.heatmap(pd.pivot_table(data=train, index=train.index.year, columns=train.index.quarter), square=True, cmap=&#39;Blues&#39;, xticklabels=[&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;]); . . Visualizing the quarterly sales for each year as % . #collapse-hide #As stacked bar chart, in % values. stack1= alt.Chart(train[:&#39;2015&#39;].reset_index()).mark_bar().encode( x=alt.X(&#39;sum(Sales)&#39;), y=&#39;year(Date):N&#39;, color=alt.Color( &#39;quarter(Date)&#39;, scale=alt.Scale(scheme=&#39;category10&#39;)), tooltip=[&quot;Date&quot;, &quot;Sales&quot;]).properties( height=100, width = 300, title = &quot;Sum of Sales by each Quarter&quot;) stack2= alt.Chart(train[:&#39;2015&#39;].reset_index()).mark_bar().encode( x=alt.X(&#39;sum(Sales)&#39;, stack=&#39;normalize&#39;), y=&#39;year(Date):N&#39;, color=alt.Color( &#39;quarter(Date)&#39;, scale=alt.Scale(scheme=&#39;category10&#39;)), tooltip=[&quot;Date&quot;, &quot;Sales&quot;] ).properties( height=100, width = 300, title = &quot;Sum of Sales as % by each Quarter&quot;) stack1 | stack2 . . pie= train[:&#39;2015&#39;].groupby(train[:&#39;2015&#39;].index.quarter)[&quot;Sales&quot;].sum().plot.bar( title=&quot;Total Sales by Quarter 2012-2015&quot;, legend=True, label=&quot;Sales each Quarter&quot;) . Seasonality Factor . This will help us understand how much each quarter contributes relative to the average demand. Note that this should be done on a de-trended series(taking first difference) but because we don&#39;t have enough data and for a quick demonstration, I am using the series as is. . #Groupby Sales by Quarter #Only use upto 2015 because we have partial data for 2016 train_2015=train[:&#39;2015&#39;] avg_2015= np.int(train[:&#39;2015&#39;].mean()) #Avg sales per quarter qrt_avg=train_2015.groupby(train_2015.index.quarter)[&quot;Sales&quot;].mean() #Groupby quarter qrt_table = pd.pivot_table(train_2015, index=train_2015.index.quarter, columns=train_2015.index.year) #add qrt_avg to qrt_table qrt_table[&quot;avg&quot;] = qrt_avg #Additive Seasonality Factor: Subtract mean from avg column qrt_table[&quot;additive&quot;] = qrt_table[&quot;avg&quot;]-avg_2015 #Multiplicative Seasonality Factor: Subtract mean from avg column qrt_table[&quot;multiplicative&quot;] = (qrt_table[&quot;avg&quot;]/avg_2015).round(2) qrt_table.index.name=&quot;Quarters&quot; qrt_table . Sales avg additive multiplicative . Date 2012 2013 2014 2015 . Quarters . 1 362.0 | 382.0 | 473.0 | 544.0 | 440.25 | -34.75 | 0.93 | . 2 385.0 | 409.0 | 513.0 | 582.0 | 472.25 | -2.75 | 0.99 | . 3 432.0 | 498.0 | 582.0 | 681.0 | 548.25 | 73.25 | 1.15 | . 4 341.0 | 387.0 | 474.0 | 557.0 | 439.75 | -35.25 | 0.93 | . Observations: . Quarter plot &amp; heatmap confirm peak in Q3, drop in Q4. | For each of the years the upward trend observed in all quarters | Kenel Density plot shows data looks normally distributed, bi-modal distribution in quarters is because of small sample size. Peaks shift right from 2012 to 2015 indicating increase in average. | Distribution becomes fatter as the years progress, indicating higher spread/variation (as seen above in boxplot too) | Though The sales peak in Q3 each year, as a % of annual sales, all quarters contribute roughly the same | Seasonal factor analysis shows that in 3rd quarter we see that sales jump up by 15% (or $73K) relative to average while in other three quartersit drop sby 1-7%. This is great for intuitive understanding of series behaviour. Another key takeaway from this analysis is that sales is not stable, as is evident from the charts above, multiplicative seasonality would capture the pattern better than additive seasonality. This insight will come handy when we create ETS model (part 2). We could also reduce the variance by taking the log so errors are additive. | Decomposition . We will de-compose the time series into trend, seasonal and residuals . . Tip: Always use a semicolon (;) after plotting any results from statsmodels. For some reason if you don&#8217;t, it will print the plots twice. Also, by default the statsmodels plots are small and do not have a figsize() argument. Use rcParams() to define the plot size . decompose = seasonal_decompose(train[&quot;Sales&quot;]) decompose.plot(); plt.rcParams[&#39;figure.figsize&#39;] = (12, 8); . Observations: . Trend is more than linear, notice a small upward take off after 2013-07. Also notice that trend is projecting upward and does not seem to be dampening/leveling off. | Seasonal pattern is consistent | Resduals are whetever is left after fitting the trend and seasonal components to the observed data. It&#39;s the component we cannot explain. We want the residuals to be i.i.d (i.e uncorrelated). If the residuals have a pattern, it means there still some structural information left to be captured. Residuals are showing some wavy pattern, which is not good. Let&#39;s perform Ljung Box test to confirm if they are i.i.d as a group. | We do not want to see any recognizable patterns in the residuals, e.g. waves, upward/downward slope, funnel pattern etc. | ljung_p = np.mean(ljung(x=decompose.resid.dropna())[1]).round(3) print(&quot;Ljung Box, p value:&quot;, ljung_p, &quot;, Residuals are uncorrelated&quot; if ljung_p&gt;0.05 else &quot;, Residuals are correlated&quot;) . Ljung Box, p value: 0.184 , Residuals are uncorrelated . Residuals are uncorrelated. If the residuals are correlated, we can perform transformations to see if it stabilizes the variance. It&#39;s also an indication that we may need to use exogenous variable to fully explain the time series behaviour or use higher order models. . . Note: Ljung Box test tests the residuals as a group. Some residuals may have significant lag but as a group, we want to make sure they are uncorrelated. . Second Order Properties of the time series . We study the second order properties to understand - . is the data stationary | is the data white noise, random walk? i.e are the lags correlated? | quantify seasonal/cyclic behviour | . Stationarity: . For the series to be stationary, it must have: . constant mean | constant variance | constant covariance (uncorrelated) | . We verify this by observing change in mean, variance, autocorrelation and with a statistical test (ADFuller test) . Is the mean constant? . train.plot(figsize=(12,6), legend=True, label=&quot;Train&quot;, cmap=&#39;gray&#39;) train[&quot;Sales&quot;].rolling(4, center=False).mean().plot(legend=True, label=&quot;Rolling Mean 4Q&quot;); print(&quot;Mean is:&quot;, train[&quot;Sales&quot;].mean()) . Mean is: 496.5 . Notice that each year, the difference between the mean and the max in Q3 increases. This can potentially mean multiplicative seasonality. . Is the variance constant? . train[&quot;Sales&quot;].rolling(4).std().plot(legend=True, label=&quot;Rolling Std Deviation 4Q&quot;); print(&quot;S.D is:&quot;, train[&quot;Sales&quot;].std().round(1)) . S.D is: 110.9 . Both mean and standard deviation are increasing, thus not stationary. . Coefficient of Variation: . Coefficient of variation gives us an idea about the variability in the process, especially when looking at sales and demand. Note that this should be used for relative comparison and does not have a strict statistical defition. It&#39;s very common measure in demand planning and inventory analytics. . c.v = s.d/mean . If C.V&lt;0.75 =&gt; Low Variability . If 0.75&lt;C.V&lt;1.3 =&gt; Medium Variability . If C.V&gt;1.3 =&gt; High Variability . cv = train[&quot;Sales&quot;].std()/train[&quot;Sales&quot;].mean() cv.round(2) . 0.22 . This is a low-variability process. . Is the covariance constant? . #Plot ACF and PACF using statsmodels plot_acf(train); plot_pacf(train); . ADFuller Test for stationarity . Augmented Dicky Fuller test is a statistical test for stionarity. If the p value is less than 0.05, the series is stationary, otherwise non-stationary. Use adfuller() from statsmodels . #Calculate ad fuller statistic adf = adfuller(train[&quot;Sales&quot;])[1] print(f&quot;p value:{adf}&quot;, &quot;, Series is Stationary&quot; if adf &lt;0.05 else &quot;, Series is Non-Stationary&quot;) . p value:0.9990329594016152 , Series is Non-Stationary . Observations: . ACF: ACF plot shows autocorrelation coeff is insignificant at all lag values (within the blue 95%CI band), except lag 1. When studying ACF plots, we look at these 4 things Are any lags significant, i.e outside the blue band of 95% CI. If they are, the series is correlated with itself at those lags. Note there is 5% chance that the lag shown as insignificant (ACF=0) is shows as significant. In our case, 1st lag is barely significant, indicating sales last quarter affect the sales this quarter. | How quickly do the bar lenghts change: If the bars are taping down, that shows presence of trend. Our series has a trend | Pattern: If the ACF shows up/down repeating pattern, it means seasonality with size equal to length of repetition. | Sign of ACF: Alternating signs in ACF shows mean-reversing process whereas if all the ACs are positive (or negative), it shows momentum process. | . | Properties of ACF help us determine the order of the MA process. More on that in part 2. . PACF: Partial autocorrelation, similar to partial correlation, shows correlation after &#39;partialing out&#39; previous lags. If a series has PACF significant at lag k, it means controlling for other lags &lt;k, lag k has a significant correlation. PACF plot is used to determine order of AR process. . | ADFuller test shows that the series is not stationary. We can try to make it stationary by differencing it. . | #De-trending de_trended = train.diff(1).dropna() adf2 = adfuller(de_trended)[1] print(f&quot;p value:{adf2}&quot;, &quot;, Series is Stationary&quot; if adf2 &lt;0.05 else &quot;, Series is Non-Stationary&quot;) de_trended.plot(); . p value:1.544700848240804e-18 , Series is Stationary . By taking the first difference we de-trended the series and it has become stationary! . Autocovariance function vs autocorrelation fucntion The autocovariance measures the linear dependence between two points on the same series observed at different times. Very smooth series exhibit autocovariance functions that stay large even when the t and s are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations. autocorrelation functions (ACF) measures the predictability (linear), and is the normalized autocovariance. ACF, just like a correlation coeff, is between [-1,1] and is easier to interprete. Both measure linear dependence between random variables. . For example, the scatterplot below shows the train[&quot;Sales&quot;] plotted against it&#39;s first lag. We can see a linear, although weak, relationship between them. Use pandas.plotting.lag_plot() . Lag plot, 1st lag . pd.plotting.lag_plot(train[&quot;Sales&quot;],1); . Lag plot, 2nd lag : Weak relationship . pd.plotting.lag_plot(train[&quot;Sales&quot;],2); . . Note: If you wish to study the lags, you can obtain it by .shift() method. . sns.scatterplot(train[&quot;Sales&quot;], train[&quot;Sales&quot;].shift(-1)); . . Important: Use statsmodels for calculating ACF and NOT pandas pd.Series.autocorr() . Statsmodels and R use mean differencing, i.e subtract the mean of the entire series before summing it up, whereas Pandas uses Pearson correlation to calculate the ACF. Pearson correlation uses mean of the subseries rather than the entire series. For a long time series, the difference between the two should be negligible but for a short series, the diffrenece could be significant. In most cases, we are more interested in the pattern in the ACF than the actual values so, in a practical sense either would work. But, to be consistent and accurate use statsmodels to calculate and plot the ACF. . Which frequencies are prominent? . We typically look at the time series in the time domain. But, we can also analyze the time series in the frequency domain. It&#39;s based on the assumption that it is made up of sine and cosine waves of different frequencies. This helps us detect periodic component of known/unknown frequencies. It can show additional details of the time series that can be easily missed. We do it with a Periodogram and Power Spectral Density plot. . Periodogram: We analyze frequency and associated intensity of frequency. Note that below I have invrted the frequency to obtain periods Period,T = 1/frequency. For example, a monthly time series has 12 seasonal periods, so we would obtain frequency = 1/12 = 0.0833. In our example, we expect to see the intensity to be high at period=4 . . Tip: Periodogram should be plotted for a de-trended time series. Time series can be obtained by differencing the series . plot_pgram(train[&quot;Sales&quot;],1); . Power Spectral Density: Periodogram assumes the frequencies to be harmonics of the fundamental frequency, whereas PSD allows the frequency to vary contunuously. PSD is calculated using autocovariance function (ACF seen above). Spectral density is the amount of variance per frequency interval. PSD shows the eaxct same information of the time series as the ACF, just in the frequency domain. We rarely use PSD for business time series analysis. Plot below shows that lower frequency content dominates the time series. If it had another bump at higher frequency, that would indicate cyclic behaviour. Sometime it can be easier to figure out MA vs AR process by looking at the PSD plot. . #Plot PSD plt.psd(train[&quot;Sales&quot;], detrend=&#39;linear&#39;); plt.title(&quot;PSD Plot&quot;); . Is the series Normal? . As mentioned above, series does not have to be Gaussian for accurate forecasting but if the data is highly skewed it can affect the model selection and forecast uncertainty. In general, if the series is non-gaussian, it should be normalized before any further transformations (differencing, log, Box Cox) at least to check if normalization helps. Normalization will also help if decide to use regression, tree-based models, NN models later. Note that with normalization, we make the z score between [0,1], with standardization on the other hand , we center the distribution with mean =0, s.d.=1. . Normality can be checked visually by plotting the density plot, q-q plot and Shapiro-Wilk test. . #Distribution Plot sns.distplot(train[&quot;Sales&quot;]); . #Q-Q Plot sm.qqplot(train[&quot;Sales&quot;], fit=True, line=&#39;45&#39;, alpha=0.5, dist=&#39;norm&#39; ); . #Jarque Bera Stastical Test for Normality from scipy.stats import jarque_bera as jb is_norm=jb(train[&quot;Sales&quot;])[1] print(f&quot;p value:{is_norm}&quot;, &quot;, Series is Normal&quot; if is_norm &gt;0.05 else &quot;, Series is Non-Normal&quot;) . p value:0.5912668357500079 , Series is Normal . Observations: . Q-Q plot shows the data follows the 45deg line very closely, deviates slightly in the left tail. | Shapiro-Wilk test shows the data is from Normal distribution | Summary . Model explainability is as important as model accuracy. We will keep above insights in mind when choosing, fitting, evaluating and selecting the model. We will choose the model that we can explain based on above insights. It is important to be able to explain the time series behavour in qualitative and quantitative terms. . In Part 2, I will cover model fitting, selection and ensemble forecasting . EDA in R . Forecasting Principles and Practice by Prof. Hyndmand and Prof. Athanasapoulos is the best and most practical book on time series analysis. Most of the concepts discussed in this blog are from this book. Below is code to run the forecast() and fpp2() libraries in Python notebook using rpy2 . import rpy2 import warnings warnings.filterwarnings(&#39;ignore&#39;) from rpy2.robjects import pandas2ri import rpy2.rinterface as rinterface pandas2ri.activate() %load_ext rpy2.ipython . %%R -i data,train -o r_train library(fpp2) r_train &lt;- ts(train$Sales, start=c(2012,01), frequency=4) r_train %&gt;% autoplot() + ggtitle(&quot;French Retail&quot;) +xlab(&quot;Year-Quarter&quot;)+ylab(&quot;Retail Sales&quot;) . %Rpush r_train . %%R -i r_train r_train %&gt;% ggsubseriesplot() . Lag Plots . %%R r_train %&gt;% gglagplot() . ACF Plots . %%R r_train %&gt;% ggAcf() . R shows 3 lags to be significant, whereas in Python we saw only the first lag to be significant. I am not sure why. Just to confirm, I am did the analysis in JMP statistical software which I use at work for statistical analysis. Below are the results. It matches with Python&#39;s results - 1st lag to be significant, spectral density plot matches too. . . Outlier detection . %%R -o olier olier &lt;- r_train %&gt;% tsoutliers() . print(olier, end=&quot;&quot;) . $index integer(0) $replacements numeric(0) . using tsoutliers() does not return any results, showing there are no statistical outliers . Summary . Here is a summary of what we have learned about this time series: . There are no null values, outliers or duplicate values in the series. Series is continuous, non-intermittent. No structural breaks. We don&#39;t have to do any cleaning. | Series has a trend. Sales have increased every year. It looks more than linear but less than exponential. We might need to try Box Cox transform. | Series has seasonality with seasonal periods = 4. Peak in Q3, drops in Q4, ramps up from Q1 to Q3. No other dominant periods. No cyclic behaviour. | Avg sales per quarter is 497, and S.D is 111, low variability. We will use this to guage the model error relative to mean and S.D. | Since it is not a high-frequency data and has fixed seasonality, we can use ETS class of models | SARIMA could be used. We will need at least 1 differencing as de-trended series was stationary | Mean, variance, covariance are not constant. Series is not stationary, not white noise and not random-walk. | Variance increasing with time. Highs also look to be increasing relative to mean (rolling avg). &#39;multiplicative&#39; seasonality might fit better in ETS model | Series is not normal but did not look highly skewed | 1st lag was significant, though barely (ACF~0.5) | We do not have any information on exogenous variables | As there are no outliers and series is not intermittent, we can use RSME for model evaluation | We never looked at the test data. All the EDA and model building must be done using the training set. Intuitively, given what we have observed in the training set, we expect the forecast to be on the upward trend with slightly mupltiplicative seasonality. Trend does not seems to ve leveling off. | References: . Forecasting: Principles and Practice, by Prof. Hyndman | Time Series Analysis and its Applications, by Robert Shumway | Time Series Analysis and Forecasting, by Montgomery &amp; Jennings | Introduction to Time Series and Analysis, by Brockwell | Practial Time Series Forecasting with R, by Galit Shmueli |",
            "url": "https://pawarbi.github.io/blog/forecasting/r/python/rpy2/altair/2020/04/15/timeseries-part1.html",
            "relUrl": "/forecasting/r/python/rpy2/altair/2020/04/15/timeseries-part1.html",
            "date": "  Apr 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Time series Forecasting in Python & R, Part 1 (EDA)",
            "content": "Project Goals . This is a quarterly sales data of a French retail company from Prof. Rob Hyndman&#39;s &quot;Forecasting Methods &amp; Applications&quot; book. I have uploaded the data to my github. I chose this example because it&#39;s deceptively simple, easy to explain/demonstrate key concepts and Prof. Hyndman later applied the state space approach to this series using family of ETS models. The goals for this first part are: . Exploratory data analysis of the time series | Explain the time series behaviour in qualitative and quantitative terms to build intution for model selection | Identify the models that can be used based on the findings in the EDA | Importing libraries . #collapse-hide #Author: Sandeep Pawar #Version: 1.0 #Date Mar 27, 2020 import pandas as pd import numpy as np import itertools #Plotting libraries import matplotlib.pyplot as plt import seaborn as sns import altair as alt plt.style.use(&#39;seaborn-white&#39;) %matplotlib inline #statistics libraries import statsmodels.api as sm import scipy from scipy.stats import anderson from statsmodels.tools.eval_measures import rmse from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import month_plot, seasonal_plot, plot_acf, plot_pacf, quarter_plot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing from statsmodels.stats.diagnostic import acorr_ljungbox as ljung #from nimbusml.timeseries import SsaForecaster from statsmodels.tsa.statespace.tools import diff as diff import pmdarima as pm from pmdarima import ARIMA, auto_arima from scipy import signal from scipy.stats import shapiro from scipy.stats import boxcox from sklearn.preprocessing import StandardScaler #library to use R in Python import rpy2 from rpy2.robjects import pandas2ri pandas2ri.activate() import warnings warnings.filterwarnings(&quot;ignore&quot;) np.random.seed(786) . . . Note: I have found that results could be significanlty different if you use different versions of the libraries, especially with statsmodels. If you want to reproduce these results, be sure to use the same versions of these libraries. For this project, I created a conda virtual environment as rpy2 requires specific versions of Pandas &amp; certain R libraries . #Printing library versions print(&#39;Pandas:&#39;, pd.__version__) print(&#39;Statsmodels:&#39;, sm.__version__) print(&#39;Scipy:&#39;, scipy.__version__) print(&#39;Rpy2:&#39;, rpy2.__version__) . Pandas: 0.25.0 Statsmodels: 0.11.0 Scipy: 1.4.1 Rpy2: 2.9.4 . #collapse-hide # Define some custom functions to help the analysis def MAPE(y_true, y_pred): &quot;&quot;&quot; %Error compares true value with predicted value. Lower the better. Use this along with rmse(). If the series has outliers, compare/select model using MAPE instead of rmse() &quot;&quot;&quot; y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 def residcheck(residuals, lags): &quot;&quot;&quot; Function to check if the residuals are white noise. Ideally the residuals should be uncorrelated, zero mean, constant variance and normally distributed. First two are must, while last two are good to have. If the first two are not met, we have not fully captured the information from the data for prediction. Consider different model and/or add exogenous variable. If Ljung Box test shows p&gt; 0.05, the residuals as a group are white noise. Some lags might still be significant. Lags should be min(2*seasonal_period, T/5) from scipy.stats import anderson https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html anderson().statistic &lt;0.05 =&gt; Normal distribution plots from: https://tomaugspurger.github.io/modern-7-timeseries.html &quot;&quot;&quot; resid_mean = np.mean(residuals) lj_p_val = min(ljung(x=residuals, lags=lags)[1]) norm_p_val = shapiro(residuals)[1] adfuller_p = adfuller(residuals)[1] fig = plt.figure(figsize=(10,8)) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2); acf_ax = plt.subplot2grid(layout, (1, 0)); kde_ax = plt.subplot2grid(layout, (1, 1)); residuals.plot(ax=ts_ax) plot_acf(residuals, lags=lags, ax=acf_ax); sns.kdeplot(residuals); #[ax.set_xlim(1.5) for ax in [acf_ax, kde_ax]] sns.despine() plt.tight_layout(); print(&quot;** Mean of the residuals: &quot;, resid_mean) print(&quot; n** Ljung Box Test, p-value:&quot;, lj_p_val, &quot;(&gt;0.05, Uncorrelated)&quot; if (lj_p_val &gt; 0.05) else &quot;(&lt;0.05, Correlated)&quot;) print(&quot; n** Shapiro Normality Test, p_value:&quot;, norm_p_val, &quot;(&gt;0.05, Normal)&quot; if (norm_p_val&gt;0.05) else &quot;(&lt;0.05, Not-normal)&quot;) print(&quot; n** AD Fuller, p_value:&quot;, adfuller_p, &quot;(&gt;0.05, Non-stationary)&quot; if (adfuller_p &gt; 0.05) else &quot;(&lt;0.05, Stationary)&quot;) return ts_ax, acf_ax, kde_ax def accuracy(y1,y2): accuracy_df=pd.DataFrame() rms_error = np.round(rmse(y1, y2),1) map_error = np.round(np.mean(np.abs((np.array(y1) - np.array(y2)) / np.array(y1))) * 100,1) accuracy_df=accuracy_df.append({&quot;RMSE&quot;:rms_error, &quot;%MAPE&quot;: map_error}, ignore_index=True) return accuracy_df def plot_pgram(series,diff_order): &quot;&quot;&quot; This function plots thd Power Spectral Density of a de-trended series. PSD should also be calculated for a de-trended time series. Enter the order of differencing needed Output is a plot with PSD on Y and Time period on X axis Series: Pandas time series or np array differencing_order: int. Typically 1 &quot;&quot;&quot; #from scipy import signal de_trended = series.diff(diff_order).dropna() f, fx = signal.periodogram(de_trended) freq=f.reshape(len(f),1) #reshape the array to a column psd = fx.reshape(len(f),1) # plt.figure(figsize=(5, 4) plt.plot(1/freq, psd ) plt.title(&quot;Periodogram&quot;) plt.xlabel(&quot;Time Period&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.tight_layout() . . Importing Data . path = &#39;https://raw.githubusercontent.com/pawarbi/datasets/master/timeseries/ts_frenchretail.csv&#39; #Sales numbers are in thousands, so I am dividing by 1000 to make it easier to work with numbers, especially squared errors data = pd.read_csv(path, parse_dates=True, index_col=&quot;Date&quot;).div(1000) data.index.freq=&#39;Q&#39; data.head() . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . . Note: I have explicitly set the index frequency to quarterly. This makes plotting and analyzing data with pandas and statsmodels easier. Many methods in Statsmodels have freq argument. Setting the frequency explicitly will pass the value automatically. More date offsets can be found in Pandas documentation here. freq=&amp;#8217;Q-DEC&amp;#8217; in the index.freq below shows quarterly data ending in December. Other advantage of setting the .freq value is that if the dates are not continous, Pandas will throw an error, which can be used to fix the data quality error and make the series continuos. Other common date offsets are: - Monthly Start: &#39;MS&#39; . Quarterly Start: &#39;QS&#39; | Weekly: &#39;W&#39; | Bi Weekly: &#39;2W&#39; | Business/ Weekday: &#39;B&#39; | Hourly: &#39;H&#39; | . data.index . DatetimeIndex([&#39;2012-03-31&#39;, &#39;2012-06-30&#39;, &#39;2012-09-30&#39;, &#39;2012-12-31&#39;, &#39;2013-03-31&#39;, &#39;2013-06-30&#39;, &#39;2013-09-30&#39;, &#39;2013-12-31&#39;, &#39;2014-03-31&#39;, &#39;2014-06-30&#39;, &#39;2014-09-30&#39;, &#39;2014-12-31&#39;, &#39;2015-03-31&#39;, &#39;2015-06-30&#39;, &#39;2015-09-30&#39;, &#39;2015-12-31&#39;, &#39;2016-03-31&#39;, &#39;2016-06-30&#39;, &#39;2016-09-30&#39;, &#39;2016-12-31&#39;, &#39;2017-03-31&#39;, &#39;2017-06-30&#39;, &#39;2017-09-30&#39;, &#39;2017-12-31&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, freq=&#39;Q-DEC&#39;) . Train Test Split: . Before analyzing the data, first split it into train and test (hold-out) for model evaluation. All the EDA and model fitting/selection should be done first using train data. Never look at the test sample until later to avoid any bias. Typically we want at least 3-4 full seasonal cycles for training, and test set length should be no less than the forecast horizon. . In this example, we have 24 observations of the quarterly data, which means 6 full cycles (24/4). Our forecast horizon is 4 quarters. So training set should be more than 16 and less than 20. I will use first 18 observations for training and keep last 6 for validation.Note that I am always selecting the last 6 values for test by using .iloc[:-6]. As we get more data, this will ensure that last 6 values are always for validation. Unlike typical train/test split, we can not shuffle the data before splitting to retain the temporal structure. . Cross-validation: . Data can be split using the above method or using cross-validation where the series is split into number of successive segments and the model is tested using one-step ahead forecast. Model accuracy in that case is based on the mean of the cross-validation errors over the number of splits used. This minimizes chances of overfitting. Be sure include at least 1-2 seasonal periods to capture the seasonality. e.g. in this case, the first training set of the CV should be min 8 values so the model has captured seasonal behaviour from 2 years. This is the preferred method when the time series is short. . Our series has 24 obervations so I can use last 6-8 for validation. When this method is used, always check the sensisitivity of the model performance and model parameters to train/test size. If AIC or AICc is used for model evaluation, it approximatley approaches cross-validation error asymptotically. . . #Split into train and test train = data.iloc[:-6] test = data.iloc[-6:] #forecast horizon h = 6 train_length = len(train) print(&#39;train_length:&#39;,train_length, &#39; n test_length:&#39;, len(test) ) . train_length: 18 test_length: 6 . train . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . 2013-06-30 409.0 | . 2013-09-30 498.0 | . 2013-12-31 387.0 | . 2014-03-31 473.0 | . 2014-06-30 513.0 | . 2014-09-30 582.0 | . 2014-12-31 474.0 | . 2015-03-31 544.0 | . 2015-06-30 582.0 | . 2015-09-30 681.0 | . 2015-12-31 557.0 | . 2016-03-31 628.0 | . 2016-06-30 707.0 | . Exploratory Data Analysis &amp; Modeling Implications . These are some of the questions I ask at various stages of model building. . Are there any null values? how many? best way to impute the null data? If null/NaNs are present, first identify why the data is missing and if NaNs mean anything. Missing values can be filled by interpolation, forward-fill or backward-fill depending on the data and context. Also make sure null doesnt mean 0, which is acceptable but has modeling implications. | It&#39;s important to understand how the data was generated (manual entry, ERP system), any transformations, assumptions were made before providing the data. | . | Are the data/dates continuous? In this exmaple I am only looking at continous time-series. There other methods that deal with non-continuous data. ETS &amp; ARIMA require the data to be continuous. If the series is not continuous, we can add dummy data or use interpolation. | . | Are there any duplicate dates, data? Remove the duplicates or aggregate the data (e.g. average or mean) to treat duplicates | . | Any &#39;potential&#39; outliers? . Outliers are defined as observations that differ significantly from the general observations. Identify if the data is susceptible to outliers/spikes, if outliers mean anything and how to define outliers. While &#39;Outlier Detection&#39; is a topic in itself, in forecasting context we want to treat outliers before the data is used for fitting the model. Both ETS and ARIMA class of models (especially ARIMA) are not robust to outliers and can provide erroneous forecasts. Data should be analyzed while keeping seasonality in mind. e.g. a sudden spike could be because of the seasonal behaviour and not be outlier. Do not confuse outlier with &#39;influential data&#39;. . | Few ways to treat outliers: . Winsorization: Use Box and whiskers and clip the values tha exceed 1 &amp; 99th percentile (not preferred) | Use residual standard deviation and compare against observed values (preferred but can&#39;t do a priori) | Use moving average to check spikes/troughs (iterative and not robust) | . | Another important reason to pay close attention to outliers is that we will choose the appropriate error metric based on that. There are many error metrics used to assess accuracy of forecasts, viz. MAE, MSE, RMSE, %MAPE, %sMAPE. If outliers are present, don&#39;t use RMSE because the squaring the error at the outlier value can inflate the RMSE. In that case model should be selected/assessed using %MAPE or %sMAPE. More on that later. . | . | Visually any trend, seasonality, cyclic behaviour? This will help us choose the appropriate model (Single, Double, Triple Exponential Smoothing, ARIMA/SARIMA) | If cyclic behiour is present (seasonality is short-order variation e.g. month/quarter, cyclicity occurs over 10-20 years e.g. recession) we will need to use different type of decomposition (X11, STL). Depending on the context and purpose of analysis, seasoanlity adjustment may also be needed. | If multiple seasonalities are present, ETS or ARIMA cannot be used. SSA, TBATS, harmonic regression are more appropriate in that case. FB Prophet can also help with multiple seasonalities. | Frequency of seasonality is important. ETS &amp; SARIMAX are not appropriate for high frequency data such as hourly, daily, sub-daily and even weekly. Consider using SSA,TBTAS, FB Prophet, deep learning models. | . | How does the data change from season to season for each period and period to period and compared to the level? Does it increas/decrease with the trend? Changes slowly, rapidly or remains constant. This is an important observation to be made, especially for ETS model, as it can determine the parametrs to be used &amp; if any preprocessing will be needed. | De-compose the series into level, trend, seasonal components and residual error. Observe the patterns in the decomposed series. | Is the trend constant, growing/slowing linearly or exponentially or some other non-linear function? | Is the seasonal pattern repetitive? | How is the seasonal pattern changing relative to level? If it is constant relative to level, it shows &quot;additive&quot; seasonality, whereas if it is growing, it&#39;s &quot;multiplicative&quot; | . | Distribution of the data? will we need any transformations? While normally distributed data is not a requirement for forecasting and doesnt necessarily improve point forecast accuracy, it can help stablize the variance and narrow the prediction interval. | Plot the histogram/KDE for each time period (e.g. each year and each seasona) to get gauge peakedness, spread in the data. It can also help compare different periods and track trends over time. | If the data is severely skewed, consider normalizing the data before training the model. Be sure to apply inverse transformation on the forecasts. Use the same transformation parameters on the train and test sets. Stabilizing the variance by using Box Cox transformation (special case being log &amp; inverse transform), power law etc can help more than normalizing the data. | Watch out for outliers before transformation as it will affect the transformation | Plottng distribution also helps track &quot;concept-drift&quot; in the data, i.e. does the underlying temporal structure / assumption change over time. If the drift is significant, refit the model or at least re-evaluate. This can be tricky in time series analysis. | Uncertainty in the training data will lead to higher uncertainty in the forecast. If the data is highly volatile/uncertain (seen by spread in the distribution, standard deviation, non-constant variance etc), ETS and ARIMA models will not be suitable. Consider GARCH and other methods. | . | Is the data stationary? Is this a white noise, random walk process? . Perhaps the most important concept to keep in mind when doing time series analysis and forecasting is that, time series is a probabilistic / stochastic process, and the time series we are analyzing is a &#39;realization of a stochastic process&#39;. A time signal could be deterministic or stochastic/probabilistic. In a deterministic process, the future values can be predicted exactly with a mathematical function e.g. y = sin(2$ pi$ft). In our case, the future values can only be expressed in terms of probability distribution. The point estimates are mean/median of the distribution. By definition, the mean has a distribution around it and as such the stakeholders should be made aware of the probabilistic nature of the forecast through uncertainty estimates. . | Stationarity: Statistical stationarity means the time series has constant mean, variance and autocorrelation is insignificant at all lags. Autocorrelation is a mouthful, all it means is the correlation with its past self. e.g. to check if two variables are linearly correlated with each other, we calculate their coeff of correlation (Pearson correlation). Similarly, autocorrelation does the same thing but with its past values (i.e lags). More on that later. For a stationary time series, the properties are the same no matter which part of the series (w.r.t time) we look at. This is a core concept of the ARIMA methods, as only stationary processes can be modeled using ARIMA. ETS can handle non-stationary processes. . | White Noise: If a time series has zero mean and a constant variance , i.e. N(0,$ sigma^2$), it&#39;s a white noise. The variables in this case are independent and identically distributed (i.i.d) and are uncorrelated. We want the residuals left after fitting the model to be a white noise. White noise can be identified by using ADFuller test and plotting autocorrelation function (ACF) plots. In an ACF plot, the autocorrelation should be insignificant (inside the 95% CI band) at all lags. | Random Walk: Random walks are non-stationary. It&#39;s mean or variance or both change over time. Random walk cannot be forecast because we have more unknowns than the data so we will end up having way too many parameters in the model. In essence, random walk has no pattern to it, it&#39;s last data point plus some random signal (drift). Thus, if the first difference of the time series results in a white noise, it&#39;s an indication of a Random Walk. e.g. most equity stocks are random walk but by looking at percent difference (%growth over time) we can study the white noise. `Next Data point = Previous Data Point + Random Noise` `Random Noise = Next Data Point - Previous Data Point` . | . | . . Note: It&#8217;s easy to mistake randomness for seasonality. In the random walk chart below, it can appear that the data has some seasonality but does not! . #collapse-hide #create white noise with N(0,1.5), 500 points np.random.seed(578) steps = np.random.normal(0,1,500) noise = pd.DataFrame({&quot;x&quot;:steps}) wnoise_chart = alt.Chart(noise.reset_index()).mark_line().encode( x=&#39;index&#39;, y=&#39;x&#39;).properties( title=&quot;White Noise&quot;) #Create random walk with N(0,1.5), 500 points steps[0]=0 rwalk = pd.DataFrame({&quot;x&quot;:100 + np.cumsum(steps)}).reset_index() rwalk_chart = alt.Chart(rwalk).mark_line().encode( x=&#39;index&#39;, y=alt.Y(&#39;x&#39;, scale=alt.Scale(domain=(80,150)))).properties( title=&quot;Random Walk&quot;) wnoise_chart | rwalk_chart . . Auto-correlation? at what lag? Study the second order properties (autocorrelation and power spectral density) of the time series along with mean, standard deviation, distribution. More details below. | . | If trend is present, momentum or mean-reversing? . Time series with momentum indicates the value tends to keep going up or down (relative to trend) depending on the immediate past. Series with mean-reversion indicates it will go up (or down) if it has gone down (or up) in the immediate past. This can be found by examining the coefficients of the ARIMA model. This provides more insight into the process and builds intuition. This doesnt not directly help with forecasting. | . | Break-points in the series? . Are there any structural breaks (shifts) in the series. Structural breaks are abrupt changes in the trend. Gather more information about the sudden changes. If the breaks are valid, ETS/ARIMA models wont work. FB Prophet, dynamic regression, deep learning models, adding more features might help. Identify the possible reasons for change, e.g. change in macros, price change, change in customer preferenaces etc. Note structural change persists for some time, while outliers do not. Break points are different from non-stationarity. Read more here for examples &amp; explanations. In case of structural break-points, consider modeling the segments of the series separately. | . | Intermittent demand? Time series is said to be intermittent when there are several 0 and small values (not nulls) in the series. ETS and ARIMA are not appropriate for this type of time series. It&#39;s a common pattern with inventory time series, especially for new items. Croston&#39;s method is one approach to use for forecasting intermittent demand. | When demand is intermittent, use RMSE rather than %MAPE as the evaluation metric. With %MAPE, the denominator would be 0 leading to erroneous results. | . | #collapse-hide #creating intermittent demand plot demand = [10, 12, 0, 3,50,0,0,18,0,4, 12,0,0,8,0,3] demanddf = pd.DataFrame({&#39;y&#39;: demand, &#39;x&#39;: np.arange(2000, 2016) } ) alt.Chart(demanddf).mark_bar().encode( x=&#39;x&#39;, y=&#39;y&#39;).properties( title=&quot;Example: Intermittent Demand&quot;, width = 700) . . Do we need any exogenous variables/external regressors? It may be necessary to include additional features/variables to accurately capture the time series behaviour. For example, the sales for a retailer might be higher on weekends, holidays, when it&#39;s warmer etc. This is different from seasonal pattern. In such cases, using the &#39;day of the week&#39; or &#39;is_holiday&#39; feature might provide better forecast. ETS models cannot use exogenous variable. SARIMAX (X is for exogenous), deep learning, XGB models are more suited. | Always inspect the residuals after fitting the model. If the residuals are correlated (use ACF/PACF plots, Ljung Box test on residuals), it&#39;s an indication that we are not capturing the time series behaviour accurately and could try adding exogenous behaviour. | . | EDA . Data Integrity / Quality . #Any missing data? print(&quot;missing_data:&quot;, train.isna().sum()) print(&quot;unique dates:&quot;, train.index.nunique()) . missing_data: Sales 0 dtype: int64 unique dates: 18 . #Counting number of values for each quarter and Year. Columsn are quarters. #Here each qquarter and year has 1 value, thus no duplicates pd.crosstab(index=train.index.year, columns=train.index.quarter) . col_0 1 2 3 4 . row_0 . 2012 1 | 1 | 1 | 1 | . 2013 1 | 1 | 1 | 1 | . 2014 1 | 1 | 1 | 1 | . 2015 1 | 1 | 1 | 1 | . 2016 1 | 1 | 0 | 0 | . &lt;font color=#4d6eae &gt;Observations:&lt;/font&gt; . No null values | Length of the train set is 18 and we have 12 unique dates/quarters so no duplicate dates | Each year and quarter has 1 observation, so no duplicates and data is continuous | Time Series . Plotting the time series and the 4 quarter rolling mean using Altair. . . Tip: Matplotlib and Seaborn create static charts, whereas plots created with Altair are interactive. You can hover over the data points to read tooltips. The most useful feature is the ability to zoom-in and out. Time series data can be dense and it&#8217;s important to check each time period to get insights. With zoom-in/out, it can be done iteractively without slicing the time series. Altair&#8217;s documentation and example library is great. . #collapse-hide #Create line chart for Training data. index is reset to use Date column train_chart=alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;Date&#39;, y=&#39;Sales&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]) #Create Rolling mean. This centered rolling mean rolling_mean = alt.Chart(train.reset_index()).mark_trail( color=&#39;orange&#39;, size=1 ).transform_window( rolling_mean=&#39;mean(Sales)&#39;, frame=[-4,4] ).encode( x=&#39;Date:T&#39;, y=&#39;rolling_mean:Q&#39;, size=&#39;Sales&#39; ) #Add data labels text = train_chart.mark_text( align=&#39;left&#39;, baseline=&#39;top&#39;, dx=5 # Nudges text to right so it doesn&#39;t appear on top of the bar ).encode( text=&#39;Sales:Q&#39; ) #Add zoom-in/out scales = alt.selection_interval(bind=&#39;scales&#39;) #Combine everything (train_chart + rolling_mean +text).properties( width=600, title=&quot;French Retail Sales &amp; 4Q Rolling mean ( in &#39;000)&quot;).add_selection( scales ) . . Sub-series plot . Sub-series plot to show how the series behaves each year in all seasons (quarterly or monthly) . #collapse-hide alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;quarter(Date)&#39;, y=&#39;Sales&#39;, column=&#39;year(Date)&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]).properties( title=&quot;Sales: Yearly Subseries plot&quot;, width=100).configure_header( titleColor=&#39;black&#39;, titleFontSize=14, labelColor=&#39;blue&#39;, labelFontSize=14 ) . . #box plot to see distribution of sales in each year fig, ax = plt.subplots(figsize = (12,8)) sns.boxplot(data=train, x=train.index.year, y = &#39;Sales&#39;, ax = ax, boxprops=dict(alpha=.3)); sns.swarmplot(data=train, x=train.index.year, y = &#39;Sales&#39;); . &lt;matplotlib.axes._subplots.AxesSubplot at 0x176e1e45278&gt; . #%Growth each year. Excluding 2016 since we have only 2 quarters growth = train[:&#39;2015&#39;].groupby(train[:&#39;2015&#39;].index.year)[&quot;Sales&quot;].sum().pct_change() growth . Date 2012 NaN 2013 0.102632 2014 0.218377 2015 0.157689 Name: Sales, dtype: float64 . Observations: . Sales has gone up each year from 2012-2015 =&gt;Positive Trend present. | Typically, Sales goes up from Q1 to Q3, peaks in Q3, drops in Q4. Definitely a seasoanl pattern. =&gt; Model should capture seasonality and trend. | Just comparing Q4 peaks, sales has gone up from $432K to $582K =&gt; Trend exists, Model should capture trend. No cyclic behaviour | Overall data looks clean, no observations outside of IQR =&gt; Clean data, no outliers | No structural breaks, intermittent pattern =&gt; ETS and SARIMA may be used | Notice that the length of the bar in box plot increases from 2012-2015. =&gt; Mean &amp; variance increasing, we will need to stabilize the variance by taking log or using Box Cox transform | Quarterly trends &amp; distrbution . Quarterly sub-series plot to see how the series behaves in each quarter across alll years. . #collapse-hide alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;year(Date)&#39;, y=&#39;Sales&#39;, column=&#39;quarter(Date)&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]).properties( title=&quot;Sales: Quarterly Subseries plot&quot;, width=100).configure_header( titleColor=&#39;black&#39;, titleFontSize=14, labelColor=&#39;blue&#39;, labelFontSize=14 ) . . . Tip: Statsmodels has a quarter_plot() method that can be used to create similar chart easily. . #Quarterly plot: Shows trend for Q1-Q4 for each of the years. Red line shows mean quarter_plot(train); . Distribution of Sales in each year . #collapse-hide #Distribution plot of each year compared with overall distribution sns.distplot(train, label=&#39;Train&#39;, hist=False, kde_kws={&quot;color&quot;: &quot;g&quot;, &quot;lw&quot;: 3, &quot;label&quot;: &quot;Train&quot;,&quot;shade&quot;:True}) sns.distplot(train[&#39;2012&#39;], label=&#39;2012&#39;, hist=False) sns.distplot(train[&#39;2013&#39;], label=&#39;2013&#39;, hist=False) sns.distplot(train[&#39;2014&#39;], label=&#39;2014&#39;, hist=False) sns.distplot(train[&#39;2015&#39;], label=&#39;2015&#39;, hist=False); . . In this case the heatmap feels redundant but when the series is long, heatmap can reveal more patterns . #collapse-hide sns.heatmap(pd.pivot_table(data=train, index=train.index.year, columns=train.index.quarter), square=True, cmap=&#39;Blues&#39;, xticklabels=[&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;]); . . Visualizing the quarterly sales for each year as % . #collapse-hide #As stacked bar chart, in % values. stack1= alt.Chart(train[:&#39;2015&#39;].reset_index()).mark_bar().encode( x=alt.X(&#39;sum(Sales)&#39;), y=&#39;year(Date):N&#39;, color=alt.Color( &#39;quarter(Date)&#39;, scale=alt.Scale(scheme=&#39;category10&#39;)), tooltip=[&quot;Date&quot;, &quot;Sales&quot;]).properties( height=100, width = 300, title = &quot;Sum of Sales by each Quarter&quot;) stack2= alt.Chart(train[:&#39;2015&#39;].reset_index()).mark_bar().encode( x=alt.X(&#39;sum(Sales)&#39;, stack=&#39;normalize&#39;), y=&#39;year(Date):N&#39;, color=alt.Color( &#39;quarter(Date)&#39;, scale=alt.Scale(scheme=&#39;category10&#39;)), tooltip=[&quot;Date&quot;, &quot;Sales&quot;] ).properties( height=100, width = 300, title = &quot;Sum of Sales as % by each Quarter&quot;) stack1 | stack2 . . pie= train[:&#39;2015&#39;].groupby(train[:&#39;2015&#39;].index.quarter)[&quot;Sales&quot;].sum().plot.bar( title=&quot;Total Sales by Quarter 2012-2015&quot;, legend=True, label=&quot;Sales each Quarter&quot;) . Observations: . Quarter plot &amp; heatmap confirm peak in Q3, drop in Q4. | For each of the years the upward trend observed in all quarters | Kenel Density plot shows data looks normally distributed, bi-modal distribution in quarters is because of small sample size. Peaks shift right from 2012 to 2015 indicating increase in average. | Distribution becomes fatter as the years progress, indicating higher spread/variation (as seen above in boxplot too) | Though The sales peak in Q3 each year, as a % of annual sales, all quarters contribute roughly the same | Decomposition . We will de-compose the time series into trend, seasonal and residuals . . Tip: Always use a semicolon (;) after plotting any results from statsmodels. For some reason if you don&#8217;t, it will print the plots twice. Also, by default the statsmodels plots are small and do not have a figsize() argument. Use rcParams() to define the plot size . decompose = seasonal_decompose(train[&quot;Sales&quot;]) decompose.plot(); plt.rcParams[&#39;figure.figsize&#39;] = (12, 8) . Observations: . Trend is more than linear, notice a small upward take off after 2013-07. Also notice that trend is projecting upward and does not seem to be dampening/leveling off. | Seasonal pattern is consistent | Resduals are whetever is left after fitting the trend and seasonal components to the observed data. It&#39;s the component we cannot explain. We want the residuals to be i.i.d (i.e uncorrelated). If the residuals have a pattern, it means there still some structural information left to be captured. Residuals are showing some wavy pattern, which is not good. Let&#39;s perform Ljung Box test to confirm if they are i.i.d as a group. | We do not want to see any recognizable patterns in the residuals, e.g. waves, upward/downward slope, funnel pattern etc. | ljung_p = min(ljung(x=decompose.resid.dropna())[1]).round(3) print(&quot;Ljung Box, p value:&quot;, ljung_p, &quot;, Residuals are uncorrelated&quot; if ljung_p&gt;0.05 else &quot;, Residuals are correlated&quot;) . Ljung Box, p value: 0.072 , Residuals are uncorrelated . Residuals are uncorrelated. If the residuals are correlated, we can perform transformations to see if it stabilizes the variance. It&#39;s also an indication that we may need to use exogenous variable to fully explain the time series behaviour. . Second Order Properties of the time series . We study the second order properties to understand - . is the data stationary | is the data white noise, random walk? i.e are the lags correlated? | quantify seasonal/cyclic behviour | . Stationarity: . For the series to be stationary, it must have: . constant mean | constant variance | constant covariance (uncorrelated) | . We verify this by observing change in mean, variance, autocorrelation and with a statistical test (ADFuller test) . Is the mean constant? . train.plot(figsize=(12,6), legend=True, label=&quot;Train&quot;, cmap=&#39;gray&#39;) train[&quot;Sales&quot;].rolling(4, center=False).mean().plot(legend=True, label=&quot;Rolling Mean 4Q&quot;); print(&quot;Mean is:&quot;, train[&quot;Sales&quot;].mean()) . Mean is: 496.5 . Notice that each year, the difference between the mean and the max in Q3 increases. This can potentially mean multiplicative seasonality. . Is the variance constant? . train[&quot;Sales&quot;].rolling(4).std().plot(legend=True, label=&quot;Rolling Std Deviation 4Q&quot;); print(&quot;S.D is:&quot;, train[&quot;Sales&quot;].std().round(1)) . S.D is: 110.9 . Both mean and standard deviation are increasing, thus not stationary. . Coefficient of Variation: . Coefficient of variation gives us an idea about the variability in the process, especially when looking at sales and demand. NOte that this should be used for relative comparison and does not have a strict statistical defition. It&#39;s very common measure in demand planning and inventory analytics. . c.v = s.d/mean . If C.V&lt;0.75 =&gt; Low Variability . If 0.75&lt;C.V&lt;1.3 =&gt; Medium Variability . If C.V&gt;1.3 =&gt; High Variability . cv = train[&quot;Sales&quot;].std()/train[&quot;Sales&quot;].mean() cv . 0.22339170922484414 . This is a low-variability process. . Is the covariance constant? . #Plot ACF and PACF using statsmodels plot_acf(train); plot_pacf(train); . ADFuller Test for stationarity . Augmented Dicky Fuller test is a statistical test for stionarity. If the p value is less than 0.05, the series is stationary, otherwise non-stationary. Use adfuller() class from statsmodels . #Calculate ad fuller statistic adf = adfuller(train[&quot;Sales&quot;])[1] print(f&quot;p value:{adf}&quot;, &quot;, Series is Stationary&quot; if adf &lt;0.05 else &quot;, Series is Non-Stationary&quot;) . p value:0.9990329594016152 , Series is Non-Stationary . Observations: . ACF: ACF plot shows autocorrelation coeff is insignificant at all lag values (within the blue 95%CI band), except lag 1. When studying ACF plots, we look at these 4 things Are any lags significant, i.e outside the blue band of 95% CI. If they are, the series is correlated with itself at those lags. Note there is 5% chance that the lag shown as insignificant (ACF=0) is shows as significant. In our case, 1st lag is barely significant, indicating sales last quarter affect the sales this quarter. | How quickly do the bar lenghts change: If the bars are taping down, that shows presence of trend. Our series has a trend | Pattern: If the ACF shows up/down repeating pattern, it means seasonality with size equal to length of repetition. | Sign of ACF: Alternating signs in ACF shows mean-reversing process whereas if all the ACs are positive (or negative), it shows momentum process. | . | Properties of ACF help us determine the order of the MA process. More on that in future blog. . ADFuller test shows that the series is not stationary. We can try to make it stationary by differencing it. | #De-trending de_trended = train.diff(1).dropna() adf2 = adfuller(de_trended)[1] print(f&quot;p value:{adf2}&quot;, &quot;, Series is Stationary&quot; if adf2 &lt;0.05 else &quot;, Series is Non-Stationary&quot;) de_trended.plot(); . p value:1.5447008482407146e-18 , Series is Stationary . By taking the first difference we de-trended the series and it has become stationary! . Autocovariance function vs autocorrelation fucntion The autocovariance measures the linear dependence between two points on the same series observed at different times. Very smooth series exhibit autocovariance functions that stay large even when the t and s are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations. autocorrelation functions (ACF) measures the predictability (linear), and is the normalized autocovariance. ACF, just like a correlation coeff, is between [-1,1] and is easier to interprete. Both measure linear dependence between random variables. . For example, the scatterplot below shows the train[&quot;Sales&quot;] plotted against it&#39;s first lag. We can see a linear, although weak, relationship between them. Use pandas.plotting.lag_plot() . Lag plot, 1st lag . pd.plotting.lag_plot(train[&quot;Sales&quot;],1); . Lag plot, 2nd lag : Weak relationship . pd.plotting.lag_plot(train[&quot;Sales&quot;],2); . . Note: If you wish to study the lags, you can obtain it by .shift() method. . sns.scatterplot(train[&quot;Sales&quot;], train[&quot;Sales&quot;].shift(-1)); . . Important: Use statsmodels for calculating ACF and NOT pandas pd.Series.autocorr() . Statsmodels and R use mean differencing, i.e subtract the mean of the entire series before summing it up, whereas Pandas uses Pearson correlation to calculate the ACF. Pearson correlation uses mean of the subseries rather than the entire series. For a long time series, the difference between the two should be negligible but for a short series, the diffrenece could be significant. In most cases, we are more interested in the pattern in the ACF than the actual values so, in a practical sense either would work. But, to be consistent and accurate use statsmodels to calculate and plot the ACF. . Which frequencies are prominent? . We typically look at the time series in the time domain. But, we can also analyze the time series in the frequency domain. It&#39;s based on the assumption that it is made up of sine and cosine waves of different frequencies. This helps us detect periodic component of known/unknown frequencies. It can show additional details of the time series that can be easily missed. We do it with a Periodogram and Power Spectral Density plot. . Periodogram: We analyze frequency and associated intensity of frequency. Note that below I have invrted the frequency to obtain periods Period,T = 1/frequency. For example, a monthly time series has 12 seasonal periods, so we would obtain frequency = 1/12 = 0.0833. In our example, we expect to see the intensity to be high at period=4 . . Tip: Periodogram should be plotted for a de-trended time series. Time series can be obtained by differencing the series . plot_pgram(train[&quot;Sales&quot;],1); . Power Spectral Density: Periodogram assumes the frequencies to be harmonics of the fundamental frequency, whereas PSD allows the frequency to vary contunuously. PSD is calculated using autocovariance function (ACF seen above). Spectral density is the amount of variance per frequency interval. PSD shows the eaxct same information of the time series as the ACF, just in the frequency domain. We rarely use PSD for business time series analysis. Plot below shows that lower frequency content dominates the time series. If it had another bump at higher frequency, that would indicate cyclic behaviour. Sometime it can be easier to figure out MA vs AR process by looking at the PSD plot. . #Plot PSD plt.psd(train[&quot;Sales&quot;], detrend=&#39;linear&#39;); plt.title(&quot;PSD Plot&quot;); . Is the series Normal? . As mentioned above, series does not have to be Gaussian for accurate forecasting but if the data is highly skewed it can affect the model selection and forecast uncertainty. In general, if the series is non-gaussian, it should be normalized before any further transformations (differencing, log, Box Cox) at least to check if normalization helps. Normalization will also help if decide to use regression, tree-based models, NN models later. Note that with normalization, we make the z score between [0,1], with standardization on the other hand , we center the distribution with mean =0, s.d.=1. . Normality can be checked visually by plotting the density plot, q-q plot and Shapiro-Wilk test. . #Distribution Plot sns.distplot(train[&quot;Sales&quot;]); . #Q-Q Plot sm.qqplot(train[&quot;Sales&quot;], fit=True, line=&#39;45&#39;, alpha=0.5, dist=&#39;norm&#39; ); . #Stastical Test is_norm=shapiro(train[&quot;Sales&quot;])[1] print(f&quot;p value:{is_norm}&quot;, &quot;, Series is Normal&quot; if is_norm &gt;0.05 else &quot;, Series is Non-Normal&quot;) . p value:0.4259689450263977 , Series is Normal . &lt;font color=#4d6eae &gt;Observations:&lt;/font&gt; . Q-Q plot shows the data follows the 45deg line very closely, deviates slightly in the left tail. | Shapiro-Wilk test shows the data is from Normal distribution | Summary . Here is a summary of what we have learned about this time series: . There are no null values, outliers or duplicate values in the series. Series is continuous, non-intermittent. No structural breaks. We don&#39;t have to do any cleaning. | Series has a trend. Sales have increased every year. It looks more than linear but less than exponential. We might need to try Box Cox transform. | Series has seasonality with seasonal periods = 4. Peak in Q3, drops in Q4, ramps up from Q1 to Q3. No other dominant periods. No cyclic behaviour. | Avg sales per quarter is 497, and S.D is 111, low variability. We will use this to guage the model error relative to mean and S.D. | Since it is not a high-frequency data and has fixed seasonality, we can use ETS class of models | SARIMA could be used. We will need at least 1 differencing as de-trended series was stationary | Mean, variance, covariance are not constant. Series is not stationary, not white noise and not random-walk. | Variance increasing with time. Highs also look to be increasing relative to mean (rolling avg). &#39;multiplicative&#39; seasonality might fit better in ETS model | Series is not normal but did not look highly skewed | 1st lag was significant, though barely (ACF~0.5) | We do not have any information on exogenous variables | As there are no outliers and series is not intermittent, we can use RSME for model evaluation | We never looked at the test data. All the EDA and model building must be done using the training set. Intuitively, given what we have observed in the training set, we expect the forecast to be on the upward trend with slightly mupltiplicative seasonality. Trend does not seems to ve leveling off. | Model explainability is as important as model accuracy. We will keep above insights in mind when choosing, fitting, evaluating and selecting the model. We will choose the model that we can explain based on above insights. It is important to be able to explain the time series behavour in qualitative and quantitative terms. . Part 2: ETS Model Part 3: ARIMA Model Part 4: FB Prophet Part 5: Selecting the best model . Using R for time series EDA . Forecasting Principles and Practice by Prof. Hyndmand and Prof. Athanasapoulos is the best and most practical book on time series analysis. Most of the concepts discussed in this blog are from this book. Below is code to run the forecast() and fpp2() libraries in Python notebook using rpy2 . import rpy2 import warnings warnings.filterwarnings(&#39;ignore&#39;) from rpy2.robjects import pandas2ri import rpy2.rinterface as rinterface pandas2ri.activate() %load_ext rpy2.ipython . %%R -i data,train -o r_train library(fpp2) r_train &lt;- ts(train$Sales, start=c(2012,01), frequency=4) r_train %&gt;% autoplot() + ggtitle(&quot;French Retail&quot;) +xlab(&quot;Year-Quarter&quot;)+ylab(&quot;Retail Sales&quot;) . %Rpush r_train . %%R -i r_train r_train %&gt;% ggsubseriesplot() . Lag Plots . %%R r_train %&gt;% gglagplot() . ACF Plots . %%R r_train %&gt;% ggAcf() . R shows 3 lags to be significant, whereas in Python we saw only the first lag to be significant. I am not sure why. Just to confirm, I am did the analysis in JMP statistical software. Below are the results. It matches with Python&#39;s results - 1st lag to be significant, spectral density plot matches too. . . Outlier detection . %%R -o olier olier &lt;- r_train %&gt;% tsoutliers() . print(olier, end=&quot;&quot;) . $index integer(0) $replacements numeric(0) . using tsoutliers() does not return any results, showing there are no statistical outliers . References: . Forecasting: Principles and Practice, by Prof. Hyndman | Time Series Analysis and its Applications, by Robert Shumway | Time Series Analysis and Forecasting, by Montgomery &amp; Jennings | Introduction to Time Series and Analysis, by Brockwell |",
            "url": "https://pawarbi.github.io/blog/forecasting/r/python/rpy2/altair/2020/04/03/timeseries-part1.html",
            "relUrl": "/forecasting/r/python/rpy2/altair/2020/04/03/timeseries-part1.html",
            "date": "  Apr 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote. . 2. This is the other footnote. You can even have a link! .",
            "url": "https://pawarbi.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": "  Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a level 1 heading in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Heres a footnote 1. Heres a horizontal rule: . . Lists . Heres a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes and . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.&#8617; . |",
            "url": "https://pawarbi.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": "  Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Sandeep Pawar . My LinkedIn is Sandeep Pawar .",
          "url": "https://pawarbi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}