{
  
    
        "post0": {
            "title": "PowerShell Script To Mass Download Power BI Reports",
            "content": "Prerequisites . MicrosoftPowerBIMgmt module already installed | To download a report, you at least must be a Contributor to the worskpace | CSV file with two columns report_id and file_name. Report ID is the ID of the report from Power BI service and file name is what you want the file to be saved as. | This will only download the report along with the data; i.e. not as a thin report with live connection. This option hasn&#39;t been made available yet in PowerShell as of writing this blog (10/11/2022) | Be aware of limitations of downloading a report | The CSV file should be in the following format: . . Script . Import-Module MicrosoftPowerBIMgmt Connect-PowerBIServiceAccount #Specify location of the CSV file. The code expects two columns: &#39;report_id&#39; and &#39;file_name&#39; #Report_id is the Report ID of the Power BI report you want to download #File name is the name you want the report to be saved as $Csv = Import-Csv -Path &quot;C: Users User Downloads download reports.csv&quot; #Specify where to download the PBIX $path = &quot;C: Users User Downloads &quot; foreach ($Csv in $Csv) { $report= $Csv.report_id $file_name = $Csv.Outfile_file if ([string]::IsNullOrEmpty($report) -or ([string]::IsNullOrEmpty($file_name))) { Write-Output &quot;Report ID or File name missing&quot; } else { Write-Output &quot;Downloading.... &quot; $file_name Export-PowerBIReport -Id $report -OutFile $path$file_name } } . . I will update this script when it&#39;s possible to download the thin report using PowerShell. If you find any errors or have a better version, please let me know. .",
            "url": "https://pawarbi.github.io/blog/powerbi/powershell/automation/2022/10/11/powershell-download-powerbi-report.html",
            "relUrl": "/powerbi/powershell/automation/2022/10/11/powershell-download-powerbi-report.html",
            "date": " • Oct 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Function To Audit Datasources For Query Folding",
            "content": "Query Folding . I will assume you already know what is query folding. If you don&#39;t, please read this documentation or watch the session I presented few months ago. I have included more resources at the bottom. . Query Folding Audit . Imagine a scenario where you are revisiting one of your old reports or are helping a client optimize their dataset performance. One of the first things you will look at are which of the queries are folding. If the query is folding, Power BI will offload all or part of the transformations back to the datasource thus resulting in faster refreshes. If you just have couple of data sources, you can check it by right clicking or using other methods I described in the video above. But if you have 20-30 queries, which is not uncommon, it will take time. What if there was a quick way so you can prioritize which queries to check ? The functions I wrote below will do exactly that. . Value.Metadata . The function relies on two M functions. Value.Metadata which checks if the query is folding. And using #section to get list of all the queries. You can read more about Value.Metadata on Chris Webb&#39;s blog here. . Function 1: __QFIndicator . This function uses Value.Metadata to check if query is folding or is foldable. If it returns FALSE, either the datasource is not foldable (such as Excel, CSV etc.) or the query is not folding. . Note: Value.Metadata function may not always work. There are scenarios where the query could be folding but it still might return FALSE. BigQuery, OData are some examples of that. . // Save as __QFIndicator (tableName as table) =&gt; let fn = if (Value.Metadata(tableName)[QueryFolding][IsFolded] = true) then &quot;Query Folded Successfully&quot; else error Error.Record( &quot;Query Folding Warning&quot;, &quot;Query not folding&quot;, &quot;Either the query does not use native query (i.e SQL) or the transformations are not foldable. If you are using SQL, check the query logs&quot; ) in fn . Function 2: __QFAudit . This function applies the above __QFIndicator function on all the queries returned by the #Section function and gives you a sorted list. . // Save as __QFAudit (optional RunFn as text) =&gt; let Source = #sections[Section1], #&quot;Converted to Table&quot; = Record.ToTable(Source), #&quot;Removed Errors&quot; = Table.RemoveRowsWithErrors(#&quot;Converted to Table&quot;), #&quot;Added Custom&quot; = Table.AddColumn( #&quot;Removed Errors&quot;, &quot;IsTable&quot;, each [Value] is table, type logical ), #&quot;Filtered Rows&quot; = Table.SelectRows(#&quot;Added Custom&quot;, each ([IsTable] = true)), QFCheck = Table.AddColumn( #&quot;Filtered Rows&quot;, &quot;QF Check&quot;, each try __QFIndicator([Value]) otherwise Character.FromNumber(10060) &amp; &quot; Check the query&quot;, type text ), Result = Table.Sort(QFCheck, QF Check) in Result . . Steps . Just save these two functions in PowerQuery | Click Invoke on __QFAudit function | It will return a table that looks like below. | Note that the table doesn&#39;t say query is not folding. It&#39;s prompting you to check the query. In some cases your data source may not support folding and in other query could be folding but Value.Metadata may not be able to pick it up. That&#39;s just the limitation of the function. You will get the same results in Dataflow where Query Folding indicator is available. | . . Note: Another limitation here is that if you add a new query, you will have to invoke the function again. The results of the #section are not available in modeling fields and it won&#8217;t refresh automatically. This is a helper function. You can read more about it here. . Resources . My session on Query Folding : https://player.vimeo.com/video/714177719?h=466f2d1c1b | Chris Webb&#39;s Blog : https://blog.crossjoin.co.uk/2016/08/02/another-way-to-check-query-folding-in-power-bipower-query-m-code/ | 30 Day Query Folding Challenge : https://www.youtube.com/watch?v=9sV3hIn8VTY | Nikola&#39;s blog: https://data-mozart.com/query-folding-devil-is-in-the-detail/ | Query Folding Guidance: https://learn.microsoft.com/en-us/power-bi/guidance/power-query-folding | .",
            "url": "https://pawarbi.github.io/blog/powerbi/powerquery/queryfolding/2022/09/20/query-folding-audit-powerbi-powerquery.html",
            "relUrl": "/powerbi/powerquery/queryfolding/2022/09/20/query-folding-audit-powerbi-powerquery.html",
            "date": " • Sep 20, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Adding Prefix To Selected Columns In Power BI Using Tabular Editor",
            "content": "Adding Prefix . I had a date table with 50+ columns and all the columns needed to have &quot;Effective&quot; in the front. So instead of just Date, I needed it to be &quot;Effective Date&quot;. If it was just a couple of columns, I could rename them manually. We can also rename columns dynamically using M when the data is imported in PowerQuery. But imagine a scenario where you are importing one date table and you want to create another date table using calculated table (for role playing dimensions) where the new date table needs to have all the columns prefixed with &quot;Effective&quot; or &quot;Ship&quot; etc. That&#39;s where below techniques will be helpful. You will need to install Tabular Editor first. . . . 1. Rename + Regex . Open Tabular Editor and connect it to the Power BI Desktop instance | Select all the columns you want to rename | Press F2 or right click &gt; Batch Rename | Select &quot;Use Regular Expressions&quot; | In the Find option, enter w.+ . This will select start of the line. | In the Replace, enter Effective $&amp;. $&amp; will select the existing string. | Click Ok. | . 2. Using C# Script . Open Tabular Editor and connect it to the Power BI Desktop instance | In the C# Script window, enter below script | Select all the columns you want to rename | Run the script | --Script starts- . //1. Change the prefix below //2. Select all the columns to rename . var prefix = &quot;Effective &quot;; . foreach (var col in Selected.Columns) { . col.Name = prefix + col.Name; } . --Script ends . . Note here that we are editing the tabular object model which means these changes won&#39;t create a step in the Power Query. Depending on your scenario, data source, data quality etc. you may or may not want that. I showed this for columns but you can use it for renaming measures as well. For measures, change the C# script from Selected.Columns to Selected.Measures. . References . https://regexr.com/ | Useful C# scripts : https://docs.tabulareditor.com/te2/Useful-script-snippets.html |",
            "url": "https://pawarbi.github.io/blog/powerbi/tabulareditor/2022/08/30/adding-prefix-to-selected-columns-powerbi.html",
            "relUrl": "/powerbi/tabulareditor/2022/08/30/adding-prefix-to-selected-columns-powerbi.html",
            "date": " • Aug 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Selectively Highlighting Data Labels in Line Chart In Power BI",
            "content": "Conditionally Highlighting Data Labels . Conditional formatting is not new in Power BI however in August 2022 release a new option was added for conditionally highlighting data labels in charts. It works the exact same way by clicking on the &quot;fx&quot; button in formatting options. This opens new way of selectively highlighting data labels by changing colors and color transperancy. I have written about changing color transperancy using HSLA in my previous blog so I will directly show the the DAX and the before/after. . In the chart below, I have added conditional formatting to only show values that are above 10%. You can use any complex logic that can be written in DAX. . . DAX With HSLA . To pass color transperancy to the color format, use the alpha value in the HSLA. 0 means transperant and 1 is opaque. Values between 0 and 1 will show various degree of transperancy. . . Add the DAX measure to the &quot;fx&quot; as shown in the video below to get the desired effect. . Other Ways . There are several other ways to achive the same end result. You can read them here or watch below video: .",
            "url": "https://pawarbi.github.io/blog/powerbi/dataviz/2022/08/12/selective-highlighting-line-charts.html",
            "relUrl": "/powerbi/dataviz/2022/08/12/selective-highlighting-line-charts.html",
            "date": " • Aug 12, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Analysis of Broadband Internet Availability Across the US",
            "content": "Motivation . I recently came across the Internet Equity Initiative by the Data Science Institute at University of Chicago. They have been studying how disparity in access to internet affects communities across Chicago. Since the pandemic started, we have been working and studying from home. It is critical to have access to not just internet but high-speed broadband internet so families can succeed in this new era of remote work/education. Technology can be a great equalizer but if any disparities exist, it can affect certain communities and put them at a disdvatnage. Microsoft&#39;s AI For Good team has been championing the issue of digital equity for a while. They relesed a study and dataset in 2019 that showed that 163M americans lack access to broadband internet, which was significanly higher than FCC&#39;s estimate of 25M. . . I was curious to find out myself how much inequality, if any, exists not just around me but also across the US.In their analysis Microsoft looked at zip code level data. I was curious about two issues: . Does your access to broadband internet depend on where you in the US ? | Does median income of a community matter ? | Note here I am interested in access to the internet. A household may have means to purchase high speed internet but that does not mean they have high-speed internet in their area. So I am interested at the aggregate level, how much disparity exists. . Data . I will be using the data provided by Microsoft&#39;s AI For Good team under its Open Data initiative. This was a privatized data to protect privacy. You can read more about the data collection and methods used here. This dataset has zip code, county id and broadband usage percentage. Margin of error is also available but I will disregard that for initial analysis. The usage number shows how many households in that zip code had access to high speed internet (25MBPS+). I will combine this data with the US census data from 2020 to understand the distribution of usage across different areas in US. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import altair as alt import statsmodels.api as sm import statsmodels.formula.api as smf from IPython.display import IFrame . Broadband Usage Data . broadband_data_path = &#39;https://raw.githubusercontent.com/microsoft/USBroadbandUsagePercentages/master/dataset/broadband_data_zipcode.csv&#39; def clean_bbdf(path): &#39;&#39;&#39; Import data and clean column names &#39;&#39;&#39; df = pd.read_csv(broadband_data_path) df.columns = df.columns.str.replace(&quot; &quot;,&quot;_&quot;) return df.set_index(&#39;COUNTY_ID&#39;) bbdf = clean_bbdf(broadband_data_path) bbdf.head(3) . ST COUNTY_NAME POSTAL_CODE BROADBAND_USAGE ERROR_RANGE_(MAE)(+/-) ERROR_RANGE_(95%)(+/-) MSD . COUNTY_ID . 45001 SC | Abbeville | 29639 | 0.948 | 0.034 | 0.110 | 0.002 | . 45001 SC | Abbeville | 29620 | 0.398 | 0.002 | 0.007 | 0.000 | . 45001 SC | Abbeville | 29659 | 0.206 | 0.152 | 0.608 | 0.043 | . Check Data Quality . def data_quality(df): &#39;&#39;&#39; Check nulls and duplicates &#39;&#39;&#39; shape = df.shape is_na = df.isna().sum().any().sum() is_dupl = df[&#39;POSTAL_CODE&#39;].duplicated().any().sum() return print(f&#39; n Shape: {shape} , n Has nulls: {is_na}, n Has duplicates: {is_dupl}&#39;) data_quality(bbdf) . Shape: (32735, 7) , Has nulls: 0, Has duplicates: 1 . print(f&#39;Number of states: {bbdf.ST.nunique()}, &#39;) print(f&#39;Number of Counties: {bbdf.index.nunique()}&#39;) . Number of states: 51, Number of Counties: 3136 . Number of Households . The broadband data shows percentage of households with access to the broadband internet so the analysis has to be performed at the household level instead of population. I will use data from US Census to get the numbers. . #### Household Size hh_path = &#39;https://raw.githubusercontent.com/pawarbi/datasets/master/households_2020.csv&#39; households = (pd.read_csv(hh_path, usecols = [&#39;Geographic Area Name&#39;,&#39;Estimate!!Total!!HOUSEHOLDS!!Total households&#39;]) .assign(zipcode = lambda s:s.iloc[:,0].str[5:]) .dropna() ) households = (households[households[&#39;Geographic Area Name&#39;] !=&quot;United States&quot;] .drop(&#39;Geographic Area Name&#39;, axis=1) .astype(&#39;int&#39;) .set_index(&#39;zipcode&#39;) .rename(columns={&#39;Estimate!!Total!!HOUSEHOLDS!!Total households&#39;:&#39;households&#39;}) ) households.head(3) . households . zipcode . 601 5555 | . 602 12901 | . 603 19431 | . Next I will aggregate the data to get weighted usage for each county in the US. . bbdf_aggregate_usage = (bbdf.reset_index().set_index(&#39;POSTAL_CODE&#39;) .join(households) .assign(household_usage = lambda s:s[&#39;BROADBAND_USAGE&#39;] * s[&#39;households&#39;]) .drop([&#39;ERROR_RANGE_(MAE)(+/-)&#39;,&#39;ERROR_RANGE_(95%)(+/-)&#39;,&#39;MSD&#39;], axis=1) .groupby(&#39;COUNTY_ID&#39;) .agg( Combined_Usage = (&#39;household_usage&#39;,&#39;sum&#39;), Total_Households = (&#39;households&#39;, &#39;sum&#39;) ) ) bbdf_aggregate_usage.head(3) . Combined_Usage Total_Households . COUNTY_ID . 1001 8380.027 | 21450 | . 1003 38315.916 | 83928 | . 1005 2702.638 | 8842 | . bbdf_aggregate_usage[&#39;Usage&#39;] = (np.divide( bbdf_aggregate_usage[&#39;Combined_Usage&#39;].values, bbdf_aggregate_usage[&#39;Total_Households&#39;].values ) ) . &lt;ipython-input-62-cc14069b83c1&gt;:1: RuntimeWarning: invalid value encountered in true_divide bbdf_aggregate_usage[&#39;Usage&#39;] = (np.divide( . bbdf_aggregate_usage.head() . Combined_Usage Total_Households Usage . COUNTY_ID . 1001 8380.027 | 21450 | 0.390677 | . 1003 38315.916 | 83928 | 0.456533 | . 1005 2702.638 | 8842 | 0.305659 | . 1007 846.087 | 7307 | 0.115791 | . 1009 3253.546 | 16802 | 0.193640 | . Above is the aggregated broadband usage normalized by households. Next we need to differentiate each county whether it is rural, metro or urban. . Rural-urban_Continuum_Code_2003 . Rural-Urban Continuum Codes form a classification scheme that distinguishes metropolitan (metro) counties by the population size of their metro area, and nonmetropolitan (nonmetro) counties by degree of urbanization and adjacency to a metro area or areas. For more information from the USDA or to download the original data, go to their section on Rural-Urban Continuum Codes. . Source : https://seer.cancer.gov/seerstat/variables/countyattribs/ruralurban.html . Rural-Urban Continuum Codes Classification: . Metro counties: . 1 (Counties in metro areas of 1 million population or more) . 2 (Counties in metro areas of 250,000 to 1 million population) . 3 (Counties in metro areas of fewer than 250,000 population) . | Non-metro counties: . 4 (Urban population of 20,000 or more, adjacent to a metro area) . 5 (Urban population of 20,000 or more, not adjacent to a metro area) . 6 (Urban population of 2,500 to 19,999, adjacent to a metro area) . 7 (Urban population of 2,500 to 19,999, not adjacent to a metro area) . 8 (Completely rural or less than 2,500 urban population, adjacent to a metro area) . 9 (Completely rural or less than 2,500 urban population, not adjacent to a metro area) . 88 (Unknown-Alaska/Hawaii State/not official USDA Rural-Urban Continuum code) . 99 (Unknown/not official USDA Rural-Urban Continuum code) . | . rucc_map = { 1:&#39;Metro 1M&#39;, 2:&#39;Metro &lt;1M&#39;, 3:&#39;Metro &lt;.25M&#39;, 4:&#39;UMetro &gt;20K&#39;, 5:&#39;UNon-Metro &gt;20K&#39;, 6:&#39;UMetro &lt;20K&#39;, 7:&#39;UNon-Metro &lt;20K&#39;, 8:&#39;RMetro &lt;2.5K&#39;, 9:&#39;RNon-Metro &lt;2.5K&#39;, 88: &#39;UNKNWN&#39;, 99:&#39;UNKNWN&#39; } county_category = { 1:&#39;Metro&#39;, 2:&#39;Metro&#39;, 3:&#39;Metro&#39;, 4:&#39;Urban&#39;, 5:&#39;Urban&#39;, 6:&#39;Urban&#39;, 7:&#39;Urban&#39;, 8:&#39;Rural&#39;, 9:&#39;Rural&#39;, 88: &#39;UNKNWN&#39;, 99:&#39;UNKNWN&#39; } . rucc = (pov .loc[:,[&#39;FIPS_code&#39;,&#39;Stabr&#39;,&#39;Area_name&#39;,&#39;Rural-urban_Continuum_Code_2003&#39;,&#39;MEDHHINC_2020&#39;]] .dropna() .rename(columns = {&#39;Rural-urban_Continuum_Code_2003&#39;:&#39;rucc_codes&#39;, &#39;MEDHHINC_2020&#39;:&#39;Median_income&#39;}) .assign(rucc_codes = lambda s:s[&#39;rucc_codes&#39;].astype(&#39;int&#39;)) .assign(rucc_area = lambda s:s[&#39;rucc_codes&#39;].map(rucc_map)) .assign(county_category = lambda s:s[&#39;rucc_codes&#39;].map(county_category)) .set_index(&#39;FIPS_code&#39;) ) rucc.head(3) . Stabr Area_name rucc_codes Median_income rucc_area county_category . FIPS_code . 1001 AL | Autauga County | 2 | 67565.0 | Metro &lt;1M | Metro | . 1003 AL | Baldwin County | 4 | 71135.0 | UMetro &gt;20K | Urban | . 1005 AL | Barbour County | 6 | 38866.0 | UMetro &lt;20K | Urban | . bbdf_county_pov_usage = bbdf_aggregate_usage.join(rucc) bbdf_county_pov_usage.head(3) . Combined_Usage Total_Households Usage Stabr Area_name rucc_codes Median_income rucc_area county_category . COUNTY_ID . 1001 8380.027 | 21450 | 0.390677 | AL | Autauga County | 2.0 | 67565.0 | Metro &lt;1M | Metro | . 1003 38315.916 | 83928 | 0.456533 | AL | Baldwin County | 4.0 | 71135.0 | UMetro &gt;20K | Urban | . 1005 2702.638 | 8842 | 0.305659 | AL | Barbour County | 6.0 | 38866.0 | UMetro &lt;20K | Urban | . Distribution of Usage Across Area Types . sns.set(rc={&quot;figure.figsize&quot;:(15, 6)}) median_usage = (bbdf_county_pov_usage .groupby(&#39;rucc_area&#39;)[&#39;Usage&#39;] .median() .sort_values(ascending=False) .round(2) ) (sns.boxenplot( data = bbdf_county_pov_usage, y=&#39;Usage&#39;, x = &#39;rucc_area&#39; , order = median_usage.index , ).set(title = &quot;Aggregated Broadband Usage (25MBPS+) By Area Type n n High Population Areas Have More Access to Broadband Than Other&quot;) ); . (sns.kdeplot( data = bbdf_county_pov_usage, x=&#39;Usage&#39;, hue = &#39;rucc_area&#39;, palette= {&#39;Metro &lt;1M&#39; : &#39;#fc8d59&#39;, &#39;Metro 1M&#39;:&#39;#fc8d59&#39;, &#39;Metro &lt;.25M&#39;:&#39;#fc8d59&#39;, &#39;RMetro &lt;2.5K&#39;:&#39;blue&#39;, &#39;RNon-Metro &lt;2.5K&#39;:&#39;blue&#39;, &#39;UMetro &lt;20K&#39;:&#39;#d8daeb&#39;, &#39;UMetro &gt;20K&#39;:&#39;#d8daeb&#39;, &#39;UNon-Metro &gt;20K&#39;:&#39;#d8daeb&#39;, &#39;UNon-Metro &lt;20K&#39;:&#39;#d8daeb&#39;}, fill = True ).set(title = &quot;Distribution Of Boradband Usage (25MBPS+) By Area Type n n Significant Distinction in Access To Broadband Depending On Where You Live&quot;) ); plt.axvline(x=bbdf_county_pov_usage.Usage.median(), label=&#39;Median Usage&#39;, lw=4, color = &#39;gray&#39;); plt.text(x = bbdf_county_pov_usage.Usage.median() + 0.025, y = 0.45, s=&quot;Median Usage&quot;, color=&#39;#404040&#39;, size=&#39;large&#39;); plt.text(x = 0.75, y = 0.3, s=&quot;Metro&quot;, color=&#39;#fc8d59&#39;, size=&#39;medium&#39;); plt.text(x = -.05, y = 0.3, s=&quot;Rural&quot;, color=&#39;blue&#39;, size=&#39;medium&#39;); plt.text(x = 0.05, y = 0.45, s=&quot;Urban&quot;, color=&#39;#d8daeb&#39;, size=&#39;medium&#39;); . bbdf_county_pov_usage.groupby(&#39;rucc_area&#39;)[&#39;Usage&#39;].median().round(2) . rucc_area Metro 1M 0.67 Metro &lt;.25M 0.50 Metro &lt;1M 0.54 RMetro &lt;2.5K 0.17 RNon-Metro &lt;2.5K 0.20 UMetro &lt;20K 0.27 UMetro &gt;20K 0.49 UNon-Metro &lt;20K 0.32 UNon-Metro &gt;20K 0.52 Name: Usage, dtype: float64 . bbdf_county_pov_usage.groupby(&#39;county_category&#39;)[&#39;Usage&#39;].median().round(2) . county_category Metro 0.56 Rural 0.19 Urban 0.35 Name: Usage, dtype: float64 . Observations: . Above chart shows distribution of %usage by area types based on RUCC. . We can clearly see that rural areas in the US have significantly lower access to broadband internet compared to metro areas. . | There is ~37% differrence between households living in rural vs metro areas. . | Metro area shows bi-modal distribution. It&#39;s possible certain metro areas have much lower access to broadband internet. I will explore that in future analysis. . | 67% of households in metros with 1 million or more population have access to broadband ! . | This is a significant disparity ! . usagev_vs_hh = (bbdf_county_pov_usage.groupby(&#39;rucc_area&#39;) .agg( Median_Usage = (&#39;Usage&#39;,&#39;median&#39;), Total_Households = (&#39;Total_Households&#39;,&#39;sum&#39;) ) ) usagev_vs_hh . Median_Usage Total_Households . rucc_area . Metro 1M 0.667304 | 65090082 | . Metro &lt;.25M 0.500584 | 12366652 | . Metro &lt;1M 0.539444 | 24873710 | . RMetro &lt;2.5K 0.168310 | 958576 | . RNon-Metro &lt;2.5K 0.200098 | 1068921 | . UMetro &lt;20K 0.265684 | 6111594 | . UMetro &gt;20K 0.486277 | 6187898 | . UNon-Metro &lt;20K 0.316405 | 3465090 | . UNon-Metro &gt;20K 0.524717 | 2360686 | . Does Median Income Of the Area Matter? . I combined the median income from US census 2020 with above data to understand the influence of median income. . sns.set_style(&quot;white&quot;) sns.set(rc={&quot;figure.figsize&quot;:(10, 6)}) (sns.scatterplot(data= bbdf_county_pov_usage, x=&#39;Median_income&#39;, y =&#39;Usage&#39;, hue = &#39;county_category&#39;, palette= {&#39;Metro &lt;1M&#39; : &#39;#fc8d59&#39;, &#39;Metro&#39;:&#39;#fc8d59&#39;, &#39;Rural&#39;:&#39;blue&#39;, &#39;Urban&#39;:&#39;#67a9cf&#39;, },alpha=0.7) ).set(title=&quot;%Households with Access to Broadband Internet vs. Median Income&quot;); plt.show() . sns.set(rc={&quot;figure.figsize&quot;:(10, 6)}) (sns.lmplot(data= bbdf_county_pov_usage, x=&#39;Median_income&#39;, y =&#39;Usage&#39;, hue = &#39;county_category&#39;, scatter=False) ).set(title=&quot;Linear Fit: %Households with Access to Broadband Internet vs. Median Income&quot;); . (sns.kdeplot(data=bbdf_county_pov_usage, x=(bbdf_county_pov_usage[&quot;Median_income&quot;]), y=&quot;Usage&quot;, hue=&quot;county_category&quot;, fill=True, alpha=0.80, thresh = 0.50, levels = 10, palette= { &#39;Metro&#39;:&#39;#fc8d59&#39;, &#39;Rural&#39;:&#39;blue&#39;, &#39;Urban&#39;:&#39;#67a9cf&#39;, }) ).set(title=&quot;Distribution of %Households with Access to Broadband Internet vs. Median Income&quot;); . Observations . Above three plots show relationship between distribution of %households with access to internet vs. the median income. . The plots show that there is a moderate positive relationship between the income and %availability. Irrespective of where you live, communities with higher median income have higher chance to having access to the broadband internet . | Notice the slope of the lines in the linear fit plot. Metro line has a slightly more slope compared to rural. This shows that income inequality affects metro areas more than the rural areas. . | In rural areas, even higher income communities cannot get access to high speed internet. I think this points to the discrepancy in the availability and infrastructure. Rural communities in US do not yet have access to the high speed internet. . | Another pointer towards the infrastructure issue is the differrence between the intercepts. For example, notice that even for lower income communities in metro areas, roughly 30% have access to broadband internet whereas in rural areas, it&#39;s almost non-existent. . | The density plot above shows 50% of the density for each of the areas. The separation between them clearly shows the discrepancy between rural and metro areas. . | Quantile Regression . Next I wanted to understand how different income communities are affected. This can be done with quantile regression where I fit linear regression to 10%, 25%, 50%, 75% and 90% quantile of usage. . qdata = bbdf_county_pov_usage.drop([&#39;Combined_Usage&#39;,&#39;Total_Households&#39;,&#39;rucc_codes&#39;],axis=1).dropna() qmodel = smf.quantreg(&#39;Usage~Median_income&#39;,qdata) qs = [10,25,50,75,90] for q in qs: res = qmodel.fit(q/100) # qdata[&quot;Q&quot;+str(q)]=[(res.params[0] + x*res.params[1]) for x in qdata.Median_income] qdata[&quot;Q&quot;+str(q)]=qmodel.predict(res.params) qdata.head() . Usage Stabr Area_name Median_income rucc_area county_category Q10 Q25 Q50 Q75 Q90 . COUNTY_ID . 1001 0.390677 | AL | Autauga County | 67565.0 | Metro &lt;1M | Metro | 0.197055 | 0.335295 | 0.481372 | 0.621848 | 0.725269 | . 1003 0.456533 | AL | Baldwin County | 71135.0 | UMetro &gt;20K | Urban | 0.215659 | 0.366749 | 0.517384 | 0.655024 | 0.752876 | . 1005 0.305659 | AL | Barbour County | 38866.0 | UMetro &lt;20K | Urban | 0.047497 | 0.082435 | 0.191876 | 0.355154 | 0.503341 | . 1007 0.115791 | AL | Bibb County | 50907.0 | Metro 1M | Metro | 0.110246 | 0.188525 | 0.313338 | 0.467049 | 0.596454 | . 1009 0.193640 | AL | Blount County | 55203.0 | Metro 1M | Metro | 0.132633 | 0.226376 | 0.356673 | 0.506971 | 0.629674 | . qmodel = smf.quantreg(&#39;Usage~Median_income&#39;,qdata) qs = [10,25,50,75,90] for q in qs: res = qmodel.fit(q/100) # qdata[&quot;Q&quot;+str(q)]=[(res.params[0] + x*res.params[1]) for x in qdata.Median_income] qdata[&quot;Q&quot;+str(q)]=qmodel.predict(res.params) . (alt.Chart(qdata).mark_circle(opacity=0.4).encode( x = &#39;Median_income:Q&#39;, y = &#39;Usage:Q&#39;, color=alt.Color(&#39;county_category&#39;, scale=alt.Scale(scheme=&#39;set2&#39;, range=[&#39;#fc8d59&#39;,&#39;blue&#39;,&#39;#67a9cf&#39;,])), tooltip = [&#39;county_category&#39;] )) + ( alt.Chart(qdata).mark_line(opacity = 0.6, color=&#39;black&#39;).encode( x = &#39;Median_income:Q&#39;, y = &#39;Q10:Q&#39; )) + ( alt.Chart(qdata).mark_line(opacity = 0.6, color=&#39;gray&#39;, size=1.5).encode( x = &#39;Median_income:Q&#39;, y = &#39;Q25:Q&#39; )) + (alt.Chart(qdata).mark_line(opacity = 0.6, color=&#39;gray&#39;, size=1.5).encode( x = &#39;Median_income:Q&#39;, y = &#39;Q50:Q&#39;)) + ( alt.Chart(qdata).mark_line(opacity = 0.6, color=&#39;gray&#39;, size=1.5).encode( x = &#39;Median_income:Q&#39;, y = &#39;Q75:Q&#39;)) + ( alt.Chart(qdata).mark_line(opacity = 0.6, color=&#39;black&#39;,).encode( x = &#39;Median_income:Q&#39;, y = &#39;Q90:Q&#39;) ).interactive().properties(width = 600, height = 600, title = { &quot;text&quot;: [&quot;Broadband Usage vs. Median Income&quot;, &quot;By County Area Type&quot;], &quot;subtitle&quot;: [&quot; n Black lines show quantile regression predictions @ 10% and 90%&quot;, &quot; n nHouseholds with higher median income have grater access to broadband internet compared to lower income housholds&quot;], &quot;color&quot;: &quot;black&quot;, &quot;subtitleColor&quot;: &quot;black&quot;, }) + (alt.Chart({&#39;values&#39;:[{&#39;x&#39;: 160000, &#39;y&#39;: 0.7}]}).mark_text( text=&#39;10th Quantile&#39;, angle=0).encode( x=&#39;x:Q&#39;, y=&#39;y:Q&#39; )) + (alt.Chart({&#39;values&#39;:[{&#39;x&#39;: 160000, &#39;y&#39;: 1.45}]}).mark_text( text=&#39;90th Quantile&#39;, angle=0).encode( x=&#39;x:Q&#39;, y=&#39;y:Q&#39; )) . Observations: . We can notice right away that 90% quantile line passes mostly through metro areas whereas 10% quantile line passes through rural areas. This again highlights th fact that metro areas have higher %households that have access to broadband internet compared to rural areas. . | Changing slopes from 10% to 90% shows that the %household is a skewed outcome and OLS with median would not have captured the non linear behaviour. . | . Note: I am still analyzing the data by plotting the coefficients and adding more covariates. I will update the blog soon. . Microsoft released a new interactive Power BI report to show how differrent states and communities are affect by this digital divide. You can read more about it here. . ms_dashboard = &#39;https://app.powerbi.com/view?r=eyJrIjoiM2JmM2QxZjEtYWEzZi00MDI5LThlZDMtODMzMjhkZTY2Y2Q2IiwidCI6ImMxMzZlZWMwLWZlOTItNDVlMC1iZWFlLTQ2OTg0OTczZTIzMiIsImMiOjF9&#39; IFrame(ms_dashboard, width=800, height=600) . Wall Street Journal very recently pubslihed a series of articles on this topic to show rural areas in the US have lack of access to boradband internet. Watch the video below to learn more. This is consistent with my observations from the baove data. . . . References: . http://internetequity.uchicago.edu/resource/internet-access-and-internet-equity/ . | https://blogs.microsoft.com/latinx/2020/01/08/how-high-speed-internet-is-bringing-people-out-of-the-dark-ages-to-reshape-work-and-life-in-rural-america/ . | https://blogs.microsoft.com/on-the-issues/2019/10/07/the-path-to-prosperity-through-access-to-high-speed-internet/ . |",
            "url": "https://pawarbi.github.io/blog/dataanalysis/aiforgood/dataforgood/python/altair/quantile%20regression/2022/07/14/digital-inequality-us-broadband-internet-analysis.html",
            "relUrl": "/dataanalysis/aiforgood/dataforgood/python/altair/quantile%20regression/2022/07/14/digital-inequality-us-broadband-internet-analysis.html",
            "date": " • Jul 14, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Creating Subgroup Ranks in Power Query Using Table.AddRankColumn Function",
            "content": "Table.AddRankColumn . Table.AddRankColumn is the latest addition to the M language which lets you create ranks in Power Query directly. No more buffer-sort-add index column trick which was computationally expensive and could be less accurate. With this function you can specify one or more columns to rank by and also the order (ascending/descending). This is not a replacement for the RANKX function in DAX. If your rank order is not going to change or need it for validation purposes, this is a great function. . Read the official documentation for all the function parameters. But Reza Rad has the most comprehensive overview of the function with examples so definitely check it out. . . Note: You need to upgrade to June 2022 version of Power BI to use it in Desktop. . Subgroup Rank . In the example below, we have three products, their colors and the Units. Rank1 column shows the rank by number of units for all the products in the ascending order. What we are looking for is the subgroup rank, i.e rank within each product type as shown in Rank2 column. . import pandas as pd df = (pd.read_clipboard() .assign(Units = lambda s:s[&#39;Units&#39;].astype(&#39;int&#39;)) .assign(Rank1 = lambda s:s[&#39;Units&#39;].rank().astype(&#39;int&#39;)) .assign(Rank2 = lambda s:s.groupby(&#39;Product&#39;)[&#39;Units&#39;].transform(&#39;rank&#39;).astype(&#39;int&#39;)) .sort_values([&#39;Product&#39;,&#39;Rank2&#39;]) ) df . Product Color Units Rank1 Rank2 . 1 Product A | Red | 4549 | 6 | 1 | . 2 Product A | Green | 5562 | 10 | 2 | . 0 Product A | Black | 7635 | 11 | 3 | . 3 Product A | Yellow | 15000 | 12 | 4 | . 4 Product B | Black | 1513 | 2 | 1 | . 5 Product B | Red | 2969 | 3 | 2 | . 8 Product B | Black | 3860 | 4 | 3 | . 7 Product B | Yellow | 5409 | 8 | 4 | . 6 Product B | Green | 5491 | 9 | 5 | . 11 Product C | Yellow | 600 | 1 | 1 | . 10 Product C | Green | 4264 | 5 | 2 | . 9 Product C | Red | 4839 | 7 | 3 | . Here is my walkthrough of how you can create subgroup ranks in Power Query. . Important Notes: . As I mention in the video, this function is not query foldable so be sure to plan your steps to utilize folding first. If you don&#39;t know what is query folding, watch my in-depth session on query folding. . | When you create the rank column, it will always sort the table by the newly created rank column. This may not always be desired. For example if your table is sorted by date or a key column when imported, that sort order will be disrupted when you create the rank column. I am not sure if there is a way to prevent that from happening. This also has an unintended effect that vertipaq compression may not be optimal. As you know, cardinality of the column matters in vertipaq compression but distribution of values matters as well. If the sort order is disrupted it may increase the size of the dataset. Be sure to check before and after. . | I haven&#39;t tested this on a large dataset but be sure to check the refresh time and dataset size after creating the rank column. . | Look at the RankKind parameter to specify how the ranking should be done. . |",
            "url": "https://pawarbi.github.io/blog/powerbi/m/2022/06/21/subgroup-rank-powerquery-powerbi.html",
            "relUrl": "/powerbi/m/2022/06/21/subgroup-rank-powerquery-powerbi.html",
            "date": " • Jun 21, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "User Feedback Link / Button In Power BI",
            "content": "Dashboard Graveyard . If you have created data analytics dashboards, you must have seen hundreds of dashboards that are created and left unused in workspaces leading to a graveyard of dead dashboards. While there might many reason for this, one of the primary reasons being lack of feedback loop. Often the business requirements change, users change, dashboards may have inaccuracies or they could be lacking usability/accessibility. To overcome this, the developer should set up a feedback loop either formally or informally so the business users can provide timely feedback. Below I show one quick method I use to help my business partners send me a feedback while they are accessing the dashboard. This has worked very well for me because it&#39;s less friction to provide feedback since it&#39;s right there on the report and it&#39;s freestyle (as opposed to forms). Users can submit feedback along with supplemental information. . Note that I am using the term dashboard loosely here :) Be sure to check the additional links at the bottom of the page to learn about the other ways to help users provide feedback. . . Mailto Link . The idea is simple, create a customized mailto link that the users can click on. We can customize the link to include email addresses, subject line and the body of the email with instructions. Be specific and succinct in your instructions. . There are several free online mailto link generators. I like to use mailtolink. Just fill out the email addresses, subject and the instructions and it will create the link for you. . . Note: For privacy reasons, I enter dummy email addresses and links in the generator and then replace them in word. Don&#8217;t include any confidential information there. You can also use it to create a template once and just edit it for future projects. . Once you have created the link, you can just embed the link in a text box and done !!! When the user clicks on the report, it will launch their default email client with the addresses, subject, instructions filled out. Once you create the url, be sure to test it in your browser. . . In Power BI, create a text box &quot;Submit Feedback&quot; and hyeperlink it to the generated mailto link. . Note: For some reason, the link doesn&#8217;t work with the buttons. I get around this by overlaying a transparent text box on a button/image so the text is hidden but you get the same effect. Try below, click on the Submit Feedback button in the top right corner: . from IPython.display import IFrame . link = &#39;https://app.powerbi.com/view?r=eyJrIjoiNjAzOGYzNTEtM2JlNS00ZmE2LThiZTQtYWRjNzI1MzJmYzc5IiwidCI6IjkxMzc2MWU4LTc4NjEtNDc0ZS05ZjM4LWQyZDc1MjUwMDExZiJ9&amp;pageName=ReportSection2a85fb54a002b410a870&#39; IFrame(link, width=800, height=600) . Be sure to let me know your feedback ! :) . Resources . There are other creative ways to achieve the same thing. . David Eldersveld shows how to embed Microsoft Forms in Apps . | Adam from Guy In A Cube walks through the steps of using Forms + Buttons for gathering feedback : . | Using Google Forms to submit feedback, very creative. . | Using Power Apps . | .",
            "url": "https://pawarbi.github.io/blog/usability/2022/06/10/submit-feedback-button-powerbi.html",
            "relUrl": "/usability/2022/06/10/submit-feedback-button-powerbi.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Query Folding Safe Data Type Changes",
            "content": "Query Folding . You can read about query folding in detail in official documentation and the resources I have provided below. But at a high level, query folding allows us to create query steps that send a single native query back to the data source instead of the mashup engine doing the transformations. As much as possible, we want to design queries so they fold. Not all PowerQuery transformations are foldable. One of those transformations is changing data type. . %%html &lt;iframe title=&quot;vimeo-player&quot; src=&quot;https://player.vimeo.com/video/714177719?h=466f2d1c1b&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt; . Checking Query Folding . It is still debatable whether changing data types actually breaks query folding. My own tests using SQL Server, Postgres (BigQuery, Bit.io), OData, Synapse Analytics Serverless/Dedicated servers show that most data type changes don&#39;t actually break folding when you check the server logs. But my friend Nikola recently pointed out that it may not always be the case. There are three ways to check if a query is folding : . Right click the step in PQ and check if View Native Query option is greyed out. If it&#39;s greyed out, folding has stopped. Below query is foldable as the option is not greyed out. However, If this option is greyed out it doesn&#39;t necessarily mean that query folding isn&#39;t happening. But if it is not greyed out then we can be sure that query folding is taking place for sure. This method is similar to using Value.Metadata() to check query folding (link). | . . Use Start Diagnostics in Power Query and check the detailed diagnostics report for the query sent to the data source. You can watch my video above to learn about this technique. This process is definitely a bit tedious. You can also use SQL Profiler in DAX Studio or SSMS to trace queries and identify the queries generated. . | Check the server logs. This is the most straightforward and sure fire way. But not everyone has access to these logs. . | . Changing Data Type . So how do we make data type changes, query folding safe? Easy - Use Table.TransformColumns() instead of Table.TransformColumnTypes() .I have verified this with all the foldable data source types and it has always worked. If it doesn&#39;t work in your scenario, please let me know. In the screenshot below, I am connected to serverless SQL pool in Synapse Analytics. I will be changing data types for three columns UnitPrice (decimal), UnitPriceDiscountPct (decimal), Is_Promotion (text). . . Decimal to Percentage . Custom Transformation : . &#39;Table.TransformColumns(Data, {{&#34;UnitPriceDiscountPct&#34;,Number.From, Percentage.Type}})&#39; . . Decimal to Currency . Custom Transformation : . &#39;Table.TransformColumns(Data, {{&#34;UnitPrice&#34;,Number.From, Currency.Type}})&#39; . . Text to Logical . Custom Transformation : . &#39;Table.TransformColumns(Data, {{&#34;Is_Promotion&#34;,Text.From, type logical}})&#39; . . You can use the exact same pattern to make any data type change query folding safe, including for text and date columns. . &#39;Table.TransformColumns(Data, {{&#34;ColumnName&#34;,&lt;..&gt;.From, type &lt;enter type&gt;}})&#39; . Summary . Data type changes do not always stop query folding. You have to check the server logs to ensure if it&#39;s truly folding or not. But you can use above described method to always force query folding. This method works for SQL server, OData and Postgres SQL. . Resources . Chris Webb&#39;s Blog: https://blog.crossjoin.co.uk/2021/02/21/query-folding-on-sql-queries-in-power-query-using-value-nativequery-and-enablefoldingtrue/ . | My Power Query, Query Folding, Power BI Guru, Alex Powers&#39;s 30 Day Query Folding Challenge . | My previous blog . | Nikola Ilic&#39;s blogs: Nikola has written and presented extensively on this topic . https://data-mozart.com/query-folding-tricks-lies-ultimate-performance-test/ . | https://data-mozart.com/what-is-a-query-folding-in-power-bi-and-why-should-i-care/ . | . | .",
            "url": "https://pawarbi.github.io/blog/powerquery/m/queryfolding/2022/06/01/query-folding-data-types-changes-powerbi.html",
            "relUrl": "/powerquery/m/queryfolding/2022/06/01/query-folding-data-types-changes-powerbi.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Extracting Matching Words Using A Pre-Defined List in Power Query/M",
            "content": "Problem . A colleague reached out to me with an interesting problem. He wanted to extract certain error codes/phrases from sentences. He had a list of all possible codes and phrases and wanted it to be dynamic so that he can change the list of possible codes in the future. For example, imagine you survey 10 BI developers on which languages they use on a daily basis. Some might say Python, SQL, R, Excel etc. We have a list of all possible languages and we want to extract only the languages for that developer based on this list. If a language does not exist in the list, it should return a null value. See below what the final solution should look like: . . Solution . I couldn&#39;t find a solution online so I created one using List functions in M. All of the steps below can be wrapped into one nice function, but for demonstration purposes I will do it step by step. If you use this solution, just convert it into a function as needed. Also to keep the blog short, I am going to assume that the words/phrases match exactly with those in the list. You will need to format them (uppercase/lowercase/remove punctuations etc.) based on your scenario. . We start with 2 columns. First column contains unique id for each respondent and the second column has their text responses. We also have a list of languages as another query. . . Step 1 : Clean and Trim . Create a new column with the text trimmed and cleaned. You can go to Add column &gt; Format &gt; Trim &gt; Clean . If their are any punctuations, you should remove those as well in this step. We need each of the words to be isolated. For example, if the sentence contains &quot;SQL,&quot; it will be treated as a one word. Remove the comma to turn &quot;SQL,&quot; into &quot;SQL&quot;. . . Step 2 : Split the Text using Text.Split . In this step we split the setence on each row into separate words and store them as a list using Text.Split() function. . . Notice after splitting we get a list on each row. . . Step 2 : Find Matching Words Using List.Intersect with List.Buffer . Now we need to lookup the pre-defined list to see if any of the words in the above list exist. The expected result is that if the list contains any of the pre-defined words, those should be returned otherwise we should get a null value. We can achieve this using List.Intersect(). I am wrapping the list with List.Buffer() to make lookup faster. . Notice below, the Person1 uses Python so the resulting list on the first row contains only &quot;Python&quot; and no other words. . Step 3 : Convert List to Table Using Table.FromList . Convert the intersected list into a table using Table.FromList. . . Step 4 : Expand the Table . Now we just need to expand the table to get each matching word on a separate row for each person. You can eaither use Table.ExpandTableColumn or just click on the two double arrows . . After extracting : . . Step 5 : Remove Intermediate Columns . Remove all the intermediate columns from steps 2-4 to keep only the resulting column with matching words! As I mentioned above, all these steps can be rolled into one nice function that can be used elsewhere as well. . Person2 had two languages Python &amp; R so we get two lines, whereas person5 &amp; 8 had no matching languages thus null. . . Final M code: . You can download the .pbix from here. If you know a better, faster, more elegant solution, please let me know. .",
            "url": "https://pawarbi.github.io/blog/powerquery/m/list/2022/03/11/powerquery-M-extracting-words.html",
            "relUrl": "/powerquery/m/list/2022/03/11/powerquery-M-extracting-words.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Relative & Absolute Table Refresh Durations in Power BI",
            "content": "Overview . When designing models and queries in Power BI dekstop, you often need to measure the individual dataset refresh duration. This is particularly important for large datasets where you are trying to optimize the queries to reduce the refresh time during development and in service. Matthew Roche from Power BI CAT team asked a question recently on Twitter about measuring refresh duration. Most pointed to Phil Seamark&#39;s Visualize Your Power BI Refresh blog and tool. I use that tool and it was the only way I was aware of. But Power BI user Eric Beery responded with an interesting solution that I didn&#39;t know about. In this blog I am exploring that solution. . All credit goes to Eric for this solution Great idea. My current workaround:Hovering over the table in the field list shows the date/time the refresh completed. So I use the difference in completion time between a small table (like date) and the large fact table in interested in to figure out how long it took. . &mdash; Eric Beery (@eric_beery11) January 6, 2022 . Relative Refresh . Solution 1 : The Easy Way . Hovering over the table name . If you hover the table in report view or model view, you can get the &quot;Data refreshed&quot; date and time. Unfortunately it doesn&#39;t give you the data refresh start time so it&#39;s not possible to get the exact duration. We can get the relative refresh time by looking at when the refresh finished for a table with little/no data. For example, the small table below finished refreshing at 11:51:39 AM and the large table finished at 11:52:15 AM. So the refresh time for the large table was roughly 45 seconds longer than the small table. This obvisouly doesn&#39;t help us a lot but that&#39;s the basic idea. . . Note: This assumes the refreshes happen in parallel. If you have disabled &quot;Allow parallel loading of tables&quot; under options, tables will be refreshed sequentially in which case this doesn&#8217;t help. . We can improvise this by adding a blank table by clicking on &quot;Enter data&quot; on the home tab. This will create a blank table without any rows. The idea here is that the blank table should take the least amount of time and thus should be close to the start of refresh time. We can use that to get a better estimation of relative refresh duration. . . As you can in the image below, the empty table loaded at 12:51:25PM and the large table finished refreshing at 12:51:52PM, thus the large table takes roughly ~25 seconds to load. It took a bit longer in the first attempt because it was the first refresh. I am guessing when I refreshed again, the connection &amp; data were already cached so it was faster. . . Solution 2 : Selective Refresh Duration . Refresh duration for a specific table . As noted above, the limitation of this method is that we it won&#39;t work withour enabling parallel load. Also, even when parallel load in enabled, the query might wait for other queries to connect/refresh which will give false duration. We can get around this by refreshing only select few tables. This is not possibel in the report view, but if we switch to model view, we can CNTRL + Select the tables to refresh. This will provide slightly more accurate relative refresh time and we don&#39;t have to wait for all the queries to refresh. This is a better way when working with many tables or when you have a large table. . . Solution 3 : Using DMV . Getting refresh times using DMV . Since this is all metadata, I thought I could query the DMV to get the same information. If we query TMSCHEMA_PARTITIONS using DAX Studio, we can get the same refresh times. . . Note in this case, there are two times- ModifiedTime and RefreshedTime. ModifiedTime tells you when the table was last saved/modified and RefreshedTime tells you when the data was refreshed. It is Also note that the times are in UTC time zone and since we are using MDX, there is no way to convert it to local time zone in DAX Studio, at least I am not aware of it. . Using DMV gives us partition level refresh time. This helped in a project where I had to partition a massive dataset and wanted to know how well it was working. Querying the DMV allowed me to tune the size/number of partitions. . DAX Studio can connect to published datasets in service. So using this method, we can even check the refresh times for tables in a published datasets. Below I am using Python to connect to a published dataset and query the DMV. . import adodbapi as ado conn = adodbapi.connect(&quot;Provider=MSOLAP.8; Data Source = &#39;powerbi://api.powerbi.com/v1.0/myorg/&lt;workspace&gt;&#39;;initial catalog=&#39;&lt;dataset name&gt;&#39;&quot;) cstring = &quot;Provider=MSOLAP.8; Data Source = &#39;powerbi://api.powerbi.com/v1.0/myorg/&lt;workspace&gt;&#39;;initial catalog=&#39;&lt;dataset name&gt;&#39;&quot; cur = conn.cursor() with ado.connect(cstring) as con: with con.cursor() as cur: dax_str = &#39;&#39;&#39; select [Name] ,[ModifiedTime] ,[RefreshedTime] from $SYSTEM.TMSCHEMA_PARTITIONS &#39;&#39;&#39; cur.execute(dax_str) data = cur.fetchall() . Convert the DMV data to a pandas dataframe . def get_dmv(data): &#39;&#39;&#39; Original script: https://stackoverflow.com/questions/56075159/convert-listadodbapi-apibase-sqlrow-to-pd-dataframe Modified to make changes to retrieve the DMV data Input: Pandas dataframe Return: Pandas dataframe with columns convrted to datetime Author: Sandeep Pawar &#39;&#39;&#39; cols = pd.DataFrame(np.array(data.ado_results)).iloc[0] values = pd.DataFrame(np.array(data.ado_results)).iloc[1:] for col in list(values.columns): values[col]=pd.to_datetime(values[col]).dt.tz_convert(&quot;UTC&quot;) values.columns = cols df = values.T df.columns = [&#39;ModifiedTime&#39;,&#39;RefreshedTime&#39;] return df . df = get_dmv(data) ((df[[&#39;RefreshedTime&#39;]]-df[[&#39;RefreshedTime&#39;]].min())/np.timedelta64(1,&#39;m&#39;)).plot.barh(title = &quot;Refresh Durations&quot;, xlabel=&#39;Datsets&#39;); . For the same dataset published to service, I deducted the refresf time of the empty table from rest of the times to get relative refresh duration. Yo can use SSMS/Tabular Editor to selectively refresh datasets (just like above) and get more relatively accurate refresh durations. . In the same twitter thread, Imke Feldmann showed that she has incorporated a similar approach in her Power BI Cleaner tool. Take a look at it below: Same here - what a great find. 👍I&#39;ve created a report on that which will be included in the next version of my #PowerBI Cleaner tool. Join the exclusive preview of it tomorrow in my chat with Reid from @HavensBI : https://t.co/78pwTQVhVv pic.twitter.com/H7D5aecL1Z . &mdash; Imke Feldmann (@TheBIccountant) January 6, 2022 . Absolute Refresh . Solution 4 : Using SQL Server Profiler with &quot;Visualize My Refresh&quot; . The most accurate method . The most accurate and reliable way to capture refresh durations are by using the SQL Server Profiler and Phil Seamark&#39;s &quot;Visualize My Refresh&quot; report. Phil will explain it better than I can so watch below: . . If you have any thoughts on improving these solutions, please let me know. .",
            "url": "https://pawarbi.github.io/blog/powerbi/tuning/optimization/2022/02/25/relative-absolute-refresh-duration.html",
            "relUrl": "/powerbi/tuning/optimization/2022/02/25/relative-absolute-refresh-duration.html",
            "date": " • Feb 25, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Marketing Promotion Campaign Analysis Using Feature Based Time Series Clustering",
            "content": "Overview . The goal of this notebook is to show how to do exploratory data analysis of time series data and use feature based clustering method to identify attributes that are similar in time domain. The novelty of this based is, instead of relying on shape of the time series, feature based approach enables more interpretable solution. I used some of these techniques in a real project so I wanted to showcase this for my own future reference. . The data here is alcohol consumption across 80+ regions of Russia. I am not sure if this is real data or not. Hypothetically, an alcohol company has been operating in St. Petersburg. Their campaign was successful in St.Petersburg so we want to identify other regions where they can potentially run similar campaigns. We have alcohol consumption for differrent alcohol types per capita from 1998 to 2016. . I don&#39;t know anything about Russia&#39;s geography or economy, so I will make reasonable assumptions in my analysis. . Importing Libraries . #!pip install altair #!pip install catch22 . import pandas as pd import numpy as np import matplotlib.pyplot as plt import altair as alt import seaborn as sns import statsmodels.api as sm import scipy.stats as st import statsmodels.stats.api as sms from sklearn.metrics import r2_score from catch22 import catch22_all from sklearn.cluster import KMeans alt.data_transformers.disable_max_rows() import warnings warnings.filterwarnings(&quot;ignore&quot;) . df = pd.read_csv(r&#39;./data/russian_alcohol_consumption.csv&#39;) df.head() . year region wine beer vodka champagne brandy . 0 1998 | Republic of Adygea | 1.9 | 8.8 | 3.4 | 0.3 | 0.1 | . 1 1998 | Altai Krai | 3.3 | 19.2 | 11.3 | 1.1 | 0.1 | . 2 1998 | Amur Oblast | 2.1 | 21.2 | 17.3 | 0.7 | 0.4 | . 3 1998 | Arkhangelsk Oblast | 4.3 | 10.6 | 11.7 | 0.4 | 0.3 | . 4 1998 | Astrakhan Oblast | 2.9 | 18.0 | 9.5 | 0.8 | 0.2 | . Data Quality . It is important that before we analyze the data, we should check the quality of the data. If there are any missing values, outliers, unusual observations etc. those should be treated before analysing the data. . Null Values . df.isna().any() . year False region False wine True beer True vodka True champagne True brandy True dtype: bool . Wine, beer, vodka, champagne and brandy have null values. Let&#39;s find if any of the columns have more than 5% null values (i.e in this at least 1). . # Which region has null values? null_regions = [] print(&quot;% of Values missing n========================================&quot;) for r in df.region.unique(): nulls = (df.query(&#39;region == @r&#39;).isna().mean()) if nulls.max() &gt; 0.05: print(r,&quot;,&quot;,nulls.idxmax(), &quot;:&quot;, 100 * round(nulls.max(),2),&quot;%&quot;) null_regions.append(r) . % of Values missing ======================================== Republic of Ingushetia , brandy : 79.0 % Republic of Crimea , wine : 84.0 % Sevastopol , wine : 84.0 % Chechen Republic , wine : 100.0 % . There are four regions with more than 79% of the values missing. If few values were missing, we could have used interpolation methods to impute the missing values, but in this we don&#39;t have enough values to do so. Let&#39;s look at which values are missing. . #alcohol types alcohols = list(df.columns[2:]) #Find number of missing values and location of the missing values each of the 4 regions null_regions_df = (df[df[&#39;region&#39;] .isin(null_regions)] .sort_values(by=[&#39;region&#39;,&#39;year&#39;]) .groupby([&#39;year&#39;,&#39;region&#39;]) .apply(lambda x : x.isna().sum()) .drop([&#39;year&#39;,&#39;region&#39;],axis=1) .assign(nulls = lambda x: x.sum(axis=1)) .reset_index() ) null_regions_df.head(5) . year region wine beer vodka champagne brandy nulls . 0 1998 | Chechen Republic | 1 | 1 | 1 | 1 | 1 | 5 | . 1 1998 | Republic of Crimea | 1 | 1 | 1 | 1 | 1 | 5 | . 2 1998 | Republic of Ingushetia | 1 | 1 | 1 | 1 | 1 | 5 | . 3 1998 | Sevastopol | 1 | 1 | 1 | 1 | 1 | 5 | . 4 1999 | Chechen Republic | 1 | 1 | 1 | 1 | 1 | 5 | . plt.title(&quot;Fig 1: Heatmap showing number of alcohol types missing by year and region n&quot;) ( sns.heatmap (pd.crosstab( null_regions_df[&#39;region&#39;], null_regions_df[&#39;year&#39;], values = null_regions_df[&#39;nulls&#39;], aggfunc=&#39;max&#39; ), cmap=&quot;magma_r&quot;, annot=True, cbar=False, square=True ) ); . Observations: . In Fig 1., the heatmap shows, out of the 5 alcohol types, how many types have missing values. . Crimea and Sevastopol has values missing for all 5 alcohol types for years 1998 to 2013 and none missing for the last 3 years. This suggests that likely the data for first 16 years either wasn&#39;t collected or wasn&#39;t available. | Chechen Republic has values missing except for one alcohol type in the last 3 years | Ingushetia has missing values at random | Since our goal is to compare the regions using alcohol consumption data for all years, it won&#39;t be possible to include these 4 regions in to the analysis. We will drop all these 4 regions from further analysis. . Duplicate Values . #Are there any duplicate values? if max((df[~df.region.isin(null_regions)].region.value_counts())) == min((df[~df.region.isin(null_regions)].region.value_counts())): print(&quot;All Regions have 19 years of data&quot;) else: print(&quot;Some of the regions of more/less data&quot;) . All Regions have 19 years of data . None of the regions have duplicate data across all years . Unusual Data . For all alcohol types we will look at min and max values to see if there are any unusual values such as 0 or negative consumption. . #Are there any observations with 0 or negative values? Consumption can be 0 but not negative df[~df.region.isin(null_regions)].describe().loc[&#39;min&#39;] . year 1998.0 wine 0.1 beer 1.0 vodka 0.4 champagne 0.1 brandy 0.0 Name: min, dtype: float64 . There is at least one region with 0 lts per capita consumption of brandy. Let&#39;s look at that closely. . df[~df.region.isin(null_regions)].query(&#39;brandy == 0&#39;) . year region wine beer vodka champagne brandy . 565 2004 | Tuva Republic | 2.1 | 25.8 | 15.2 | 0.2 | 0.0 | . In 2004, Tuva Republic did not record any consumption of brandy. . (df.query(&#39;region == &quot;Tuva Republic&quot;&#39;) .set_index(&#39;year&#39;)[&#39;brandy&#39;] .plot(title=&quot;Fig 2: Brandy consumption for Tuva Republic n 0 consumption in 2004&quot;, legend=True) ) (plt.axhline(df.query(&#39;region == &quot;Tuva Republic&quot;&#39;) .set_index(&#39;year&#39;)[&#39;brandy&#39;] .median(), color=&#39;red&#39;) ); . In 2004, the brandy consumption in Tuva dropped from 0.1 ltr to 0. The median consumption over the entire period was 0.2 ltr. So, considering overall low consumption, 0 does not seem like an outlier. We will keep the value as is. . # Max values df[~df.region.isin(null_regions)].describe().iloc[:,1:] . wine beer vodka champagne brandy . count 1539.000000 | 1539.000000 | 1539.000000 | 1539.000000 | 1539.000000 | . mean 5.637544 | 51.722190 | 11.902404 | 1.315172 | 0.524185 | . std 2.811555 | 25.115577 | 5.078808 | 0.798462 | 0.399331 | . min 0.100000 | 1.000000 | 0.400000 | 0.100000 | 0.000000 | . 25% 3.550000 | 32.700000 | 8.400000 | 0.800000 | 0.200000 | . 50% 5.400000 | 50.300000 | 11.600000 | 1.200000 | 0.400000 | . 75% 7.400000 | 67.500000 | 15.000000 | 1.660000 | 0.700000 | . max 18.100000 | 207.300000 | 40.600000 | 5.560000 | 2.300000 | . Except for beer, the max values for all other alcohols seem reasonable. In case of beer, the 75% value is 67.5 lts and max is 207, which is 5 standard deviations away from 75th percentile. We need to understand which region has this value and how should we treat it. . Below, we will drop the regions with null values and create a new dataframe with percentage share of each alcohol type for the given year and region. . df1 = df[~df.region.isin(null_regions)] df1[&#39;total&#39;]=(df1[alcohols].sum(axis=1)) df1 = (df1 .assign(winepct = df1[&#39;wine&#39;]/df1[&#39;total&#39;]) .assign(beerpct = df1[&#39;beer&#39;]/df1[&#39;total&#39;]) .assign(vodkapct = df1[&#39;vodka&#39;]/df1[&#39;total&#39;]) .assign(champct = df1[&#39;champagne&#39;]/df1[&#39;total&#39;]) .assign(brandypct = df1[&#39;brandy&#39;]/df1[&#39;total&#39;]) .assign(year = pd.to_datetime(df1.year,format=&#39;%Y&#39;)) .round(3) ) df1.head(5) . year region wine beer vodka champagne brandy total winepct beerpct vodkapct champct brandypct . 0 1998-01-01 | Republic of Adygea | 1.9 | 8.8 | 3.4 | 0.3 | 0.1 | 14.5 | 0.131 | 0.607 | 0.234 | 0.021 | 0.007 | . 1 1998-01-01 | Altai Krai | 3.3 | 19.2 | 11.3 | 1.1 | 0.1 | 35.0 | 0.094 | 0.549 | 0.323 | 0.031 | 0.003 | . 2 1998-01-01 | Amur Oblast | 2.1 | 21.2 | 17.3 | 0.7 | 0.4 | 41.7 | 0.050 | 0.508 | 0.415 | 0.017 | 0.010 | . 3 1998-01-01 | Arkhangelsk Oblast | 4.3 | 10.6 | 11.7 | 0.4 | 0.3 | 27.3 | 0.158 | 0.388 | 0.429 | 0.015 | 0.011 | . 4 1998-01-01 | Astrakhan Oblast | 2.9 | 18.0 | 9.5 | 0.8 | 0.2 | 31.4 | 0.092 | 0.573 | 0.303 | 0.025 | 0.006 | . Exploratory Data Analysis . To better understand the pattern in the data and compare the regions quickly over the 19 year period, decile heatmap as show in Fig 3 is created. In this visual, for each year, the total alcohol conumption is binned into 10 bins (i.e deciles). For example, the region with the highest consumption will be in 10th decile while the region with the lowest consumption will be in the 1st decile. The lighter color indicates high decile (and high consumption) while dark color shows low consumption. The regions are sorted by total consumption so the regions with high consumption are at the top. . plt.figure(figsize = (10,22)) plt.title(&quot;Fig 3: Decile Heat Map n Shows distribution of binned alcohol consumption by year and region n Lighter color: High consumption, darker color: low consumption&quot;, fontsize =14) sns.heatmap(df1[[&#39;year&#39;,&#39;region&#39;,&#39;total&#39;]] .assign(year=df1.year.dt.year) .set_index([&#39;year&#39;,&#39;region&#39;]) .groupby(level=0) .transform(lambda x: pd.qcut(x, q = np.linspace(0,1,11), labels=False)) .unstack(level=0) .assign(t = lambda x: x.sum(axis=1)) .sort_values(by=&#39;t&#39;, ascending=False) .drop(&#39;t&#39;,axis=1), cmap=&#39;magma&#39;,cbar_kws={&quot;orientation&quot;: &quot;horizontal&quot;}); . Observations: . Moscow has light color all throughout the 19 years showing it had the highest alcohol consumption continuoulsy for all years. | Saint Petersburg is towards the top, thus one of the regions with high alcohol consumptions | Saint Petersburg was in high deciles until 2010 and then the alcohol sumption dropped drammatically in the last 5-6 years. | A visual comparison shows Chelyabinsk Oblask, Yaroslavi Oblast, Perm also show a similar trend as that of Saint Petersburg | Some regions such as Karaychay, Dagestan, Chukotka, Adyega etc which are at the bottom, had very low and flat trend in consumption compared to Saint Petersburg | Some regions such as Tver, Nizhny, Sakhlin showed pick up in consumption in later years, opposite to Saint Petersburg | Let&#39;s compare the consumption further by looking at box plot and by individual alcohol type. . df1_melt = df1.iloc[:,:7].melt([&#39;year&#39;,&#39;region&#39;],var_name=&#39;Alcohol&#39;, value_name=&#39;Consumption&#39;) df1_melt.head(3) . year region Alcohol Consumption . 0 1998-01-01 | Republic of Adygea | wine | 1.9 | . 1 1998-01-01 | Altai Krai | wine | 3.3 | . 2 1998-01-01 | Amur Oblast | wine | 2.1 | . Trend Analysis: . Fig 4 shows distribution of total alcohol consumption for each of the year and by region. Values that are within the box plot are within the inter-quartile range ,where as values outside are potential outliers. The thick blue line shows the median consumption over the period. Light orange color line shows total consumption of Saint Petersburg for comaprison purposes. Each dot in fig 5 is a region. You can hover over the circle to get breakdown of consumption by alcohol type, volume and percentage. . Fig 5 shows the consumption for each selected region over the 19 year period. Select the region fro teh dropdown menu to get the consumption by type. . You can zoom-in/out and pan to explore the data in fig 4 &amp; 5. . median_line = alt.Chart(df1).mark_line().encode( x = &#39;year:T&#39;, y = &#39;median(total):Q&#39; ).interactive() total_box = alt.Chart(df1).mark_boxplot(opacity=0.3).encode( x = &#39;year:T&#39;, y = &#39;total:Q&#39;, ).interactive() total_scatter = alt.Chart(df1).mark_circle(opacity=0.8).encode( x = &#39;year:T&#39;, y = &#39;total:Q&#39;, color=&#39;region&#39;, tooltip = list(df1.columns)).interactive() total_line = alt.Chart(df1).mark_line(opacity=0.1).encode( x = &#39;year:T&#39;, y = &#39;total:Q&#39;, color=&#39;region&#39;, tooltip = list(df1.columns), strokeWidth = alt.condition(&quot;datum.region == &#39;Saint Petersburg&#39;&quot;,alt.value(3),alt.value(1)), ).interactive() total_line_stp = alt.Chart(df1.query(&#39;region == &quot;Saint Petersburg&quot;&#39;)).mark_line(opacity=0.4, strokeDash=[1,1]).encode( x = &#39;year:T&#39;, y = &#39;total:Q&#39;, color=alt.value(&quot;#FFAA00&quot;), strokeWidth = alt.condition(&quot;datum.region == &#39;Saint Petersburg&#39;&quot;,alt.value(3),alt.value(1)), tooltip = list(df1.columns) ).interactive() # (total_scatter + total_line_stp).properties(width=700, height = 400, title = &quot;Total Alcohol consumption for all regions vs. Year&quot;) top = (total_scatter + total_box + median_line + total_line + total_line_stp).properties(width=700, height = 300, title = &quot;Fig 4: Total Alcohol consumption for all regions vs. Year&quot;) region_dropdown = alt.binding_select(options= sorted(list(df1_melt.region.unique()))) region_select = alt.selection_single(fields=[&#39;region&#39;], bind=region_dropdown, name=&quot;Select&quot;) bottom = (alt.Chart(df1_melt) .mark_line().encode(alt.X(&#39;year:T&#39;), alt.Y(&#39;mean(Consumption):Q&#39;, scale=alt.Scale(domain=[0,210])), color = &#39;Alcohol:N&#39;, tooltip=[&#39;year&#39;,&#39;Consumption&#39;,&#39;Alcohol&#39;]) ).add_selection( region_select ).transform_filter( region_select ).properties(width=700, height = 200, title = &quot;Fig 5:Select Region: Alcohol consumption by Year, Alcohol Type &quot;) (top &amp; bottom.interactive()) . Observations: . Above plot shows the distribution of total consumption over the years for all non-null regions. Some observations: . Total alcohol consumption in Russia increased from 1998 to 2008, levelled off over the next 4 years and then started declining after 2012. Global recession started in ~2008. It is highly likely that that may have affected the consumption. . | The box plot shows the distribution of total alcohol consumption for all regions. Overall, the the total consumption for all regions was within the interquartile range, except in years 2004-2006 for Zabalkalsky Krai (three blue dots in the chart that are away from all other points). For Zabaykalsky Krai, the consumption was significantly higher from 2004 to 2006. Selecting Zabaykalsky Krai in fig 5 shows thatthe beer consumption in 2006 was 207 lts, which is the outlier we found in the analysis above. Interestingly, consumption of vodka and other alcohols also increased over that period indicating it&#39;s not an error in values. The consumption dropped precipitously after 2006. It stayed the same before, during and after the global recession, contrary to overall trend in Russia. . | Moscow shows trend similar to overall trend, while Moscow Oblast is exactly opposite. Beer consumption of Moscow Oblast increased rammatically in 2006 and was not affected by the Recession. . | Selecting Saint Petersburg in fig 5 shows that beer consumption increased steadily from 1998 to 2006 and dropped significantly thereafter and continued its decline. . | Saint Petersburg is one of the most visted cities of Russia and tourism is a big part of the economy. Global recession in 2008 might have led to the decline tourism and thus copnsumption of alcohol. . | There is no seasonality observed in this data and data are non-stationary. . | Comparing Saint Petersburg with Rest of Russia . Fig 6 and 7 show breakdown of alcohol consumption by volume and percentage for all of Russia. Fig 8 &amp; 9 show similar metrics for Saint Petersburg. This will help us compare the trends and identify insights. All 4 charts are zoomable and can be panned. Fig 10 shows the alcohol consumption for Saint Petersburg by rank order. . total_n = alt.Chart(df1_melt).mark_area(size=20).encode( y=alt.Y(&#39;sum(Consumption)&#39;, stack=&quot;normalize&quot;, axis=alt.Axis(title=&#39;%Alcohol consumed&#39;)), x=&#39;year:T&#39;, color=&#39;Alcohol&#39;, tooltip=[&#39;sum(Consumption):Q&#39;,&#39;year&#39;] ).interactive().properties(title = &#39;Fig 7: All Regions:%Alcohol Consumption by Type &amp; Year&#39;) total_z = alt.Chart(df1_melt).mark_area(size=20).encode( y=alt.Y(&#39;sum(Consumption)&#39;, stack=&quot;zero&quot;, axis=alt.Axis(title=&#39;Alcohol consumed by vol&#39;)), x=&#39;year:T&#39;, color=&#39;Alcohol&#39;, tooltip=[&#39;sum(Consumption):Q&#39;,&#39;year&#39;] ).interactive().properties(title = &#39;Fig 6: All Regions:Total Alcohol Consumption by Type &amp; Year by vol&#39;) total_n_stp = alt.Chart(df1_melt.query(&#39;region ==&quot;Saint Petersburg&quot;&#39;)).mark_area(size=20).encode( y=alt.Y(&#39;sum(Consumption)&#39;, stack=&quot;normalize&quot;, axis=alt.Axis(title=&#39;%Alcohol consumed&#39;)), x=&#39;year:T&#39;, color=&#39;Alcohol&#39;, tooltip=[&#39;sum(Consumption):Q&#39;,&#39;year&#39;] ).interactive().properties(title = &#39;Fig 9:St Petersburg: %Alcohol Consumption by Type &amp; Year&#39;) total_z_stp = alt.Chart(df1_melt.query(&#39;region ==&quot;Saint Petersburg&quot;&#39;)).mark_area(size=20).encode( y=alt.Y(&#39;sum(Consumption)&#39;, stack=&quot;zero&quot;, axis=alt.Axis(title=&#39;Alcohol consumed by volume&#39;)), x=&#39;year:T&#39;, color=&#39;Alcohol&#39;, tooltip=[&#39;sum(Consumption):Q&#39;,&#39;year&#39;] ).interactive().properties(title = &#39;Fig 8: St Petersburg: Total Alcohol Consumption by Type &amp; Year by vol&#39;) ( total_z | total_n) &amp; ( total_z_stp | total_n_stp) . (df1[[&#39;wine&#39;,&#39;beer&#39;,&#39;vodka&#39;,&#39;champagne&#39;,&#39;brandy&#39;,&#39;region&#39;,&#39;year&#39;]] .query(&#39;region == &quot;Saint Petersburg&quot;&#39;) .drop(&#39;region&#39;,axis=1) .set_index(&#39;year&#39;) .rank(axis=1, ascending=False) .astype(&#39;int&#39;) .plot(figsize=(10,6), label=True, title=&quot;Fig 10: Alcohol Consumption Rank: Saint Petersburg n Beer is #1, Wine took over Vodka in 2013 as #2&quot;) ); . df1[&#39;weak&#39;] = df1[&#39;beer&#39;] + df1[&#39;wine&#39;] + df1[&#39;champagne&#39;] df1[&#39;hard&#39;] = df1[&#39;vodka&#39;] + df1[&#39;brandy&#39;] df1[&#39;hardpct&#39;] = df1[&#39;hard&#39;]/df1[&#39;total&#39;] df1[&#39;weakpct&#39;] = df1[&#39;weak&#39;]/df1[&#39;total&#39;] (alt.Chart(df1.query(&#39;region == &quot;Saint Petersburg&quot;&#39;)[[&#39;year&#39;,&#39;hard&#39;,&#39;weak&#39;]].melt([&#39;year&#39;], var_name=&#39;type&#39;)) .mark_area() .encode( x=&#39;year:T&#39;, y= alt.Y(&#39;sum(value)&#39;, stack=&#39;normalize&#39;), color=&#39;type&#39;, tooltip=[&#39;year&#39;,&#39;type&#39;,&#39;value&#39;]) ).properties(title=&#39;Fig 11: Hard vs. Weak alcohol by % for Saint Petersburg&#39;) . Observations: . 1. Total consumption: Overall consumption of alcohol per capita for Russia and Saint Petersburg follow similar trends. Both show decline in consumption in the last 5-10 years but at differrent rates. The decline started much earlier (2009) in Saint Petersburg while it remained steady from 2009-2012 for Russia and then started decreasing. Saint Peterberg&#39;s consumption dropped by ~70% compared to only 25% for Russia. Saint Peterburg accounted for 2% of total consumption at its peak in 2009 and only 0.08% in 2016. . 2. Beer Consumption: Beer consumption decreased by volume across Russia and Saint Petersburg in the last few years. But interestingly, while the overall consumption dropped in volume, preference for beer increased from ~ 55% in the late 1990s to ~ 75% by 2016. In contrast, consumers in Saint Petersburg always preferred beer in the 1990s (~63%), it grew to 87% at its peak in 2009 and dropped to 67% by 2016. While beer was still the preferred beverage, its preference has been decreasing in Saint Petersburg in constrast to the national average. My hypothesis is that tourism is a big part of Saint Petersburg&#39;s economy, so recession in 2008-09 may have caused reduction in tourism and consumption of beer. . 3.Vodka: Vodka is synonymous with Russia and yet vodka isn&#39;t the most consumed alcohol. Consumption of alcohol decreased in Russia and Saint Petersburg. Vodka remained the second most consumed alcohol in Saint Petersburg until 2013. As show in fig 10, vodka became the third most consumed alcohol by volume. . 4.Wine: Preference for wine over vodka steadily increased in the first 16 years and then took over vodka as the second most consumed alcohol by volume in Saint Petersburg. While some reasearch has been done to identify the reasons, it&#39;s not conclusive. It&#39;s postulated that women prefer wine over beer and vodka so perhaps change in Saint Petersburg&#39;s demographic mixture may have led to wine&#39;s popularity but not much data is available to confirm that. As show in fig 10, wine became more popular than vodka in 2014. Consumers in Saint Petersburg drink 5% more wine per capita than national average (14% vs. 9%). . 5. Champagne &amp; Brandy: These two are lumped together because historically these two alcohols haven&#39;t been consumed much, neither in Russia nor in Saint Petersburg. For example, in 2016 Saint Petersburg consumers drank 41 ltr beer on average and only 3.2 lts champagne and brandy. Although the percentage of these two has been increasing, at the expense of beer, but still the volumes are very low. . 6. Hard vs. weak alcohol: We can group the alcohols by the percentage of alcohol present in them. Vodka, brandy contain ~40% alcohol whereas beer, champagne &amp; wine contain 4-12% alcohol. If we group the alcohols this way as hard vs. weak based on alcohol content, we see a differrent picture. As shown in fig 11, although by volume the amount of hard alcohol consumed is low, by percentage consumers in Saint Petersburg are actually preferring hard over weak alcohols at a rapid pace. . Saint Petersburg Vs. Other Regions: . We compared Saint Petersburg against the national average. Now let&#39;s compare Saint Petersburg against the regions over two differrent periods by median alcohol consumption. As mentioned in the observations above (and fig 10), Saint Peterburg&#39;s alcohol consumption pattern changed in 2014. To put it in perspective, fig 12-15 compare Saint Petersburg&#39;s median alcohol consumption over two differrent periods, 1998-2013 and 2013-2016. . Fig 12: Median consumption of wine vs. beer from 1998-2013. Each circle is a region, orange circle is Saint Petersburg. Size of the circle shows vodka consumption. . Fig 13: Median consumption of wine vs. beer from 2014-2016. Each circle is a region, orange circle is Saint Petersburg. Size of the circle shows vodka consumption. . Fig 14: Median consumption of wine vs. vodka from 1998-2013. Each circle is a region, orange circle is Saint Petersburg. Size of the circle shows beer consumption. . Fig 15: Median consumption of wine vs. beer from 2014-2016. Each circle is a region, orange circle is Saint Petersburg. Size of the circle shows beer consumption. . All figures are zoomable and pannable and have tooltips. . L3Y_mean = df1.set_index(&#39;year&#39;).loc[&#39;2014&#39;:].iloc[:,:4].groupby(&#39;region&#39;).median().reset_index() L16Y_mean = df1.set_index(&#39;year&#39;).loc[:&#39;2013&#39;].iloc[:,:4].groupby(&#39;region&#39;).median().reset_index() (alt.Chart(L16Y_mean).mark_circle(size=100).encode( alt.X(&#39;beer&#39;, scale=alt.Scale(domain=[0,150])), alt.Y(&#39;wine&#39;,scale=alt.Scale(domain=[0,14])), tooltip=[&#39;region&#39;,&#39;wine&#39;,&#39;beer&#39;, &#39;vodka&#39;], color = alt.condition(&quot;datum.region == &#39;Saint Petersburg&#39;&quot;, alt.value(&quot;orange&quot;), alt.value(&quot;steelblue&quot;)), size=&#39;vodka&#39; ).properties(title =&#39;Fig 12:Median Consumption Wine vs. Beer (1998-2013)&#39;)).interactive() | (alt.Chart(L3Y_mean).mark_circle(size=100).encode( alt.X(&#39;beer&#39;, scale=alt.Scale(domain=[0,150])), alt.Y(&#39;wine&#39;, scale=alt.Scale(domain=[0,14])), tooltip=[&#39;region&#39;,&#39;wine&#39;,&#39;beer&#39;,&#39;vodka&#39;], color = alt.condition(&quot;datum.region == &#39;Saint Petersburg&#39;&quot;, alt.value(&quot;orange&quot;), alt.value(&quot;steelblue&quot;)), size=&#39;vodka&#39; ).properties(title =&#39;Fig 13:Median Consumption Wine vs. Beer (2014-2016)&#39;)).interactive() . (alt.Chart(L16Y_mean).mark_circle(size=100).encode( alt.X(&#39;wine&#39;, scale=alt.Scale(domain=[0,20])), alt.Y(&#39;vodka&#39;,scale=alt.Scale(domain=[0,22])), tooltip=[&#39;region&#39;,&#39;wine&#39;,&#39;beer&#39;, &#39;vodka&#39;], color = alt.condition(&quot;datum.region == &#39;Saint Petersburg&#39;&quot;, alt.value(&quot;orange&quot;), alt.value(&quot;steelblue&quot;)), size=&#39;beer&#39; ).properties(title =&#39;Fig 14:Median Consumption Wine vs. Vodka(1998-2013)&#39;)).interactive() |((alt.Chart(L3Y_mean).mark_circle(size=100).encode( alt.X(&#39;wine&#39;, scale=alt.Scale(domain=[0,20])), alt.Y(&#39;vodka&#39;, scale=alt.Scale(domain=[0,22])), tooltip=[&#39;region&#39;,&#39;wine&#39;,&#39;beer&#39;,&#39;vodka&#39;], color = alt.condition(&quot;datum.region == &#39;Saint Petersburg&#39;&quot;, alt.value(&quot;orange&quot;), alt.value(&quot;steelblue&quot;)), size=&#39;beer&#39; ).properties(title =&#39;Fig 15:Median Consumption Wine vs. Vodka(2014-2016)&#39;)).interactive()) . Observations: . In fig 12, we can clearly see beer consumption in Saint Petersburg was the highest on average over the 16 year period. It&#39;s the only region with beer consumption over 100 ltr per person on average ! There are more regions in the 40-60 lts range.Size of the circle also shows, all regions cosumed roughly similar amount of vodka. If we compare that with fig 12, not only beer but vodka consumption also reduced significantly. Moscow Oblast consumers started drinking more beer compared to national average. The shape of the scatterplot is also interesting. Circle in fig 12 are dispersed horizontally whereas circles in fig 13 are distributed vertically more, showing shift towards wine. Also, if you can imagine a line of best fit in these two scatterplots, in fig 12 the beer and wine preference have strong correlation, but in fig 13 that correlation seems weaker. Saint Petersburg is around in the middle of the scatter plot vertically, showing wine consumption is around the national average range. | In fig 14 &amp; 15, the vertical shift in circle can be easily observed, showing reduction in vodka consumption. Saint Petersburg is also towards the top third in wine consumption in 2014-2016 period. | Influence of Macroeconomic Factors . Research has shown that macroeconomic factors play a role in alcohol consumption pattern. One of the ways to gauge macroeconomic factor is by using gross domestic product (GDP) o fthe country. GDP is a measure of monetary value of goods produced in the country ref. We can use GDP of Russia from 1998 to 2016 and try to see if GDP has any correlation with consumption of alcohol. Data for GDP numbers is taken from here. . russia_gdp = pd.read_table(&#39;https://raw.githubusercontent.com/pawarbi/datasets/master/russia_gdp&#39;).sort_values(by=&#39;Year &#39;) russia_gdp.columns = russia_gdp.columns.str.replace(&#39; &#39;,&#39;&#39;).str.replace(&#39; &#39;,&#39;&#39;).str.lower() russia_gdp[&#39;percapita&#39;] = pd.to_numeric([x.replace(&#39;$&#39;,&#39;&#39;).replace(&#39;,&#39;,&#39;&#39;).rstrip().lstrip() for x in russia_gdp[&#39;percapita&#39;]]) russia_gdp[&#39;year&#39;]=pd.to_datetime(russia_gdp[&#39;year&#39;],format=&#39;%Y&#39;) russia_gdp.set_index(&#39;year&#39;,inplace=True) russia_gdp = russia_gdp[&#39;1998&#39;:&#39;2016&#39;] russia_gdp.head(3) . gdp percapita growthrate . year . 1998-01-01 $270.96B | 1835 | -5.30% | . 1999-01-01 $195.91B | 1331 | 6.40% | . 2000-01-01 $259.71B | 1772 | 10.00% | . russia_gdp2 = ((df1[[&#39;year&#39;,&#39;total&#39;]] .join(russia_gdp,on=&#39;year&#39;) .set_index(&#39;year&#39;)[[&#39;total&#39;,&#39;percapita&#39;]] .reset_index().groupby(&#39;year&#39;)[&#39;total&#39;,&#39;percapita&#39;] .mean() )) russia_gdp2.head(3) . total percapita . year . 1998-01-01 37.260988 | 1835.0 | . 1999-01-01 43.042963 | 1331.0 | . 2000-01-01 48.173210 | 1772.0 | . print(&quot;The correlation coefficient between total alcohol consumption and GDP per capita is: &quot;, round(russia_gdp2[&#39;total&#39;].corr(russia_gdp2[&#39;percapita&#39;]),2)) . The correlation coefficient between total alcohol consumption and GDP per capita is: 0.75 . total_vs_gdp = (alt.Chart(russia_gdp2.reset_index()) .mark_circle(size=60) .encode( x=&#39;percapita&#39;, y=&#39;total&#39;, tooltip=[&#39;year&#39;,&#39;total&#39;,&#39;percapita&#39;]) .properties(title=&#39;Fig 16Total Alcohol Consumption per capita vs. GDP per capita (1998-2016)&#39;) ) (total_vs_gdp + total_vs_gdp.transform_regression(&#39;percapita&#39;, &#39;total&#39;, method=&#39;linear&#39;).mark_line()).interactive() | (total_vs_gdp + total_vs_gdp.transform_regression(&#39;percapita&#39;, &#39;total&#39;, method=&#39;log&#39;).mark_line()).interactive() . Regression Analysis, GDP vs. Alcohol consumption . mod = sm.OLS(russia_gdp2.total.iloc[:-4], np.log(russia_gdp2.percapita.iloc[:-4])) res = mod.fit() print(res.summary()) . OLS Regression Results ======================================================================================= Dep. Variable: total R-squared (uncentered): 0.979 Model: OLS Adj. R-squared (uncentered): 0.977 Method: Least Squares F-statistic: 646.3 Date: Sat, 05 Feb 2022 Prob (F-statistic): 4.07e-13 Time: 02:49:34 Log-Likelihood: -56.716 No. Observations: 15 AIC: 115.4 Df Residuals: 14 BIC: 116.1 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] percapita 8.4538 0.333 25.422 0.000 7.741 9.167 ============================================================================== Omnibus: 5.388 Durbin-Watson: 0.126 Prob(Omnibus): 0.068 Jarque-Bera (JB): 3.462 Skew: -1.176 Prob(JB): 0.177 Kurtosis: 3.063 Cond. No. 1.00 ============================================================================== Notes: [1] R² is computed without centering (uncentered) since the model does not contain a constant. [2] Standard Errors assume that the covariance matrix of the errors is correctly specified. . russia_gdp2[&#39;pred&#39;] = (res.predict(np.log(russia_gdp2.percapita))) (russia_gdp2[&#39;total&#39;] .plot(legend=True, label=&#39;Total Consumption&#39;, title=&#39;Fig. 17: Actual vs. Predicted Total Consumption (All Regions)&#39;) ) (russia_gdp2[&#39;pred&#39;] .plot(legend=True, label=&#39;Predicted Consumption&#39;, color=&#39;red&#39;, style=&#39;--&#39;, figsize=(10,6)) ); . Observations: . In fig 16, total alcohol consumption per year and the GDP numbers for Russia for that year are plotted. For visual comparison, two lines are fitted, the one on the left is a linear fit and the one on the right si with log of GDP. We can clearly see that log-linear model fits better visually. . | For a more accuarate analysis, linear regression model using log of GDP is fit to predict the consumption of alcohol. Adjusted R2 values is 97% with P value significantly less than the critical 5% threshold. P value of Jarque Bera test on residuals also shows that the residuals are normally distributed thus no bias in the model. . | This shows that macroeconomic conditions of the country play a significant role in alcohol consumption behavior of the consumers. The practical significance of that is before launching the marketing campaign across the nation, the company should asess the macroeconomic factors to maximize the success of the campaign and sales. . | In fig 17, the linear model is used to predict the total alcohol consumption. As we can see, although the numbers are very accurate, it captured the trend very well. . | Regional GDP comparison . We are interested in regional pattern in alcohol consumption, so lets check regional GDP numbers and compare them with Saint Peterburg&#39;s GDP. We can use the regional GDP numbers from here . ru_gdp_regional = pd.read_table(&#39;https://raw.githubusercontent.com/sapawar4/datasets/main/russia_gdp_regional&#39;) ru_gdp_regional.head(3) . Region GDP . 0 Republic of Adygea | 3790 | . 1 Altai Krai | 3730 | . 2 Amur Oblast | 6000 | . print(&quot;This data has GDP of&quot;, len(ru_gdp_regional), &quot;regions of Russia, for year 2018&quot;) . This data has GDP of 80 regions of Russia, for year 2018 . This is the GDP of differrent regions of Russia for year 2018. Since the alcohol data is from 1996-2016, we can&#39;t use these number directly but we can normalize the numbers by Saint Petersburg&#39;s GDP to get an idea of relative gdp. Regions with GDP &gt;= 1 indicate GDP of those regions are equal to greater than that of Saint Petersburg&#39;s. . Russia&#39;s average inflation between 2016-2018 was ~3.5% (source: https://www.macrotrends.net/countries/RUS/russia/inflation-rate-cpi). Since the alcohol consumption data is only up to 2016, we can adjust the 2018 GDP numbers by inflation to get an approximate estimtion of 2016 GDP numbers. . ru_gdp_regional[&#39;GDP&#39;] = np.round(np.divide(ru_gdp_regional[&#39;GDP&#39;],1.035**2),0) ru_gdp_regional.head(3) . Region GDP . 0 Republic of Adygea | 3538.0 | . 1 Altai Krai | 3482.0 | . 2 Amur Oblast | 5601.0 | . ru_gdp_regional[&#39;rgdp&#39;] = (np.divide( ru_gdp_regional[&#39;GDP&#39;], ru_gdp_regional.query(&#39;Region == &quot;Saint Petersburg&quot;&#39;)[&#39;GDP&#39;].max()) ) (alt.Chart(ru_gdp_regional).mark_bar() .encode(x=&#39;Region:N&#39;, y = &#39;rgdp:Q&#39;, tooltip=[&#39;Region&#39;,&#39;GDP&#39;,&#39;rgdp&#39;], color = alt.condition(&quot;datum.rgdp &lt; 1 &quot;, alt.value(&quot;steelblue&quot;), alt.value(&quot;orange&quot;))) .properties(width=900, title = &#39;Fig 18: Relative adjusted GDP of differrent regions of Russia&#39;) .interactive() ) . yr_median = pd.DataFrame(df1.set_index(&#39;year&#39;)[&#39;2014&#39;:].groupby(&#39;region&#39;)[&#39;total&#39;].median()) . ru_gdp_regional=ru_gdp_regional.set_index(&#39;Region&#39;).join(yr_median).reset_index() ru_gdp_regional= ru_gdp_regional.dropna() ru_gdp_regional.head(3) . Region GDP rgdp total . 0 Republic of Adygea | 3538.0 | 0.305632 | 36.0 | . 1 Altai Krai | 3482.0 | 0.300795 | 55.9 | . 2 Amur Oblast | 5601.0 | 0.483846 | 65.4 | . regional_gdp_chart = (alt.Chart(ru_gdp_regional) .mark_circle(size=60) .encode( x=&#39;GDP&#39;, y=&#39;total&#39;, tooltip=[&#39;Region&#39;,&#39;total&#39;,&#39;GDP&#39;], color = alt.condition(&quot;datum.Region == &#39;Saint Petersburg&#39;&quot;, alt.value(&quot;orange&quot;), alt.value(&quot;steelblue&quot;))) ) box_lin = alt.Chart(ru_gdp_regional).mark_boxplot().encode(x=&#39;GDP&#39;, tooltip=[&#39;Region&#39;]).properties(height = 100,title = &#39;Distribution of GDP w Outliers&#39;) combo1 = (regional_gdp_chart + regional_gdp_chart.transform_regression(&#39;GDP&#39;, &#39;total&#39;, method=&#39;log&#39;).mark_line() &amp; box_lin).properties(title=&#39;Fig 19 Median Comsumption vs GDP with Outliers&#39;) . ru_gdp_regional.sort_values(&#39;GDP&#39;,ascending=False).iloc[:3] . Region GDP rgdp total . 40 Nenets Autonomous Okrug | 102985.0 | 8.896424 | 85.9 | . 78 Yamalo-Nenets Autonomous Okrug | 84613.0 | 7.309347 | 99.1 | . 59 Sakhalin Oblast | 35679.0 | 3.082153 | 103.2 | . regional_gdp_chart2 = (alt.Chart(ru_gdp_regional.sort_values(&#39;GDP&#39;,ascending=False).iloc[3:]) .mark_circle(size=60) .encode( x=&#39;GDP&#39;, y=&#39;total&#39;, tooltip=[&#39;Region&#39;,&#39;total&#39;,&#39;GDP&#39;], color = alt.condition(&quot;datum.Region == &#39;Saint Petersburg&#39;&quot;, alt.value(&quot;orange&quot;), alt.value(&quot;steelblue&quot;))) ) box_lin2 = alt.Chart(ru_gdp_regional.sort_values(&#39;GDP&#39;,ascending=False).iloc[3:]).mark_boxplot().encode(x=&#39;GDP&#39;, tooltip=[&#39;Region&#39;]).properties(height = 100, title = &#39;Distribution of GDP w/o Outliers&#39;) combo2 = (regional_gdp_chart2 + regional_gdp_chart2.transform_regression(&#39;GDP&#39;, &#39;total&#39;, method=&#39;log&#39;).mark_line() &amp; box_lin2).properties(title=&#39;Fig 20 Median Comsumption vs GDP without Outliers&#39;) . (combo1 | combo2).configure_view(strokeWidth=0) . print(&quot;The correlation coefficient between regional GDP and median alcohol consumption across differrent regions is&quot;,round((ru_gdp_regional.iloc[6:].total).corr(np.log(ru_gdp_regional.iloc[6:].GDP)),2)) . The correlation coefficient between regional GDP and median alchol consumption across differrent regions is 0.52 . Observations: . Fig 18 compares the regional GDP numbers relative to Saint Petersburg. There are 11 regions that have GDP equal to or more than that of Saint Peterburg&#39;s. | Fig 19 and 20 show scatterplot of GDP and median alcohol consumption for year 2016 for 80 regions in Russia. The boxplot at the bottom of the scatterplot show the distribution of GDP and presence of outliers. For example the left scatterplot shows there are there are at least 2 significant outliers. Fig 20 shows the same plot with outliers removed. Orange circle is Saint Petersburg. The log linear model similar to one used earlier also fits very well in this case, showing overall the GDP vs. consumption model holds good even at regional level. | The correlation coefficient between regional GDP and alcohol consumption is 0.52, showing moderate correlation. There could be other factors such as gender mix, inflation, personal income, supply chain factors, government policies etc that may influence consumption not just GDP. | The significance of this is that regional GDP should be considered as one of the factors to determine the regions that are similar to Saint Petersburg. It&#39;s definitely not the only factor but could be one of the main factors based on the analyses above. | Cross Correlation Analysis . Cross correlation analysis allows us to compare two time series. It measures a degree of similarity between a time series and lagged verion of another series. This will help us find regions similar to Saint Petersburg based on total alcohol consumption. . def get_ccf(s1,s2,lag, topN): ccf_dict = {} for r in list(nonstp.region.unique()): s2 = (nonstp.query(&#39;region == @r&#39;)[&#39;total&#39;].values) ccf = sm.tsa.stattools.ccf(stp_total, s2)[lag] ccf_dict[r]=[ccf] return pd.DataFrame(ccf_dict).T.nlargest(topN, [0]) . stp_total = df1.query(&#39;region == &quot;Saint Petersburg&quot;&#39;)[&#39;total&#39;].values nonstp = df1[[&#39;year&#39;,&#39;region&#39;,&#39;total&#39;]].query(&#39;region != &quot;Saint Petersburg&quot;&#39;) get_ccf(stp_total,nonstp, lag=0,topN=20).plot.barh(figsize=(12,8), title=&#39;Fig 21: Top 20 regions by cross-correlation&#39;, label=False); . (pd.Series( sm.tsa.stattools.ccf(stp_total, df1.query(&#39;region == &quot;Republic of Dagestan&quot;&#39;)[&#39;total&#39;].values))[:12] .plot.bar(title=&#39;Fig 22:Cross Correlation: Saint Peterburg vs. Republic of Dagestan&#39;, figsize=(12,6)) ); . Observations: . Fig 21 shows the top 20 regions that have high cross correlation coefficient with Saint Petersburg. Region with highest correlation by total consumption is Republic of Dagestan. Only the first lag is considered in this figure. | In fig 22, for republic of Dagestan, first 12 lags are plotted. It shows that first and second lags are quite significant. This implies total cosumption in Dagestan from two years ago can be used to predict the alcohol consumption in Saint Petersburg. Although that doesn&#39;t help practically, we at least know now which regions are similar to Saint Petersburg based on total consumption. | Clustering . Time series data can be clustered using various ways: . Distance based clustering : Instead of using euclidean distance, we can use Dynamic Time Warping (DTW) to find similarity between two time series. This is a shape based approach meaning the two series are considered similar if their shapes match based on DTW metric. . | Functional data analysis: In this case, the basis function of the time series is obtained, and time series are clustered based on the similarity of the function. . | Feature based : Features or characteristics of the time series based on its temporal structure are calculated and then those are used for clustering. . | The main downside of the first two methods is that they do not provide much explainable insights into why and how the series are clustered. We will use the feature based approach instead. In fetaure based approach, various metrics such as mean, mode, skewness, lags, ACF, PACF etc. along with many other metrics are used. In this case, we will use catch22 features which have been shown to capture temporal structure of the series adequately without sacrificing the accuracy. Ref. . From our initial analysis we know that, in Saint Petersburg: . Overall alcohol consumption has decreased, especially in the last ~5 years | Consumption of beer has decreased drastically | There is an uptick in wine consumption and decrease in vodka consumption | Although more weak alcohol is consumed, preference for hard liquer has been increasing | Macroeconomic factors such as GDP influence alcohol consumption | . We want to capture these features in the time series and find regions that show similar characteristics. . Modeling methodology: . Combine consumption of alcohols in two groups based on alcohol content - hard (vodka + brandy) and week (beer + wine + champagne) | Obtain &#39;catc22&#39; features for both of these alcohols types | Cluster the data using K-means algorithm | Identify regions that are common in both clusters | df1_l_hard = df1[[&#39;region&#39;,&#39;hard&#39;]].set_index(&#39;region&#39;) df1_pivot_hard = df1[[&#39;year&#39;,&#39;region&#39;,&#39;hard&#39;]].pivot(index=&#39;year&#39;,columns=&#39;region&#39;) c22_h = pd.DataFrame() for region in set(df1.region.unique()): c22_h[region] = catch22_all(df1_l_hard.loc[region][&#39;hard&#39;].values)[&#39;values&#39;] c22_h.index = catch22_all(df1_l_hard.loc[region][&#39;hard&#39;].values)[&#39;names&#39;] . ## Time series features for hard alcohols c22_hard = c22_h.T c22_hard . DN_HistogramMode_5 DN_HistogramMode_10 CO_f1ecac CO_FirstMin_ac CO_HistogramAMI_even_2_5 CO_trev_1_num MD_hrv_classic_pnn40 SB_BinaryStats_mean_longstretch1 SB_TransitionMatrix_3ac_sumdiagcov PD_PeriodicityWang_th0_01 ... FC_LocalSimple_mean1_tauresrat DN_OutlierInclude_p_001_mdrmd DN_OutlierInclude_n_001_mdrmd SP_Summaries_welch_rect_area_5_1 SB_BinaryStats_diff_longstretch0 SB_MotifThree_quantile_hh SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 SP_Summaries_welch_rect_centroid FC_LocalSimple_mean3_stderr . Nizhny Novgorod Oblast 0.732248 | 0.547634 | 3.0 | 8.0 | 0.485987 | -0.050743 | 1.000000 | 11.0 | 0.062500 | 3.0 | ... | 0.250000 | 0.078947 | 0.894737 | 0.653610 | 8.0 | 1.798106 | 0.0 | 0.0 | 0.392699 | 0.735469 | . Magadan Oblast -0.614018 | -0.423923 | 2.0 | 4.0 | 0.318822 | 0.026515 | 0.888889 | 4.0 | 0.046296 | 2.0 | ... | 0.333333 | -0.631579 | 0.052632 | 0.185831 | 7.0 | 1.878967 | 0.0 | 0.0 | 0.589049 | 0.982601 | . Lipetsk Oblast 0.380434 | 0.160263 | 2.0 | 10.0 | 0.475436 | -1.053809 | 0.944444 | 13.0 | 0.166667 | 0.0 | ... | 0.142857 | -0.210526 | 0.894737 | 0.409234 | 6.0 | 1.827175 | 0.0 | 0.0 | 0.589049 | 0.472617 | . Republic of North Ossetia-Alania -0.400949 | -0.557504 | 4.0 | 14.0 | 0.763766 | -0.034504 | 0.888889 | 7.0 | 0.074074 | 3.0 | ... | 0.333333 | -0.684211 | 0.842105 | 0.660165 | 7.0 | 1.611158 | 0.0 | 0.0 | 0.196350 | 0.311650 | . Orenburg Oblast 0.327689 | 0.483742 | 4.0 | 10.0 | 0.665650 | -0.087016 | 1.000000 | 11.0 | 0.166667 | 5.0 | ... | 0.142857 | -0.473684 | 0.894737 | 0.658906 | 4.0 | 1.644073 | 0.0 | 0.0 | 0.196350 | 0.488831 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Kostroma Oblast 0.568742 | 0.736033 | 4.0 | 10.0 | 0.625786 | -0.075374 | 1.000000 | 10.0 | 0.166667 | 0.0 | ... | 0.125000 | -0.421053 | 0.894737 | 0.715419 | 5.0 | 1.846053 | 0.0 | 0.0 | 0.196350 | 0.517566 | . Kursk Oblast 0.817754 | 0.676729 | 2.0 | 7.0 | 0.549905 | -0.429980 | 0.944444 | 12.0 | 0.062500 | 2.0 | ... | 1.250000 | 0.289474 | 0.789474 | 0.502553 | 4.0 | 1.849898 | 0.0 | 0.0 | 0.392699 | 0.876202 | . Stavropol Krai -0.496139 | -0.496139 | 4.0 | 14.0 | 0.841241 | -0.028304 | 0.944444 | 8.0 | 0.166667 | 4.0 | ... | 0.285714 | -0.684211 | 0.789474 | 0.703954 | 5.0 | 1.611158 | 0.0 | 0.0 | 0.196350 | 0.303376 | . Kaliningrad Oblast 0.402788 | 0.402788 | 2.0 | 6.0 | 0.431913 | -0.128262 | 0.944444 | 8.0 | 0.062500 | 3.0 | ... | 0.250000 | -0.315789 | 0.894737 | 0.441250 | 4.0 | 2.003931 | 0.0 | 0.0 | 0.785398 | 0.882366 | . Voronezh Oblast 0.358083 | -0.452264 | 2.0 | 8.0 | 0.621714 | -0.240525 | 0.888889 | 9.0 | 0.074074 | 3.0 | ... | 0.400000 | -0.157895 | 0.894737 | 0.530539 | 4.0 | 1.692020 | 0.0 | 0.0 | 0.392699 | 0.689238 | . 81 rows × 22 columns . df1_l_wk = df1[[&#39;region&#39;,&#39;weak&#39;]].set_index(&#39;region&#39;) df1_pivot_wk = df1[[&#39;year&#39;,&#39;region&#39;,&#39;weak&#39;]].pivot(index=&#39;year&#39;,columns=&#39;region&#39;) c22_w = pd.DataFrame() for region in df1.region.unique(): c22_w[region] = catch22_all(df1_l_wk.loc[region][&#39;weak&#39;].values)[&#39;values&#39;] c22_w.index = catch22_all(df1_l_wk.loc[region][&#39;weak&#39;].values)[&#39;names&#39;] . #Time series features for weak alcohols c22_weak= c22_w.T c22_weak . DN_HistogramMode_5 DN_HistogramMode_10 CO_f1ecac CO_FirstMin_ac CO_HistogramAMI_even_2_5 CO_trev_1_num MD_hrv_classic_pnn40 SB_BinaryStats_mean_longstretch1 SB_TransitionMatrix_3ac_sumdiagcov PD_PeriodicityWang_th0_01 ... FC_LocalSimple_mean1_tauresrat DN_OutlierInclude_p_001_mdrmd DN_OutlierInclude_n_001_mdrmd SP_Summaries_welch_rect_area_5_1 SB_BinaryStats_diff_longstretch0 SB_MotifThree_quantile_hh SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 SP_Summaries_welch_rect_centroid FC_LocalSimple_mean3_stderr . Republic of Adygea 0.595243 | 0.775842 | 2.0 | 5.0 | 0.252250 | 0.463747 | 1.000000 | 11.0 | 0.062500 | 4.0 | ... | 0.250000 | 0.315789 | -0.842105 | 0.442748 | 3.0 | 1.812143 | 0.0 | 0.0 | 0.589049 | 0.616533 | . Altai Krai 0.578897 | 0.387142 | 3.0 | 6.0 | 0.625786 | -0.026379 | 1.000000 | 12.0 | 0.074074 | 0.0 | ... | 0.600000 | 0.473684 | -0.789474 | 0.579394 | 4.0 | 1.849898 | 0.0 | 0.0 | 0.392699 | 0.750386 | . Amur Oblast 1.194978 | 1.036788 | 4.0 | 10.0 | 0.897133 | -0.009958 | 0.944444 | 10.0 | 0.074074 | 0.0 | ... | 0.833333 | 0.421053 | -0.684211 | 0.842913 | 3.0 | 1.549173 | 0.0 | 0.0 | 0.196350 | 0.552741 | . Arkhangelsk Oblast -0.483755 | -0.246369 | 2.0 | 4.0 | 0.296951 | -0.712265 | 0.888889 | 4.0 | 0.046296 | 4.0 | ... | 0.333333 | -0.368421 | -0.789474 | 0.103270 | 6.0 | 1.878967 | 0.0 | 0.0 | 0.785398 | 1.149274 | . Astrakhan Oblast 0.932883 | 1.088741 | 3.0 | 11.0 | 0.509388 | -0.102313 | 0.944444 | 12.0 | 0.074074 | 3.0 | ... | 1.000000 | 0.421053 | -0.789474 | 0.668325 | 4.0 | 1.688174 | 0.0 | 0.0 | 0.392699 | 0.726105 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Chuvash Republic 0.829517 | 0.647259 | 4.0 | 8.0 | 0.679860 | 0.127388 | 0.888889 | 12.0 | 0.074074 | 0.0 | ... | 0.166667 | 0.421053 | -0.736842 | 0.595159 | 4.0 | 1.611158 | 0.0 | 0.0 | 0.392699 | 0.470437 | . Chukotka Autonomous Okrug -0.514813 | -0.342103 | 4.0 | 10.0 | 0.792951 | 0.063017 | 1.000000 | 6.0 | 0.166667 | 3.0 | ... | 0.125000 | 0.789474 | -0.736842 | 0.619796 | 3.0 | 1.812143 | 0.0 | 0.0 | 0.196350 | 0.411099 | . Sakha (Yakutia) Republic 0.833131 | 0.959731 | 5.0 | 13.0 | 0.987765 | 0.017513 | 0.722222 | 10.0 | 0.166667 | 0.0 | ... | 0.571429 | 0.631579 | -0.631579 | 0.824523 | 2.0 | 1.398986 | 0.0 | 0.0 | 0.196350 | 0.274629 | . Yamalo-Nenets Autonomous Okrug 0.576638 | 0.735361 | 3.0 | 8.0 | 0.373232 | 0.095685 | 0.944444 | 13.0 | 0.074074 | 0.0 | ... | 1.000000 | 0.210526 | -0.736842 | 0.583341 | 5.0 | 1.812143 | 0.0 | 0.0 | 0.392699 | 0.654370 | . Yaroslavl Oblast 0.362071 | 0.884441 | 2.0 | 4.0 | 0.237669 | -0.264563 | 0.833333 | 10.0 | 0.046296 | 5.0 | ... | 0.666667 | 0.315789 | -0.315789 | 0.323033 | 3.0 | 1.937106 | 0.0 | 0.0 | 0.785398 | 1.014128 | . 81 rows × 22 columns . kmeans1 = KMeans(n_clusters=4, random_state=1).fit(c22_hard) kmeans2 = KMeans(n_clusters=4, random_state=1).fit(c22_weak) c22_final = pd.DataFrame() c22_final[&#39;regions&#39;] = list(c22_weak.index) c22_final[&#39;hard_cluster&#39;] = kmeans1.labels_ c22_final[&#39;weak_cluster&#39;] = kmeans2.labels_ c22_final . regions hard_cluster weak_cluster . 0 Republic of Adygea | 3 | 2 | . 1 Altai Krai | 0 | 0 | . 2 Amur Oblast | 2 | 3 | . 3 Arkhangelsk Oblast | 1 | 1 | . 4 Astrakhan Oblast | 3 | 2 | . ... ... | ... | ... | . 76 Chuvash Republic | 2 | 0 | . 77 Chukotka Autonomous Okrug | 3 | 2 | . 78 Sakha (Yakutia) Republic | 1 | 3 | . 79 Yamalo-Nenets Autonomous Okrug | 0 | 0 | . 80 Yaroslavl Oblast | 3 | 1 | . 81 rows × 3 columns . c22_final.query(&#39;regions == &quot;Saint Petersburg&quot;&#39;) . regions hard_cluster weak_cluster . 57 Saint Petersburg | 0 | 0 | . h__ = int(c22_final.query(&#39;regions == &quot;Saint Petersburg&quot;&#39;)[&#39;hard_cluster&#39;].values) w__ = int(c22_final.query(&#39;regions == &quot;Saint Petersburg&quot;&#39;)[&#39;weak_cluster&#39;].values) . Now let&#39;s get similar regions for both alcohol types. . c22_final.query(&#39;hard_cluster == @h__&#39;)[&#39;regions&#39;] . 1 Altai Krai 6 Belgorod Oblast 7 Bryansk Oblast 12 Voronezh Oblast 15 Zabaykalsky Krai 16 Ivanovo Oblast 17 Irkutsk Oblast 18 Kabardino-Balkar Republic 20 Republic of Kalmykia 24 Republic of Karelia 27 Kostroma Oblast 28 Krasnodar Krai 29 Krasnoyarsk Krai 32 Leningrad Oblast 37 Moscow 39 Murmansk Oblast 43 Novosibirsk Oblast 46 Oryol Oblast 47 Penza Oblast 49 Primorsky Krai 53 Tuva Republic 54 Rostov Oblast 55 Ryazan Oblast 56 Samara Oblast 57 Saint Petersburg 59 Sakhalin Oblast 60 Sverdlovsk Oblast 65 Republic of Tatarstan 67 Tomsk Oblast 68 Tula Oblast 69 Tyumen Oblast 71 Ulyanovsk Oblast 72 Khabarovsk Krai 75 Chelyabinsk Oblast 79 Yamalo-Nenets Autonomous Okrug Name: regions, dtype: object . c22_final.query(&#39;weak_cluster == @w__&#39;)[&#39;regions&#39;] . 1 Altai Krai 5 Republic of Bashkortostan 7 Bryansk Oblast 8 Republic of Buryatia 11 Vologda Oblast 13 Republic of Dagestan 17 Irkutsk Oblast 25 Kemerovo Oblast 31 Kursk Oblast 35 Mari El Republic 41 Nizhny Novgorod Oblast 43 Novosibirsk Oblast 45 Orenburg Oblast 47 Penza Oblast 57 Saint Petersburg 60 Sverdlovsk Oblast 67 Tomsk Oblast 69 Tyumen Oblast 74 Khanty–Mansi Autonomous Okrug – Yugra 75 Chelyabinsk Oblast 76 Chuvash Republic 79 Yamalo-Nenets Autonomous Okrug Name: regions, dtype: object . #Find regions that are common in both hard &amp; weak alcohol clusters similar = set(c22_final.query(&#39;weak_cluster == @w__&#39;)[&#39;regions&#39;].values).intersection(set(c22_final.query(&#39;hard_cluster == @h__&#39;)[&#39;regions&#39;].values)) similar . {&#39;Altai Krai&#39;, &#39;Bryansk Oblast&#39;, &#39;Chelyabinsk Oblast&#39;, &#39;Irkutsk Oblast&#39;, &#39;Novosibirsk Oblast&#39;, &#39;Penza Oblast&#39;, &#39;Saint Petersburg&#39;, &#39;Sverdlovsk Oblast&#39;, &#39;Tomsk Oblast&#39;, &#39;Tyumen Oblast&#39;, &#39;Yamalo-Nenets Autonomous Okrug&#39;} . Out of 81 regions, 10 regions show patters that show trends for hard &amp; weak alcohols that are similar to that of Saint Petersburg. Let&#39;s look at those closely. You can in the charts below, feature based clustering method identified regions that follow similar time series pattern. . Beer Consumption . for r in similar: df1_melt[(df1_melt.region.isin(similar))].query(&#39;Alcohol == &quot;beer&quot;&#39;).set_index(&#39;year&#39;).query(&#39;region == @r&#39;)[&#39;Consumption&#39;].plot() . Wine Consumption . for r in similar: df1_melt[(df1_melt.region.isin(similar))].query(&#39;Alcohol == &quot;wine&quot;&#39;).set_index(&#39;year&#39;).query(&#39;region == @r&#39;)[&#39;Consumption&#39;].plot() . Vodka Consumption . for r in similar: df1_melt[(df1_melt.region.isin(similar))].query(&#39;Alcohol == &quot;vodka&quot;&#39;).set_index(&#39;year&#39;).query(&#39;region == @r&#39;)[&#39;Consumption&#39;].plot() . Conclusion: . It is recommended to run the next campaign in following 10 regions: &#39;Altai Krai&#39;, &#39;Bryansk Oblast&#39;, &#39;Chelyabinsk Oblast&#39;, &#39;Irkutsk Oblast&#39;, &#39;Novosibirsk Oblast&#39;, &#39;Penza Oblast&#39;, &#39;Sverdlovsk Oblast&#39;, &#39;Tomsk Oblast&#39;, &#39;Tyumen Oblast&#39;, &#39;Yamalo-Nenets Autonomous Okrug&#39; | In the analysis it was found that consumers in Saint Petersburg have been consuming less alcohol compared to the past, especially beer. They have also been preferring wine over vodka in the last 3 years. It was also found that although the consumers have been drinking less alcohol, the preference towards hard alcohols (vodka &amp; brandy) has been picking up over weak alcohols (beer, wine and champagne) . | To find regions similar to that of Saint Petersburg, feature based time series clustering technique was used to find regions that show similar trends for both hard and weak alcohols. . | Analysis showed that macroeconomic factors play an important rule in alcohol consumption. We recommend that the company assess macroeconomics (CPI, GDP, international markets etc.) to decide when to launch the campaign. We could not include GDP data in clustering due to unavailability of data for all regions, but we recommend carrying out additional clustering with inclusion of GDP to find similar regions. . | Limitations: . I did not focus on any geospatial analysis. Russia is a vast country and many regions operate autonomously so there are likely to be strong differrences among some regions despite being similar in alchol volume consumption. . | I have made assumptions regarding GDP numbers . | One limitation of K-Means is its sensitivity to initialization. Although I spent some time tuning parameters, I did not include it in the analysis here to keep it short. In my real project, I used GMM instead of K-Means. . | Catch22 is one of the many libraries that allow time series feature extraction. You can also take a look at tsfresh and tsfeat . |",
            "url": "https://pawarbi.github.io/blog/python/timeseries/clustering/catch22/eda/altair/dataviz/2022/02/04/feature-based-clustering-time-series.html",
            "relUrl": "/python/timeseries/clustering/catch22/eda/altair/dataviz/2022/02/04/feature-based-clustering-time-series.html",
            "date": " • Feb 4, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Parameterizing Value.NativeQuery For Query Folding",
            "content": "Query Folding . In a recent video by Guy In A Cube, Adam showed a great trick to pass a list to the where clause in a SQL statement. I want to show another approach which is more parametric and can fold. It does come with one drawback that I will share. Watch Adam&#39;s video first: . . Query Folding Using Value.NativeQuery() . It&#39;s known that instead of pasting query in the SQL server connection dialog box like below, you can use the same SQL statement in Value.NativeQuery() to make it foldable. This has the advantage of potentially speeding up the refresh times by offloading all the processing to the server instead of the gateway. I highly recommend reading this blog by Chris Webb, which is how I learned about this function. I have also added few more resources about Query Folding at the end of the blog, please check them out. . This will not fold : . . Using Value.NativeQuery() . This will fold: . . What&#39;s not generally known is that you can pass parameters to the Value.NativeQuery() using the optional third argument. Again, I learned about it from Chris&#39; blog here. I modified the SQL statement and passed a parameter @Date by using the third optional argument as shown below. You can use as many parameters as you like. The query is still foldable. . . Note: Be sure to set [EnableFolding=true] to fold subsequent query steps. If you don&#8217;t specify this, your initial query will fold but not the queries following the initial query . . But What About Lists ? . Good question. If we try to pass a list in a where clause like Adam did, however, it will not work. See below. The parameters only work for single values. . . . The Solution : Use JSON . The solution to this is using OPENJSON format to pass the values. I have not tested it against all the foldable sources. It certainly works with SQL Server and Synapse Analytics. I found this solution last year on a forum somewhere and I can&#39;t find the source anymore unfortunately. If anyone knows it, please let me know so I can attribute to the original author. . Create the parameter as a JSON object and read it in SQl statement using OPENJSON . . . What If You Have An Existing List ? . If you have an existing list that&#39;s derived from a column, you can use Text.Combine(), like Adam did, and pass it to the JSON parameter we created. If you want to pass numbers to the where clause like I did above, here are the steps to pass your list named _mylist as a parameter: . First convert the numbers to text, you can use Number.ToText(). Easier to just convert the column to text before creating the list | Concatenate the list: &quot;[&quot; &amp; Text.Combine(_mylist,&quot;,&quot;) &amp; &quot;]&quot; | Now you can specify this list as a parameter directly in the Value.NativeQuery() parameters option | . The only drawback here is the automatically generated SQL query does not show the values passed to IN. But it folds, that&#39;s what matters. . Resources . Other than Chris Webb&#39;s blog, here are couple more resources on Query Folding: . 30DQUERY Challenge . If you want to improve your M and learn the techniques to make queries foldable, I highly recommend participating in Alex Powers&#39; #30DQUERY challenge. . Ben Gribaudo&#39;s Blog . Ben&#39;s blog is the most comprehensive resource on Query Folding. If you want to get deeper understanding of query folding and M in general, you have to read Ben&#39;s blog. . Watch How Power Query Thinks by Ben .",
            "url": "https://pawarbi.github.io/blog/power%20bi/powerquery/queryfolding/m/optimization/2022/01/25/parameter-valuenativequery-query-folding-where-clause-in-powerbi.html",
            "relUrl": "/power%20bi/powerquery/queryfolding/m/optimization/2022/01/25/parameter-valuenativequery-query-folding-where-clause-in-powerbi.html",
            "date": " • Jan 25, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Adding Interactive Widgets To Visuals Using Deneb in Power BI",
            "content": "Deneb . Deneb is a free Microsoft certified custom visual available in the Apps gallery. Unlike many other custom visuals, it&#39;s highly customizable and fairly easy to setup. It uses JSON syntax of the Vega/Vega-Lite languages to create visuals. You can read more about it on it&#39;s official site. Kudos to its creator Daniel Marsh-Patrick for open-sourcing it. Please support his efforts here. Also check out some awesome visuals created by Kerry Kolosko using Deneb. . As I have mentioned before on my blog, I use Python &amp; Power BI together in my workflow. Since Deneb uses Vega-Lite, I can use any other Vega-Lite library to develop the visuals and use them in Power BI with Deneb. While you can build Deneb visuals using JSON, I like using Altair to create the visuals and then use that JSON in Deneb. I have already created a video on how you can do that. Please watch it to learn more, I won&#39;t cover it here again. . The goal of this blog is to show you three things that are not possible in native Power BI visuals: . Adding interactive widgets that are bound to a visual | Creating data-driven conditional labels | Creating composite visuals by layering and adding visuals to each other to build more complex visuals | . Be sure read my other blog on changing color transperancy using native Power BI visuals. You can achieve similar effect in Power BI but it&#39;s not as responsive and has limitations. . . Adding Interactive Widgets . You can think of widgets as the slicers that are bound to a visual and can be used by the user to control various formatting options of the visual. This can be used to personalize the visual, as well help in exploring the data interactively. These widgets can be slicers, radio buttons, dropdowns etc. Unlike the Power BI slicer, since these are bound to the visual, they are very responsive. I will show couple of example of how you can use Altair to first build the visual and then use the JSON to re-create it in Power BI. . Create Widget Using Altair . To build the visual, we don&#39;t really need data from Power BI. We can use some dummy data to build the visual, which can then be transferred to Deneb. I will use Pandas for creating the dataframe. . import pandas as pd import altair as alt import numpy as np print(&quot;Altair version:&quot;,alt.__version__) . Altair version: 4.1.0 . Creating a dummy dataframe with X, Y, Z columns with 50 observations. . df = pd.DataFrame({ &#39;X&#39;: range(50), &#39;Y&#39;: np.random.rand(50).cumsum(), &#39;Z&#39;: np.random.rand(50)*100 } ).round(0) df.head() . X Y Z . 0 0 | 1.0 | 60.0 | . 1 1 | 1.0 | 19.0 | . 2 2 | 1.0 | 77.0 | . 3 3 | 2.0 | 31.0 | . 4 4 | 3.0 | 51.0 | . Building Base Visuals . I will build two visuals and show how you can create a composite visual. Try interacting with the visual by zooming and hovering. I have annotated the code below if you are not familiar with Altair. If you have never used Altair before, still follow along. The code we generate from the visual can be applied to any data in Power BI. . In this example, we want to build an interactive, composite visual to analyze multivariate data. The bar chart shows X vs. Z and the scatter plot shows X vs Y. We are interested in analyzing the Z variable. By adding interactivity, we can visualize Z in XZ and YZ planes in a single visual. . First I will create the base visuals with conditional formatting and then show how to add the widget. . Bar Chart . bar = (alt.Chart(df).mark_bar().encode( #Create a bar chart using df dataframe x=&#39;X&#39;, #X variable y=&#39;Z&#39;, #Y variable is Z tooltip=[&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;], #Add tooltip color=alt.condition( alt.datum.Z&lt; 20, #Conditional formatting, threshold 20 alt.value(&#39;#477998&#39;), alt.value(&#39;hsla(232, 7%, 20%, 0.25)&#39;), #if Z &lt; 20, Blue otherwise gray ) ).properties(width=500, height=400, title=&quot;Bar Chart with Widget&quot;)) text = bar.mark_text( align=&#39;right&#39;, #add text label baseline=&#39;middle&#39;, #align data labels dx= 3, dy= -5 #align x &amp; y positions of labels ).encode( text=&#39;Z:Q&#39;, color=alt.condition( alt.datum.Z&lt; 20, #Conditional formatting, threshold 20 alt.value(&#39;#477998&#39;), alt.value(&#39;hsla(232, 7%, 20%, 0)&#39;)) #only show label if Z &lt; 20, 0 is for alpha ) bartext = (bar + text).interactive() #add bar chart layer to text layer bartext . Scatter Plot . scatter = (alt.Chart(df).mark_circle().encode( #Create a scatterplot using df dataframe x=&#39;X&#39;, #X axis is X y=&#39;Y&#39;, #Y axis uses Y variable tooltip=[&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;], color=alt.condition( alt.datum.Z &lt; 20, alt.value(&#39;red&#39;), alt.value(&#39;hsla(232, 7%, 20%, 0.25)&#39;) ) ).properties(width=300, height=400, title=&quot;Sactterplot with Widget&quot;)) scatter.interactive() . Let&#39;s combine these two to make a composite chart. This is a single visual now with two chart types. . (bartext | scatter) . Few things to notice in the code above: . For the bar chart, I have defined an alternate condition that values less than 20 are blue in color and values above 20 are gray | I added text as another chart on top of the bar chart. This allow us to create data-driven labels. In the options, notice I defined the text color as hsla(232, 7%, 20%, 0). The last value here 0 is the alpha that defines the transperancy. If Z &gt; 20, the text will be become transparent and only values below 20 will be appear. | For scatterplot, values below 20 are red and values above 20 are gray | I combined the two charts together using &quot; | &quot;. | . In the base visuals, I defined the color threshold 20 manually. Now we want to add a slicer widget so the user can control that threshold. To do that, we have to define a selector and bind that to the visuals above. . Below I am defining the min and max range for the slicer, name of the slicer and the default value. . Creating Selector . slider = alt.binding_range(min=0, max=100, step=1, name=&#39;color_threshold:&#39;) selector = alt.selection_single(name=&quot;SelectorName&quot;, fields=[&#39;color_threshold&#39;], bind=slider, init={&#39;color_threshold&#39;: 20}) rule = alt.Chart(df).mark_rule(color=&#39;red&#39;).encode( x = selector.color_threshold ) . Now I just need to change the color threshold value I entered (20) to the slider variable color_threshold and bind the selector to the visuals. . bar = (alt.Chart(df).mark_bar().encode( #Create a bar chart using df dataframe x=&#39;X&#39;, #X variable y=&#39;Z&#39;, #Y variable is Z tooltip=[&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;], #Add tooltip color=alt.condition( alt.datum.Z&lt; selector.color_threshold, #Conditional formatting the selector variable alt.value(&#39;#477998&#39;), alt.value(&#39;hsla(232, 7%, 20%, 0.25)&#39;), #if Z &lt; 20, Blue otherwise gray ) ).properties(width=500, height=400, title=&quot;Bar Chart with Widget&quot;)) text = bar.mark_text( align=&#39;right&#39;, #add text label baseline=&#39;middle&#39;, #align data labels dx= 3, dy= -5 #align x &amp; y positions of labels ).encode( text=&#39;Z:Q&#39;, color=alt.condition( alt.datum.Z&lt; selector.color_threshold, #Conditional formatting alt.value(&#39;#477998&#39;), alt.value(&#39;hsla(232, 7%, 20%, 0)&#39;)) ) bartext = (bar + text).interactive() bartext scatter = (alt.Chart(df).mark_circle().encode( # x=&#39;X&#39;, y=&#39;Y&#39;, tooltip=[&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;], color=alt.condition( alt.datum.Z &lt; selector.color_threshold, #passed the color_threshold variable alt.value(&#39;red&#39;), alt.value(&#39;hsla(232, 7%, 20%, 0.25)&#39;) ) ).properties(width=300, height=400, title=&quot;Sactterplot with Widget&quot;)) scatter.interactive() final =(bartext | scatter).add_selection( selector ) final . Try changing the color_threshold value to see how the charts behave. As you change the slider, bars below the threshold are highlighted in blue, their labels appear and the corrseponding values in X &amp; Y are highlighted in red in the scatter plot. Notice how responsive the charts are. . Extracting JSON for Deneb . To extract the JSON from the visual, you can use to_json() method or you can just click on the three dots in the top right hand corner of the above visual and select View Source as show below. This will give you the JSON that can be used in Deneb. . . #hide_output { &quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;hconcat&quot;: [ { &quot;layer&quot;: [ { &quot;mark&quot;: &quot;bar&quot;, &quot;encoding&quot;: { &quot;color&quot;: { &quot;condition&quot;: { &quot;value&quot;: &quot;#477998&quot;, &quot;test&quot;: &quot;(datum.Z &lt; SelectorName.color_threshold)&quot; }, &quot;value&quot;: &quot;hsla(232, 7%, 20%, 0.25)&quot; }, &quot;tooltip&quot;: [ {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Y&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot;} ], &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot;} }, &quot;height&quot;: 400, &quot;selection&quot;: { &quot;selector017&quot;: { &quot;type&quot;: &quot;interval&quot;, &quot;bind&quot;: &quot;scales&quot;, &quot;encodings&quot;: [&quot;x&quot;, &quot;y&quot;] }, &quot;SelectorName&quot;: { &quot;type&quot;: &quot;single&quot;, &quot;fields&quot;: [&quot;color_threshold&quot;], &quot;bind&quot;: { &quot;input&quot;: &quot;range&quot;, &quot;max&quot;: 100, &quot;min&quot;: 0, &quot;name&quot;: &quot;color_threshold:&quot;, &quot;step&quot;: 1 }, &quot;init&quot;: {&quot;color_threshold&quot;: 20} } }, &quot;title&quot;: &quot;Bar Chart with Widget&quot;, &quot;width&quot;: 500 }, { &quot;mark&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;align&quot;: &quot;right&quot;, &quot;baseline&quot;: &quot;middle&quot;, &quot;dx&quot;: 3, &quot;dy&quot;: -5 }, &quot;encoding&quot;: { &quot;color&quot;: { &quot;condition&quot;: { &quot;value&quot;: &quot;#477998&quot;, &quot;test&quot;: &quot;(datum.Z &lt; SelectorName.color_threshold)&quot; }, &quot;value&quot;: &quot;hsla(232, 7%, 20%, 0)&quot; }, &quot;text&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot;}, &quot;tooltip&quot;: [ {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Y&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot;} ], &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot;} }, &quot;height&quot;: 400, &quot;title&quot;: &quot;Bar Chart with Widget&quot;, &quot;width&quot;: 500 } ] }, { &quot;mark&quot;: &quot;circle&quot;, &quot;encoding&quot;: { &quot;color&quot;: { &quot;condition&quot;: { &quot;value&quot;: &quot;red&quot;, &quot;test&quot;: &quot;(datum.Z &lt; SelectorName.color_threshold)&quot; }, &quot;value&quot;: &quot;hsla(232, 7%, 20%, 0.25)&quot; }, &quot;tooltip&quot;: [ {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Y&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot;} ], &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Y&quot;} }, &quot;height&quot;: 400, &quot;selection&quot;: { &quot;SelectorName&quot;: { &quot;type&quot;: &quot;single&quot;, &quot;fields&quot;: [&quot;color_threshold&quot;], &quot;bind&quot;: { &quot;input&quot;: &quot;range&quot;, &quot;max&quot;: 100, &quot;min&quot;: 0, &quot;name&quot;: &quot;color_threshold:&quot;, &quot;step&quot;: 1 }, &quot;init&quot;: {&quot;color_threshold&quot;: 20} } }, &quot;title&quot;: &quot;Sactterplot with Widget&quot;, &quot;width&quot;: 300 } ], &quot;data&quot;: {&quot;name&quot;: &quot;data-2a92a2aad2788a9669f8b00966e4e773&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: { &quot;data-2a92a2aad2788a9669f8b00966e4e773&quot;: [ {&quot;X&quot;: 0, &quot;Y&quot;: 1, &quot;Z&quot;: 60}, {&quot;X&quot;: 1, &quot;Y&quot;: 1, &quot;Z&quot;: 19}, {&quot;X&quot;: 2, &quot;Y&quot;: 1, &quot;Z&quot;: 77}, {&quot;X&quot;: 3, &quot;Y&quot;: 2, &quot;Z&quot;: 31}, {&quot;X&quot;: 4, &quot;Y&quot;: 3, &quot;Z&quot;: 51}, {&quot;X&quot;: 5, &quot;Y&quot;: 3, &quot;Z&quot;: 17}, {&quot;X&quot;: 6, &quot;Y&quot;: 4, &quot;Z&quot;: 69}, {&quot;X&quot;: 7, &quot;Y&quot;: 4, &quot;Z&quot;: 19}, {&quot;X&quot;: 8, &quot;Y&quot;: 5, &quot;Z&quot;: 71}, {&quot;X&quot;: 9, &quot;Y&quot;: 6, &quot;Z&quot;: 44}, {&quot;X&quot;: 10, &quot;Y&quot;: 6, &quot;Z&quot;: 10}, {&quot;X&quot;: 11, &quot;Y&quot;: 7, &quot;Z&quot;: 54}, {&quot;X&quot;: 12, &quot;Y&quot;: 7, &quot;Z&quot;: 20}, {&quot;X&quot;: 13, &quot;Y&quot;: 8, &quot;Z&quot;: 64}, {&quot;X&quot;: 14, &quot;Y&quot;: 9, &quot;Z&quot;: 21}, {&quot;X&quot;: 15, &quot;Y&quot;: 9, &quot;Z&quot;: 5}, {&quot;X&quot;: 16, &quot;Y&quot;: 9, &quot;Z&quot;: 71}, {&quot;X&quot;: 17, &quot;Y&quot;: 10, &quot;Z&quot;: 91}, {&quot;X&quot;: 18, &quot;Y&quot;: 11, &quot;Z&quot;: 52}, {&quot;X&quot;: 19, &quot;Y&quot;: 11, &quot;Z&quot;: 70}, {&quot;X&quot;: 20, &quot;Y&quot;: 12, &quot;Z&quot;: 26}, {&quot;X&quot;: 21, &quot;Y&quot;: 13, &quot;Z&quot;: 30}, {&quot;X&quot;: 22, &quot;Y&quot;: 13, &quot;Z&quot;: 28}, {&quot;X&quot;: 23, &quot;Y&quot;: 14, &quot;Z&quot;: 35}, {&quot;X&quot;: 24, &quot;Y&quot;: 15, &quot;Z&quot;: 22}, {&quot;X&quot;: 25, &quot;Y&quot;: 15, &quot;Z&quot;: 49}, {&quot;X&quot;: 26, &quot;Y&quot;: 16, &quot;Z&quot;: 58}, {&quot;X&quot;: 27, &quot;Y&quot;: 16, &quot;Z&quot;: 4}, {&quot;X&quot;: 28, &quot;Y&quot;: 17, &quot;Z&quot;: 46}, {&quot;X&quot;: 29, &quot;Y&quot;: 17, &quot;Z&quot;: 99}, {&quot;X&quot;: 30, &quot;Y&quot;: 18, &quot;Z&quot;: 33}, {&quot;X&quot;: 31, &quot;Y&quot;: 19, &quot;Z&quot;: 63}, {&quot;X&quot;: 32, &quot;Y&quot;: 19, &quot;Z&quot;: 66}, {&quot;X&quot;: 33, &quot;Y&quot;: 19, &quot;Z&quot;: 60}, {&quot;X&quot;: 34, &quot;Y&quot;: 20, &quot;Z&quot;: 0}, {&quot;X&quot;: 35, &quot;Y&quot;: 20, &quot;Z&quot;: 87}, {&quot;X&quot;: 36, &quot;Y&quot;: 21, &quot;Z&quot;: 37}, {&quot;X&quot;: 37, &quot;Y&quot;: 22, &quot;Z&quot;: 25}, {&quot;X&quot;: 38, &quot;Y&quot;: 22, &quot;Z&quot;: 66}, {&quot;X&quot;: 39, &quot;Y&quot;: 22, &quot;Z&quot;: 95}, {&quot;X&quot;: 40, &quot;Y&quot;: 23, &quot;Z&quot;: 20}, {&quot;X&quot;: 41, &quot;Y&quot;: 23, &quot;Z&quot;: 99}, {&quot;X&quot;: 42, &quot;Y&quot;: 24, &quot;Z&quot;: 88}, {&quot;X&quot;: 43, &quot;Y&quot;: 24, &quot;Z&quot;: 34}, {&quot;X&quot;: 44, &quot;Y&quot;: 25, &quot;Z&quot;: 33}, {&quot;X&quot;: 45, &quot;Y&quot;: 25, &quot;Z&quot;: 61}, {&quot;X&quot;: 46, &quot;Y&quot;: 26, &quot;Z&quot;: 21}, {&quot;X&quot;: 47, &quot;Y&quot;: 26, &quot;Z&quot;: 92}, {&quot;X&quot;: 48, &quot;Y&quot;: 27, &quot;Z&quot;: 27}, {&quot;X&quot;: 49, &quot;Y&quot;: 27, &quot;Z&quot;: 55} ] } } . To use it in Deneb, you will need to make three changes to the code above: . Change the names X, Y and Z to the names of the columns you will be using in Power BI. For example, if you want to use Sales column on Y axis, wherever you see Y in the code above, change it to Sales | Change &#39;data&#39;: {&#39;name&#39;: &#39;data-22c7bda41ed58dcd275c5ceb7d2df6d9&#39;} to &#39;data&#39;: {&#39;name&#39;: &#39;dataset&#39;} | Delete everything starting from &#39;$schema&#39; and below | . The final JSON you need is below. You can customize the width, height, title, column names, column type, colors, threshold based on your needs. . . Note: If you are using a text column on your X-axis, change the &amp;#8217;type&amp;#8217;: &amp;#8217;quantitative&amp;#8217; below to &amp;#8217;type&amp;#8217;: &amp;#8217;nominal&amp;#8217; . #hide_output { &quot;config&quot;: { &quot;view&quot;: { &quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300 } }, &quot;hconcat&quot;: [ { &quot;layer&quot;: [ { &quot;mark&quot;: &quot;bar&quot;, &quot;encoding&quot;: { &quot;color&quot;: { &quot;condition&quot;: { &quot;value&quot;: &quot;#477998&quot;, &quot;test&quot;: &quot;(datum.Z &lt; SelectorName.color_threshold)&quot; }, &quot;value&quot;: &quot;hsla(232, 7%, 20%, 0.25)&quot; }, &quot;tooltip&quot;: [ { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot; }, { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Y&quot; }, { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot; } ], &quot;x&quot;: { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot; }, &quot;y&quot;: { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot; } }, &quot;height&quot;: 400, &quot;selection&quot;: { &quot;selector017&quot;: { &quot;type&quot;: &quot;interval&quot;, &quot;bind&quot;: &quot;scales&quot;, &quot;encodings&quot;: [&quot;x&quot;, &quot;y&quot;] }, &quot;SelectorName&quot;: { &quot;type&quot;: &quot;single&quot;, &quot;fields&quot;: [ &quot;color_threshold&quot; ], &quot;bind&quot;: { &quot;input&quot;: &quot;range&quot;, &quot;max&quot;: 100, &quot;min&quot;: 0, &quot;name&quot;: &quot;color_threshold:&quot;, &quot;step&quot;: 1 }, &quot;init&quot;: { &quot;color_threshold&quot;: 20 } } }, &quot;title&quot;: &quot;Bar Chart with Widget&quot;, &quot;width&quot;: 500 }, { &quot;mark&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;align&quot;: &quot;right&quot;, &quot;baseline&quot;: &quot;middle&quot;, &quot;dx&quot;: 3, &quot;dy&quot;: -5 }, &quot;encoding&quot;: { &quot;color&quot;: { &quot;condition&quot;: { &quot;value&quot;: &quot;#477998&quot;, &quot;test&quot;: &quot;(datum.Z &lt; SelectorName.color_threshold)&quot; }, &quot;value&quot;: &quot;hsla(232, 7%, 20%, 0)&quot; }, &quot;text&quot;: { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot; }, &quot;tooltip&quot;: [ { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot; }, { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Y&quot; }, { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot; } ], &quot;x&quot;: { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot; }, &quot;y&quot;: { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot; } }, &quot;height&quot;: 400, &quot;title&quot;: &quot;Bar Chart with Widget&quot;, &quot;width&quot;: 500 } ] }, { &quot;mark&quot;: &quot;circle&quot;, &quot;encoding&quot;: { &quot;color&quot;: { &quot;condition&quot;: { &quot;value&quot;: &quot;red&quot;, &quot;test&quot;: &quot;(datum.Z &lt; SelectorName.color_threshold)&quot; }, &quot;value&quot;: &quot;hsla(232, 7%, 20%, 0.25)&quot; }, &quot;tooltip&quot;: [ { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot; }, { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Y&quot; }, { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Z&quot; } ], &quot;x&quot;: { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;X&quot; }, &quot;y&quot;: { &quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Y&quot; } }, &quot;height&quot;: 400, &quot;selection&quot;: { &quot;SelectorName&quot;: { &quot;type&quot;: &quot;single&quot;, &quot;fields&quot;: [&quot;color_threshold&quot;], &quot;bind&quot;: { &quot;input&quot;: &quot;range&quot;, &quot;max&quot;: 100, &quot;min&quot;: 0, &quot;name&quot;: &quot;color_threshold:&quot;, &quot;step&quot;: 1 }, &quot;init&quot;: { &quot;color_threshold&quot;: 20 } } }, &quot;title&quot;: &quot;Sactterplot with Widget&quot;, &quot;width&quot;: 300 } ], &quot;data&quot;: { &quot;name&quot;: &quot;dataset&quot; } } . Steps . Install Deneb from App Source. Add your columns to the visual. | . Make sure you remove the summarization | . Select Edit from the three dots in the top right hand corner | . Select Empty as we will be adding the code we extracted above | . Add the JSON code. You will see the message Cannot read property type of null in the right pane. | To fix the message, click on the spanner. | . This will render the visuals. Now just go back to the Power BI canvas and resize the window by dragging the corners. You can always change the width and height specified in the JSON to make it fit in your report as needed. | . You can download the .pbix file from here . As I mentioned above, these widgets can be double slicers, radio buttons, dropdowns etc. and can be used to change pretty much any property of the visual. This is extremely powerful. Also as you saw above, you can create composite visuals that are very effective at analyzing complex, multivariate data. One thing I did not show in this blog is adding cross-highlighting. It&#39;s fairly easy to do that. You can read about it on Deneb&#39;s documentation. I will show it to in a future blog post. .",
            "url": "https://pawarbi.github.io/blog/power%20bi/deneb/custom%20visual/altair/python/2022/01/22/Deneb-Custom-visual-interactive-widget-composite.html",
            "relUrl": "/power%20bi/deneb/custom%20visual/altair/python/2022/01/22/Deneb-Custom-visual-interactive-widget-composite.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Generating Random Numbers With Triangular Distribution in Power Query",
            "content": "Triangular Distribution . Although normal distributions are well known, in risk modeling triangular distributions are more useful. Normal distributions are symmetric and most risk profiles in real life are not. This is where triangular distributions can come handy. Also, if you are not sure about which distribution to use, triangular distribution can be used as a first guess. . For example, you are in a meeting and you have been asked to create a schedule risk model. You ask the PM to estimate the uncertainty in the task durations for a new project. It will be hard to estimate the duration using mean and standard deviation to define the normal distribution, plus it will be symmetric (same risk of not completing vs. completing completing on time). Instead you could ask the PM, what&#39;s your worst estimate, best estimate and most likely estimate. You can define a triangular distribution using these three values. It is more common to define schedules using PERT distribution, which I have covered in DAX here. . . Custom Function . #hide-output //Author: Sandeep Pawar //Date: Jan 17, 2022 //PawarBI.com (lower as number, middle as number , upper as number ) as number =&gt; let // Parameters x = middle-lower, y = upper-lower, z = upper-middle, a = 1, b1 = -2*lower, b2 = -2*upper, t = (x/y), u = Number.Random(), // Calculation c1 = Number.Power(lower,2)-(u*x*y), c2 = Number.Power(upper,2)-((1-u)*y*z), // Traingle Inverse t1 = (-1 * b1 + Number.Sqrt(Number.Power(b1,2) - (4*a*c1)))/ (2*a), t2 = (-1 * b2 - Number.Sqrt(Number.Power(b2,2) - (4*a*c2)))/(2*a), inv = if u &lt;= t then t1 else t2 in inv . Steps . Steps are exactly same as the steps shown here, except use the above function. Make sure you have unique numbers on each row, if you do not, you will need to add an index column. . Here is the resulting distribution: . df = pd.read_clipboard().set_index(&#39;Column1&#39;).drop(&#39;Index&#39;,axis=1) df.head(5) . Random Number . Column1 . 1 22.048133 | . 2 40.940600 | . 3 46.184121 | . 4 59.807112 | . 5 52.808201 | . sns.displot(df[&#39;Random Number&#39;], rug=True, kde=True); . Case Study: Risk Modeling in Power BI . I gave this presentation at Scottish Summit in 2021. If you are new to risk modeling, uncertainty estimation etc, please watch this video. In this video, I show the DAX version, but you could use either DAX or M to generate the random numbers. . .",
            "url": "https://pawarbi.github.io/blog/power%20bi/random%20number/m/power%20query/risk%20modeling/monte%20carlo/2022/01/17/PowerQuery-Triangular-Distribution.html",
            "relUrl": "/power%20bi/random%20number/m/power%20query/risk%20modeling/monte%20carlo/2022/01/17/PowerQuery-Triangular-Distribution.html",
            "date": " • Jan 17, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Generating Normally Distributed Random Numbers in Power Query",
            "content": "Power Query Doesn&#39;t Have NORMINV() . In Excel, if you want to generate a column with random numbers that are normally distributed, you can use the NORMINV() function like this. You can specify the probability (which is usually a random number drawn from uniform distribution), mean and standard deviation. While DAX has the NORM.INV() function, M does not. If you create simulations, what-if scenario analyses etc., more than likely you will need to generate a column with random numbers that follow the Gaussian distribution. I have written a blog post on how to generate various distributions using DAX, you can read it here. . In this blog, I will share a simple formula to generate the normally distributed random numbers using M. It uses the Box-Muller transform to generate the inverse distribution. I won&#39;t go into the theory and math, but if you are interested you can read it here. . . Custom Function . #hide-output // Gaussian Random Number Generator with mean =mean and standard number as sd using Box-Mueller Transform // Add an index column to the table before invoking this function. let gaussianrandom = (mean as number, sd as number) as number=&gt; ( sd * ( Number.Sqrt(- 2 * Number.Ln(Number.Random()) ) * Number.Cos( 2.0 * 3.14159265358979323846 * Number.Random() ) ) + mean ) in gaussianrandom . Steps . Create a Power Query function using the formula above. In the below example, I named the function _NormalDist | . . You will need to have unique rows. If you don&#39;t, create an index column (Add Column &gt; Index Column). | To create a new column that follows the Gaussian distribution using the above function, go to Add Column and use the above function. In the example below, I created a new column that has mean of 10 and standard deviation of 0.25 | . . Here is the result: | . Refresh the report and you will see the numbers in your table. If you see same number on all rows, just add another index column and remove it again. . NORM.INV() in DAX generates new numbers every time the report is refreshed. In Power Query you can disable the refresh for this table, and hence generated numbers will stay the same even after refreshing the report. If you open the PowerQuery, however, it will generate new numbers. You can use Table.Buffer() to freeze it but I haven&#39;t had luck with that. If you know how to do it, please let me know. . Here is the resulting distribution: . df = pd.read_clipboard().set_index(&#39;Column1&#39;) df.head(5) . Random Numbers . Column1 . 1 10.104795 | . 2 9.967639 | . 3 10.172852 | . 4 10.163127 | . 5 10.043073 | . sns.displot(df[&#39;Random Numbers&#39;], rug=True, kde=True); . print(&quot;The mean and standard deviation of random numbers : &quot;, round(df[&#39;Random Numbers&#39;].mean(),2), round(df[&#39;Random Numbers&#39;].std(),2)) . The mean and standard deviation of random numbers : 9.99 0.25 . I also wrote another function to generate Traingular Distribution,which is very common to simulate risk profiles. Hope to share that soon. .",
            "url": "https://pawarbi.github.io/blog/power%20bi/random%20number/m/power%20query/2021/12/22/PowerQuery-Normal-distribution.html",
            "relUrl": "/power%20bi/random%20number/m/power%20query/2021/12/22/PowerQuery-Normal-distribution.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Dynamically Changing the Color Transparency in Power BI",
            "content": "Changing the Color Transperancy . As someone who uses Python/R heavily for exploratory data analysis and Power BI for publishing the final data analytics reports, I have always missed the ability to adjust the color transperancy in Power BI. In Power BI you can change the color dynamically and conditionally but there is no native functionality to change the transperancy. . I was working on a project where I wanted to highlight certain clusters in the data to the business user. Sure, I could change the color but it&#39;s very challenging when the data points are concentrated in a small area and they overlap each other. In Python and R you can easily adjust the alpha value in most plots to see the dense area clearly. . Solution . The solution is to create a measure that passes a color as HSLA values. HSLA is Hue, Saturation, Lightness &amp; Alpha. It&#39;s the alpha value that can be adjusted to make any color transparent. In Power BI you can pass color values as HEX, RGB, CSS color names and as HSLA. All you have to do find the HSLA color by using an online color converter such as this one :https://htmlcolors.com/hsla-color and voila you can now adjust the transparency dynamically by coupling it with What-If parameter. Below are the steps. . Find the HSLA color using the online conveter | Create a What If parameter. This is optional. If you want to hard code the alpha value, you can just include that in the measure. When developing the viz, you may first want to use the what-if to find which alpha value works the best for your case and then just hard code it in the measure. | . . Create measures with HSLA value . . | . Additional measure if you want to highlight a category . . Now in conditional formatting option (the &#39;fx&#39; button), pick &#39;Field Value and select the measure. . . | . Use Cases . Consider the first scatterplot below. In this case the data points are so dense that it&#39;s hard to figure out exactly where the points are concentrated. . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import altair as alt . df = pd.read_csv(&quot;https://raw.githubusercontent.com/selva86/datasets/master/diamonds.csv&quot;).sample(5000) df.head() . carat cut color clarity depth table price x y z . 4183 0.91 | Ideal | G | SI2 | 62.7 | 57.0 | 3557 | 6.15 | 6.19 | 3.87 | . 8924 1.01 | Good | H | SI1 | 62.9 | 57.0 | 4496 | 6.27 | 6.36 | 3.97 | . 48564 0.64 | Ideal | I | IF | 62.0 | 54.0 | 1991 | 5.53 | 5.56 | 3.44 | . 39008 0.40 | Ideal | E | VVS2 | 61.5 | 55.0 | 1056 | 4.79 | 4.77 | 2.94 | . 4074 0.31 | Ideal | D | SI1 | 62.4 | 56.0 | 571 | 4.33 | 4.35 | 2.71 | . The original dataset has 53000 rows. For demonstration I will sample 5000 rows randomly. . df.shape . (5000, 10) . alt.Chart(df).mark_circle(size=60).encode( x=&#39;carat&#39;, y=&#39;price&#39;, ).interactive() . As you can see in the plot above, the data points overlap each other and it&#39;s hard to see the distribution of the data. In most python and R libraries you can specify the alpha or opacity values to better understand the data density. . alt.Chart(df).mark_circle(size=60).encode( x=&#39;carat&#39;, y=&#39;price&#39;, ).configure_mark( opacity=0.01, ).interactive() . Now it&#39;s much easier to see that the most data points are concetrated in the lower left region. With the method shown above, you can implement the same technique in Power BI. . from IPython.display import IFrame pbi = &#39;https://app.powerbi.com/view?r=eyJrIjoiOTY2YTBhODctNmFhNS00OTFhLThiZmYtZmY4OTI3OTZiMzQ0IiwidCI6IjkxMzc2MWU4LTc4NjEtNDc0ZS05ZjM4LWQyZDc1MjUwMDExZiJ9&#39; IFrame(pbi, width=800, height=600) . The same technique can be used to highlight certain observations based on a logical condition set in a measure. Because we can set the transparency, we can make those observation pop in the plot and send other observations to the background. | . This can also be used to make certain data points invisible. For example, if you wanted to hide certain data just set the alpha value to 0 in the measure and now those data points (or even text) can be made invisible ! | .",
            "url": "https://pawarbi.github.io/blog/powerbi/dataviz/eda/2021/09/03/color-transperancy-powerbi.html",
            "relUrl": "/powerbi/dataviz/eda/2021/09/03/color-transperancy-powerbi.html",
            "date": " • Sep 3, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Segmentation Using Multivariate Clustering in Power BI",
            "content": "Clustering In Power BI . Power BI has native clustering capabilities that are rarely utilized, partially due to poor documentation on this topic. In this video, I show how you can perform clustering in Power BI and also how to use the results to gain insights from these clusters. I also discuss some of the limitations of this approach. . .",
            "url": "https://pawarbi.github.io/blog/powerbi/clustering/machinelearning/2021/08/25/segmentation-clustering-powerbi-multivariate.html",
            "relUrl": "/powerbi/clustering/machinelearning/2021/08/25/segmentation-clustering-powerbi-multivariate.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Power BI AutoML Deep Dive",
            "content": "Power BI AutoML Deep Dive . In this session I wanted to show, not only how to create the machine learning models, but more importantly how to create models that answer business questions and are explainable and robust. Please watch and let me know your thoughts. Topic covered: . Machine Learning Basics | License Requirements | How to create models that are: Business focused (cost/loss functions/profit curves) | Generalizable (underfitting/overfitting/error terrain analysis) | Interpretable (SHAP, feature importance) | Manageable (MLOps process and workflow) | . | Recommendations, Limitations | . .",
            "url": "https://pawarbi.github.io/blog/powerbi/automl/machinelearning/2021/08/25/powerbi-automl.html",
            "relUrl": "/powerbi/automl/machinelearning/2021/08/25/powerbi-automl.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "",
            "content": "Power BI AutoML Deep Dive . In this session I wanted to show, not only how to create the machine learning models, but more importantly how to create models that answer business questions and are explainable and robust. Please watch and let me know your thoughts. Topic covered: . Machine Learning Basics | License Requirements | How to create models that are: Business focused (cost/loss functions/profit curves) | Generalizable (underfitting/overfitting/error terrain analysis) | Interpretable (SHAP, feature importance) | Manageable (MLOps process and workflow) | . | Recommendations, Limitations | . youtube https://youtu.be/_CaFacQiW_E .",
            "url": "https://pawarbi.github.io/blog/2022/10/14/2021-08-25-powerbi-automl-deepdive.html",
            "relUrl": "/2022/10/14/2021-08-25-powerbi-automl-deepdive.html",
            "date": " • Oct 14, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Selecting Columns by Data Type in Power Query",
            "content": "Motivation . It&#39;s not uncommon to have a table with hundreds of columns with different column types such as numerical, text, date, percentage etc. You can select columns by using the UI option &quot;Choose Columns&quot; and then selecting which columns to keep. But if you have have tens of columns of the same type, you will have to manually go through the list of columns and select the columns to keep. As far as I know, there is no direct way to only keep, for example, numerical columns. In Pandas, you can use select_dtypes method and pass which columns to choose based on the data types. No such option or function is available in Power Query. . In this example, I will show how I achieved it. You can customize it or turn it into a function based on your use case. . Update 1: Using Value.Type &amp; Type.Is . I am using two M functions to achieve the desired output - Value.Type and Type.Is. . Value.Type returns the data type of the value. For example, Value.Type(123) returns number as type, while Value.Type(&quot;sandeep&quot;) returns text as type. There is not much about it in the official documentation here . Similarly, Type.Is is a logical function that you can use to get a True/False if the type equals defined column type. For example, to check if number 123 is a text you would write Type.Is(Value.Type(123), Text.Type). This will return FALSE while Type.Is(Value.Type(123), Number.Type) returns TRUE. Here we first obtained the type of the value using Value.Type and then compared it with Number.Type. Since both match, the result is TRUE. You can see a list of all data types in M here. . We can combine these two functions with parameters to dynamically select columns based on selected data type. . First create a parameter list as shown below. Include all data types you want to select. In this case, I am only selecting text, numeric and date type. I have also included &#39;All&#39; to return the entire table. . . You can see the code below, but at a high level here are the steps: . Create a parameter list | Ensure all columns have correct data types assigned. This is important because that&#39;s how M will idenify the columns to select | Unpivot the table to create two columns - &#39;ColumnName&#39; &amp; &#39;Value&#39; | Use Value.Type and Type.Is and create a new column that shows the data type of the value | Link parameter list with the filter to select data type | Get filtered column list and pass it to Table.SelectColumns | . You can see it in action below: . . To apply this to your table, you only need to make sure SourceTable2 is your table with all columns with correctly assigned data types. You could convert it to a function to take table and data type as arguments. I will leave it to you. . . Update 2: Using Table.Schema . While thinking more about ways to make this function better, I actually found two more ways. One is to use Table.ColumnsofType (documentation) and other is to use Table.Schema. Table.Schema gives you metadata of the entire table along with the column types. I can use the same approach as above, but just use Table.Schema instead to get data type of each column. This is faster, better, cleaner!!! Here is the code: . .",
            "url": "https://pawarbi.github.io/blog/powerquery/m/powerbi/2021/05/04/selecting-columns-columntype-powerbi.html",
            "relUrl": "/powerquery/m/powerbi/2021/05/04/selecting-columns-columntype-powerbi.html",
            "date": " • May 4, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Operationalizing Machine Learning models with Azure ML and Power BI",
            "content": "Machine Learning in Power BI . Power BI and Azure ML have native integration with each other, which means not only that you can consume the deployed models in Power BI but also use the resources/tools in Azure ML to manage the model lifecycle. There is a misconception that you need Power BI Premium to use Azure ML. In this session I show that&#39;s not the case. Below are the topics covered: . Limitation of using Python or R in Power BI . Formula firewall | Privacy setting of the datasource must be Public | 30 min query timeout | Have to use personal gateway | Cannot use enhanced metadata | Dependency management | Not scaleable | No MLOps | . | Steps to access Azure ML models in Power BI . Give Azure ML access to Power BI tenant in IAM | Use scoring script with input/output schema | . | . . Topics Covered: . Invoking Azure ML models in Power Query in Desktop and using dataflow in service . | Thoughts on batch-scoring . | Other considerations: . Azure ML models can only be called for real-time inference. | Use ACI for small models and AKS for large models that requore GPU, scaleable compute, low latency, high availability | Real-time inferencing is expensive! Use batch scoring if real-time is not needed | Incremental refreshes in Power BI will work with models invoked from Azure ML but watch out for (query) performance | With Azure Arc, you can use your own Kubernets service! Deploy the model in Azure ML but use your own K8 (AWS, GCP etc.) | Premium Per User gives you access to Auto ML. If you have access to PPU, it may be worth first experimenting with AutoML before using Azure ML model to save cost. PPU Automl doesn&#39;t offer the same level of MLOps capabilities though. I will be presenting a technical deep-dive on using Power BI AutoML on May 26 at Portland Power BI User Group. Sign up on their page if you are interested in the topic. | . | . . References: . https://docs.microsoft.com/en-us/power-bi/connect-data/service-aml-integrate . | https://pawarbi.github.io/blog/powerbi/r/python/2020/05/15/powerbi-python-r-tips.html . | https://azure.microsoft.com/en-us/blog/innovate-across-hybrid-and-multicloud-with-new-azure-arc-capabilities/ . | https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-python-scripts . | https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-power-bi-custom-model . | . | .",
            "url": "https://pawarbi.github.io/blog/azureml/mlops/powerbi/2021/04/14/operationalize-mlops-powerbi-azureml-machinelearning.html",
            "relUrl": "/azureml/mlops/powerbi/2021/04/14/operationalize-mlops-powerbi-azureml-machinelearning.html",
            "date": " • Apr 14, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "",
            "content": "Machine Learning in Power BI . Power BI and Azure ML have native integration with each other, which means not only you can consume the model deploy in Power BI but also use the resources/tools in Azure ML to manage the MLOps process. There is a misconception that you need Power BI Premium to use Azure ML. In this session I show that&#39;s not the case. Below are the topics covered: . Limitation of using Python or R in Power BI . Formula firewall | Privacy setting of the datasource must be Public | 30 min query timeout | Have to use personal gateway | Cannot use enhanced metadata | Dependency management | Not scaleable | No MLOps | . | Steps to access Azure ML models in Power BI . Give Azure ML access to Power BI tenant in IAM | Use scoring script with input/output schema | . | . . Topics Covered: . Invoking Azure ML models in Power Query in Desktop and using dataflow in service . | Thoughts on batch-scoring . | Other considerations: . Azure ML models can only be called for real-time inference. | Use ACI for small models and AKS for large models that requore GPU, scaleable compute, low latency, high availability | Real-time inferencing is expensive! Use batch scoring if real-time is not needed | Incremental refreshes in Power BI will work with models invoked from Azure ML but watch out for (query) performance | With Azure Arc, you can use your own Kubernets service! Deploy the model in Azure ML but use your own K8 (AWS, GCP etc.) | Premium Per User gives you access to Auto ML. If you have access to PPU, it may be worth first experimenting with AutoML before using Azure ML model to save cost. PPU Automl doesn&#39;t offer the same level of MLOps capabilities though. | . | . . References: . https://docs.microsoft.com/en-us/power-bi/connect-data/service-aml-integrate . | https://pawarbi.github.io/blog/powerbi/r/python/2020/05/15/powerbi-python-r-tips.html . | https://azure.microsoft.com/en-us/blog/innovate-across-hybrid-and-multicloud-with-new-azure-arc-capabilities/ . | https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-python-scripts . | https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-power-bi-custom-model . | . | .",
            "url": "https://pawarbi.github.io/blog/2022/10/14/2021-04-05-operationalizing-ml-models-powerbi-azureml-mlops.html",
            "relUrl": "/2022/10/14/2021-04-05-operationalizing-ml-models-powerbi-azureml-mlops.html",
            "date": " • Oct 14, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "The Magic of SHD",
            "content": "What&#39;s SHD? . SHD stands for (S)ingle Exponential Smoothing, (H)olt&#39;s, (D)amped forecasting algorithm. It&#39;s not often that you can describe the entire algorithm in one single sentence but I just did that. And this simple algorithm often outperforms some of the most complex forecasting algorithms including DNNs and FB Prophet on univariate low frequency time series. I have used it on many projects successfully with great results. I am sharing it because the great Spyros Makridakis reminded on twitter that SHD was found superior in all M (M5 would be an exception) competitions. . The simple arithmetic average of Single, Hold and Damped exponential smoothing often used as a benchmark to compare the forecasting accuracy of methods. @stephensenn https://t.co/WduPWioPvI . &mdash; Spyros Makridakis (@spyrosmakrid) March 8, 2021 . Not many know about this gem so I thought I would share my code. It&#39;s a reminder that you don&#39;t always need complex algorithms to create forecast predictions. Use what&#39;s simple and parsimonious. . How does it work? . Just take arithmatic mean of forecast from SES, Holt&#39;s and Damped . . How does it stack against other algorithms? . Read it yourself. It worked as good and even better than most other algorithms in the M3 competition. It works particularly well with low frequency time series (Yearly, monthly). It works well because we are ensembling three different algorithms. It&#39;s been shown that forecast combinations often outperform single best models. . I will demonstrate it using an example below. This is the same dataset I used in my two previous blogs. . import pandas as pd import numpy as np import statsmodels.api as sm from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt import statsmodels from statsmodels.tsa.statespace.exponential_smoothing import ExponentialSmoothing import scipy from scipy.stats import boxcox from scipy.special import inv_boxcox from statsmodels.tools.eval_measures import rmse . print(&#39;Pandas:&#39;, pd.__version__) print(&#39;Statsmodels:&#39;, sm.__version__) print(&#39;Scipy:&#39;, scipy.__version__) print(&#39;Numpy:&#39;, np.__version__) . Pandas: 1.1.5 Statsmodels: 0.12.2 Scipy: 1.5.2 Numpy: 1.19.1 . SHD . def combshd(train,horizon,seasonality, init): # Author: Sandeep Pawar # Date: 8/30/2020 # version: 1.1 &#39;&#39;&#39; params - :train numpy array or Pandas series with univariate data :horizon forecast horizon (int) :seasonality For monthly 12, yearly 1, quarerly 4 (int) :init initialization (&#39;heuristic&#39;,&#39;concentrated&#39;) output numpy array if length equal to specified horizon &#39;&#39;&#39; train_x,lam = boxcox (train) ses=(sm.tsa.statespace.ExponentialSmoothing(train_x, trend=True, seasonal=None, initialization_method= init, damped_trend=False).fit()) fc1 = inv_boxcox(ses.forecast(horizon),lam) holt=(sm.tsa.statespace.ExponentialSmoothing(train_x, trend=True, seasonal=seasonality, initialization_method= init, damped_trend=False).fit()) fc2 = inv_boxcox(holt.forecast(horizon),lam) damp=(sm.tsa.statespace.ExponentialSmoothing(train_x, trend=True, seasonal=seasonality, initialization_method= init, damped_trend=True).fit()) fc3 = inv_boxcox(damp.forecast(horizon),lam) fc = (fc1+fc2+fc3)/3 return fc . data = pd.read_csv(&quot;https://raw.githubusercontent.com/pawarbi/datasets/master/timeseries/ts_frenchretail.csv&quot;) data[&#39;Date&#39;]= pd.to_datetime(data[&#39;Date&#39;]) data.set_index(&#39;Date&#39;, inplace= True) train = data.iloc[:-4] test = data.iloc[-4:] data.head() . Sales . Date . 2012-03-31 362000 | . 2012-06-30 385000 | . 2012-09-30 432000 | . 2012-12-31 341000 | . 2013-03-31 382000 | . data.plot(); . print(&quot;Train legth:&quot;, len(train), &quot; nTest legth:&quot;,len(test)) assert len(data)==len(train) + len(test) . Train legth: 20 Test legth: 4 . Create forecast . shd_pred = combshd(train = train[&#39;Sales&#39;].values,horizon=len(test),seasonality = 4, init = &#39;heuristic&#39;) . rmse(test[&#39;Sales&#39;].values,shd_pred ).round(0) . 63734.0 . RMSE using SHD is 63734. For comparison, FB Prophet gave ~66,000 and SARIMA) was ~82,000. I was able to further improve this by ensembling many algorithms but still it&#39;s impressive that with just few lines of code you can create a sophisticated algorithm ! . A reminder, always start with baseline simple algorithms. In practice, fast and frugal wins the long race. .",
            "url": "https://pawarbi.github.io/blog/pandas/numpy/data-cleaning/2021/03/08/shd-forecasting-ses-exponential-damped.html",
            "relUrl": "/pandas/numpy/data-cleaning/2021/03/08/shd-forecasting-ses-exponential-damped.html",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Removing Non-Alphanumeric Characters From A Column",
            "content": "This is a short blogpost. I wanted to document this recipe for my own benefit, and hopefully it will help others. I was working with a very messy dataset with some columns containing non-alphanumeric characters such as #,!,$^*) and even emojis. . numpy has two methods isalnum and isalpha. . isalnum returns True if all characters are alphanumeric, i.e. letters and numbers. documentation . isalpha returns True if all characters are alphabets (only alphabets, no numbers).documentation . import numpy as np import pandas as pd . df = pd.DataFrame({&#39;col&#39;:[&#39;abc&#39;, &#39;a b c&#39;, &#39;a_b_c&#39;, &#39;#$#$abc&#39;, &#39;abc111&#39;, &#39;abc111#@$@&#39;, &#39; abc !!! 123&#39;, &#39;ABC&#39;]}) . df . col . 0 abc | . 1 a b c | . 2 a_b_c | . 3 #$#$abc | . 4 abc111 | . 5 abc111#@$@ | . 6 abc !!! 123 | . 7 ABC | . Remove symbols and return alphanumerics . def alphanum(element): return &quot;&quot;.join(filter(str.isalnum, element)) . df.loc[:,&#39;alphanum&#39;] = [alphanum(x) for x in df.col] . df . col alphanum . 0 abc | abc | . 1 a b c | abc | . 2 a_b_c | abc | . 3 #$#$abc | abc | . 4 abc111 | abc111 | . 5 abc111#@$@ | abc111 | . 6 abc !!! 123 | abc123 | . 7 ABC | ABC | . Remove symbols &amp; numbers and return alphabets only . def alphabets(element): return &quot;&quot;.join(filter(str.isalpha, element)) . df.loc[:,&#39;alphabets&#39;] = [alphabets(x) for x in df.col] df . col alphanum alphabets . 0 abc | abc | abc | . 1 a b c | abc | abc | . 2 a_b_c | abc | abc | . 3 #$#$abc | abc | abc | . 4 abc111 | abc111 | abc | . 5 abc111#@$@ | abc111 | abc | . 6 abc !!! 123 | abc123 | abc | . 7 ABC | ABC | ABC | . Bonus: Remove symbols &amp; characters and return numbers only . def numbers(element): return &quot;&quot;.join(filter(str.isnumeric, element)) . df.loc[:,&#39;num&#39;] = [numbers(x) for x in df.col] df . col alphanum alphabets num . 0 abc | abc | abc | | . 1 a b c | abc | abc | | . 2 a_b_c | abc | abc | | . 3 #$#$abc | abc | abc | | . 4 abc111 | abc111 | abc | 111 | . 5 abc111#@$@ | abc111 | abc | 111 | . 6 abc !!! 123 | abc123 | abc | 123 | . 7 ABC | ABC | ABC | | . df.dtypes . col object alphanum object alphabets object num object dtype: object . Note that the num column is returned as an object (i.e. string) and not a number so be sure to convert it to int .",
            "url": "https://pawarbi.github.io/blog/pandas/numpy/data-cleaning/2021/03/05/removing-non-alphanumeric-symbols-characters-from-column-numpy-pandas-dataframe.html",
            "relUrl": "/pandas/numpy/data-cleaning/2021/03/05/removing-non-alphanumeric-symbols-characters-from-column-numpy-pandas-dataframe.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Introduction to Azure ML SDK",
            "content": "Azure Saturday Hamburg, Feb 20, 2021 . Sandeep Pawar . Twitter : @PawarBI | LinkedIn: in/sanpawar | Blog : PawarBI.com . . Note: This notebook was presented at Azure Saturday Hamburg . Agenda . - Machine Learning Process - As Advertised . Motivation | Demo | . - Glimpse of Real Machine Learning Process - Using Azure ML . What is Azure ML Service | Classes in Azure ML SDK | Workspace | DataOps using Datastore &amp; Datasets | Experiments | Model Deployment | . Motivation . Before I talk about Azure ML, I would like to first provide some motivation for why we want to learn and use Azure ML. . The goal of this presentation is not to show how to create machine learning models but rather, how to use Azure ML to operationalize the machine learning models at scale. I will create an example machine learning model but really the focus is understanding the common &#39;design patterns&#39; in Azure ML. If you are familiar with theory of machine lerning, this presentation/example notebook will help you understand the often neglected MLOps part of ML. If you do not have experience with creating ML models or are new to Python/Azure, focus on the logical process rather than the exact mechanics. You can always revisit this example notebook or Microsoft Learn but hopefully from this session you will understand, at a high-level, how to use Azure ML to deploy ML models in production. . Machine Learning Process - As Advertised . Let&#39;s start with a typical machine learning process. You will see plenty of tutorials on how to create machine learning models. Just type in &quot;Machine learning process&quot; in Google and you will see below results. Most of these describe the process broadly as follows: . Obtain data | Clean data | EDA | Preprocess the data | Build model(s) | Validate the model | Serialize the model | . . Let&#39;s follow this process to build a model. . Data . I will use a dataset from UCI Machine Learning reporsitory called &quot;Bank Marketing Data Set&quot;. You may have seen this in many tutorials. I chose this dataset because the focus of this presentation is learning Azure ML so I wanted to pick something that most can understand and I recently gave a presentation on Machine Learning Model Interpretability using the same dataset. In case you are interested in that topic, you will already be familiar with this dataset after this presentation. . This dataset has 20 features, mix of numerical and categorical features, and a target label with &quot;Yes/No&quot; values. It&#39;s a binary classification problem and the goal is to predict if a customer will sign up for a bank term deposit. Feel free to explore the dataset on your own before proceeding. . #collapse-hide import pandas as pd import numpy as np from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt %matplotlib inline from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import roc_auc_score, accuracy_score from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn import metrics from interpret import show from interpret.perf import ROC from sklearn import metrics import seaborn as sns . . print(pd.__version__) . 1.2.1 . Obtain data . There are 32950 observations and 20 features. Each observation describes a potential customer with their details such as job, age, martial status etc. and also the macro economic conditions (employment rate, bond rate etc. when that customer was last contacted. . #https://archive.ics.uci.edu/ml/datasets/Bank+Marketing path = &quot;https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv&quot; data = pd.read_csv(path) print(data.shape) data.head() . (32950, 21) . age job marital education default housing loan contact month day_of_week ... campaign pdays previous poutcome emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed y . 0 57 | technician | married | high.school | no | no | yes | cellular | may | mon | ... | 1 | 999 | 1 | failure | -1.8 | 92.893 | -46.2 | 1.299 | 5099.1 | no | . 1 55 | unknown | married | unknown | unknown | yes | no | telephone | may | thu | ... | 2 | 999 | 0 | nonexistent | 1.1 | 93.994 | -36.4 | 4.860 | 5191.0 | no | . 2 33 | blue-collar | married | basic.9y | no | no | no | cellular | may | fri | ... | 1 | 999 | 1 | failure | -1.8 | 92.893 | -46.2 | 1.313 | 5099.1 | no | . 3 36 | admin. | married | high.school | no | no | no | telephone | jun | fri | ... | 4 | 999 | 0 | nonexistent | 1.4 | 94.465 | -41.8 | 4.967 | 5228.1 | no | . 4 27 | housemaid | married | high.school | no | yes | no | cellular | jul | fri | ... | 2 | 999 | 0 | nonexistent | 1.4 | 93.918 | -42.7 | 4.963 | 5228.1 | no | . 5 rows × 21 columns . Clean the data . The column names contain some columns with periods (&#39;.&#39;) in them. We will clean the column names, change dtype of some columns to categoricals and also binarize the target to [1,0] instead of yes/no. . #Define functions to clean the data def clean_col_names(df): df.columns = [col.replace(&#39;.&#39;,&#39;_&#39;) for col in df.columns] return df def clean_dtype(df): cat_cols = [&#39;job&#39;,&#39;marital&#39;,&#39;education&#39;,&#39;default&#39;,&#39;housing&#39;,&#39;loan&#39;,&#39;contact&#39;,&#39;month&#39;,&#39;day_of_week&#39;,&#39;poutcome&#39;] for col in cat_cols: df.loc[:,col] = df[col].astype(&#39;category&#39;) return df def binarize_y(y): y = (y==&#39;yes&#39;).astype(int) return y . #Copy and Clean the data df = data.copy() df = clean_col_names(df) df = clean_dtype(df) df.head(3) . age job marital education default housing loan contact month day_of_week ... campaign pdays previous poutcome emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed y . 0 57 | technician | married | high.school | no | no | yes | cellular | may | mon | ... | 1 | 999 | 1 | failure | -1.8 | 92.893 | -46.2 | 1.299 | 5099.1 | no | . 1 55 | unknown | married | unknown | unknown | yes | no | telephone | may | thu | ... | 2 | 999 | 0 | nonexistent | 1.1 | 93.994 | -36.4 | 4.860 | 5191.0 | no | . 2 33 | blue-collar | married | basic.9y | no | no | no | cellular | may | fri | ... | 1 | 999 | 1 | failure | -1.8 | 92.893 | -46.2 | 1.313 | 5099.1 | no | . 3 rows × 21 columns . Split the data . Before conducting the exploratory data analysis (EDA), we will split the data into train and test. EDA should always be performed on the training data only to prevent information leakage, i.e overfitting. Test set should be used for final model evaluation. . Quick note - I have dropped the duration column because based on my analysis explained here, this feature leaks information so I am dropping it. Watch the presentation if you would like to understand how creating interpretable models can help avoid such data leakage. . X = df.drop([&#39;y&#39;,&#39;duration&#39;], axis=1) y = df.y y = binarize_y(y) . cat_cols = [&#39;job&#39;,&#39;marital&#39;,&#39;education&#39;,&#39;default&#39;,&#39;housing&#39;,&#39;loan&#39;,&#39;contact&#39;,&#39;month&#39;,&#39;day_of_week&#39;,&#39;poutcome&#39;] num_cols = list(set(X.columns)-set(cat_cols)) x1,x2, y1,y2 = train_test_split(X,y, stratify=y, train_size=0.80, shuffle=True, random_state = 0) print(&quot;Training set:&quot;,len(x1),&quot; nTest set:&quot;,len(x2)) . Training set: 26360 Test set: 6590 . Training set has 26,360 observations and test has 6590 observations. The 80/20 split is arbitrary at this point. You can create learning curves to figure out how much data you need for training. It will also depend on the algorithm you are using. . x1.head(3) . age job marital education default housing loan contact month day_of_week campaign pdays previous poutcome emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed . 23612 55 | technician | divorced | professional.course | unknown | yes | no | cellular | aug | tue | 1 | 999 | 0 | nonexistent | 1.4 | 93.444 | -36.1 | 4.965 | 5228.1 | . 32560 40 | admin. | single | university.degree | no | no | no | telephone | jun | mon | 2 | 999 | 0 | nonexistent | 1.4 | 94.465 | -41.8 | 4.958 | 5228.1 | . 15168 51 | technician | divorced | unknown | unknown | no | no | telephone | jul | fri | 1 | 999 | 0 | nonexistent | 1.4 | 93.918 | -42.7 | 4.962 | 5228.1 | . Exploratory Data Analysis . The data is clean for our demonstration purposes. Before building the model, you should invest significant time in understanding the data first. This is definitely the most important part of building a reliable machine learning model. In this demo, I am going to skip this step and leave it up to you. . #For demonstration, using only few numerical columns and 1000 random observations sns.pairplot(x1[[&#39;emp_var_rate&#39;,&#39;cons_price_idx&#39;,&#39;cons_conf_idx&#39;,&#39;euribor3m&#39;]].sample(1000),diag_kind=&#39;kde&#39;); . print((y1.value_counts(normalize=True))) (y1.value_counts(normalize=True)).plot(kind=&#39;bar&#39;); . 0 0.887936 1 0.112064 Name: y, dtype: float64 . In the bar chart above, 1 is &#39;yes&#39; and 0 is &#39;no&#39;. As you can see, ~89% customers did not sign up for the term deposit and 11% did. Thus, the target labels are not balanced (i.e not ~50/50%). This will affect the model performance metric we choose. For imbalanced dataset, using accuracy as the metric can lead to incorrect results. ROC-AUC is often used in such situations. This is a big topic so for now we just need to know that based on the EDA we see that the target is imbalanced and we will have to keep it in mind when building the model. . Preprocess the data . In the previous steps we split the data and now we are ready to build the ML pipeline. We build the preprocessing pipeline for catgorical and numerical columns using Pipeline() from sklearn. . Categorical columns will be encoded using OneHotEncoder and numerical features will be scaled using StandardScaler. Standard scaler will bring all numerical features to mean = 0 and std dev = 1. There are various ways of encoding and scaling but for demo purposes we will stick with this. . Also note that not all ML algorithms need encoding and scaling. Linear methods such as Logistic Regression do while tree-based algorithms (Random Forest, GBMs) don&#39;t. We will still preprocess the data so we can use the same pipeline for different algirithms, if needed. . #Get column index for each of the columns types cat_nums = [list(x1.columns).index(col) for col in cat_cols] num_nums = [list(x1.columns).index(col) for col in num_cols] . print(cat_nums) print(num_nums) . [1, 2, 3, 4, 5, 6, 7, 8, 9, 13] [14, 0, 15, 12, 11, 10, 17, 16, 18] . #One hot encode cat_ohe_step = (&#39;ohe&#39;, OneHotEncoder(sparse=False, handle_unknown=&#39;ignore&#39;)) #Build Pipeline cat_pipe = Pipeline([cat_ohe_step]) num_pipe = Pipeline([(&#39;std&#39;, StandardScaler())]) transformers = [ (&#39;cat&#39;, cat_pipe, cat_nums), (&#39;num&#39;, num_pipe, num_nums) ] ct = ColumnTransformer(transformers=transformers) ct . ColumnTransformer(transformers=[(&#39;cat&#39;, Pipeline(steps=[(&#39;ohe&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False))]), [1, 2, 3, 4, 5, 6, 7, 8, 9, 13]), (&#39;num&#39;, Pipeline(steps=[(&#39;std&#39;, StandardScaler())]), [14, 0, 15, 12, 11, 10, 17, 16, 18])]) . Visualize the preprocessing steps: . #Visualize the Preprocessing steps from sklearn import set_config set_config(display=&#39;diagram&#39;) ct . ColumnTransformerColumnTransformer(transformers=[(&#39;cat&#39;, Pipeline(steps=[(&#39;ohe&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False))]), [1, 2, 3, 4, 5, 6, 7, 8, 9, 13]), (&#39;num&#39;, Pipeline(steps=[(&#39;std&#39;, StandardScaler())]), [14, 0, 15, 12, 11, 10, 17, 16, 18])]) . cat[1, 2, 3, 4, 5, 6, 7, 8, 9, 13] . OneHotEncoderOneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False) . num[14, 0, 15, 12, 11, 10, 17, 16, 18] . StandardScalerStandardScaler() . Build the model . I am going to use Random forest algorithm. Random Forest often gives a good baseline performance right out of the box in most scenarios without overfitting. Also, Random Forest has a nice feature - &#39;Out of Bag&#39; (OOB) score. It will help us estimate model performance over multiple bootstrapped samples thus providing a good proxy for cross-validation performance. I am using OOB here, just to save model building time. In a real project, you will carefully construct a CV scheme. . We use the above pipeline of transformations with the Random Forest estimator with default parameters. Set the oob_score=True to get the OOB score. Also, class_weight is set to balanced to mitigate class imbalance. . pipe = Pipeline([ (&#39;ct&#39;, ct), (&#39;rf&#39;, RandomForestClassifier(oob_score=True, random_state=0, class_weight = &#39;balanced&#39;)), ]) #Fit the model pipe.fit(x1,y1) . PipelinePipeline(steps=[(&#39;ct&#39;, ColumnTransformer(transformers=[(&#39;cat&#39;, Pipeline(steps=[(&#39;ohe&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False))]), [1, 2, 3, 4, 5, 6, 7, 8, 9, 13]), (&#39;num&#39;, Pipeline(steps=[(&#39;std&#39;, StandardScaler())]), [14, 0, 15, 12, 11, 10, 17, 16, 18])])), (&#39;rf&#39;, RandomForestClassifier(class_weight=&#39;balanced&#39;, oob_score=True, random_state=0))]) . ct: ColumnTransformerColumnTransformer(transformers=[(&#39;cat&#39;, Pipeline(steps=[(&#39;ohe&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False))]), [1, 2, 3, 4, 5, 6, 7, 8, 9, 13]), (&#39;num&#39;, Pipeline(steps=[(&#39;std&#39;, StandardScaler())]), [14, 0, 15, 12, 11, 10, 17, 16, 18])]) . cat[1, 2, 3, 4, 5, 6, 7, 8, 9, 13] . OneHotEncoderOneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False) . num[14, 0, 15, 12, 11, 10, 17, 16, 18] . StandardScalerStandardScaler() . RandomForestClassifierRandomForestClassifier(class_weight=&#39;balanced&#39;, oob_score=True, random_state=0) . Validate the model . #Access the RF estimator from the pipeline rf = pipe[-1] # OOB, by default, gives Accuracy score. This is a slightly imbalanced dataset, # so we will calculate AUC on OOB predictions oob_pred = np.argmax(rf.oob_decision_function_,axis=1) auc1 = metrics.roc_auc_score(y1, oob_pred) print(&quot;OOB AUC is: &quot;,np.round(auc1,2)) print(&quot;OOB AUC on test set is: &quot;,np.round(roc_auc_score(y2, pipe.predict(x2)),2)) . OOB AUC is: 0.63 OOB AUC on test set is: 0.63 . Although the AUC is not very high, OOB gave an excellent estimation of the test score. We are happy with the model and it&#39;s ready to be used for future predictions. . Serialize the Model . Serialize the model using joblib . import joblib joblib.dump(pipe, &#39;baseline_rf.pkl&#39;) . [&#39;baseline_rf.pkl&#39;] . Test the pickle file on the test set again to make sure it&#39;s working as expected. . joblib.load(&#39;baseline_rf.pkl&#39;).predict(x2) . array([0, 0, 1, ..., 0, 0, 0]) . Success ! . We followed the entire process that&#39;s laid out in the &#39;typical&#39; machine learning process. But is that how it works in real life? . The answer is - A Resounding NO. . This is in fact just the fraction of the actual process. In reality it&#39;s very convoluted, non-linear process with multiple stakeholders/teams involved in creating the final model. You have business stakeholders defining the goals and business objectives, IT/Data Engineers who work on extracting/staging the data, Data scientist creating the models, software engineers integrating it with the product and business intelligence developers consuming the predictions in a dashboard. All these teams collaborate with each other, going through many iterations before finalizng a model. . . Ref: https://chapeau.freevariable.com/static/202010/mlops-tube.png . Things that are missing from this &#39;Typical&#39; process are: . Multiple stakeholders and collaborators. Identifying business objectives and tying it to model metric | Computational resources needed to run the ML models. If you are working on a dataset with millions of rows or a DNN model, you will very likely need GPUs | Experimentation design: algorithms, preprocessing steps, feature selection, feature engineering. You will create 1000s of models before identifying few model candidates that meet the business objectives. | Experiment tracking: You will need to efficiently track these 1000s of ML experiments to understand the patterns | Data versioning: You will work with several different versions of the data. By the time you arrive at the final model, the data used for training &amp; evaluating that model will be very different from what you started with. You or your colleagues will need to use a different version of that data for som eother project. | Track model artifacts: Each model will have its dependency requirements, input/output schema, hyper parameters | Package the model: Containerize the model with the dependendencies | Deploy &amp; monitor: scale, data security, performance monitoring, data drift, model interpretability | . This is where Azure Machine Learning Service helps! It&#39;s a fully managed cloud service that lets you: . Work in collaboration while giving control on data security and resources | Scale the compute targets as needed | Track data and model versions | Experiment with thousands of models and keep track of them | Deploy the models based on requirements (real-time, batch, IoT) | Monitor in production | Trace the model back to data and model artificats | DevOps | . Azure ML is a Fully managed MLOPS Platform that will help you manage the machine learning process based on project requirements. . Azure ML Service . Hopefully above example gave you reasons to learn and understand why MLOps is important. With Scikit-learn you can create the models but it won&#39;t help you put those models in production. We will now see how to operationalize this model using Azure ML. . Tour of Azure ML Service . Create a free Azure account by visiting the Azure page: https://azure.microsoft.com/en-us/services/machine-learning/ The account is free and you get $200 credit for the first 30 days. Create a &#39;Pay-As-You-Go&#39; subscription so you will incur costs for only the services you use. Be careful of creating compute resources. Shut them down when you are not using them to avoid a costly surprises. If you are a student, you may get some additional benefits. . Create Pay As You Go subscription . Create Azure ML Resource . From MS Docs: A resource group is a container that holds related resources for an Azure solution. The resource group can include all the resources for the solution, or only those resources that you want to manage as a group. You decide how you want to allocate resources to resource groups based on what makes the most sense for your organization. Generally, add resources that share the same lifecycle to the same resource group so you can easily deploy, update, and delete them as a group. . The resource group stores metadata about the resources. Therefore, when you specify a location for the resource group, you are specifying where that metadata is stored. For compliance reasons, you may need to ensure that your data is stored in a particular region. . . What&#39;s in Azure Resources ? . Ref: https://miro.medium.com/max/700/0*2B9p3J0A0efCL2J0.jpg . You can access and manage these resources in Azure ML studio using GUI. Some of these resources can also be managed using Azure ML SDK. As you create machine learning models, you will need to access these resources based on project requirements. The Python sdk will allow you to access them in your notebook on the fly. If the resources don&#39;t exist, you can create them programmatically. . Architecture . Azure ML Python SDK . I highly recommend creating a virtual enviroment that&#39;s specific to Azure ML projects to manage dependencies, especially for Azure Auto ML. Azure AutoML dependencies are often hard to resolve. . . Create a virtual enviroment (e.g evenv) and install Azure ML : pip install --upgrade azureml-sdk[notebooks,automl]. You can read more here . print(&quot;I am working with, Azure ML sdk ver: &quot;,azureml.core.VERSION) . I am working with, Azure ML sdk ver: 1.20.0 . Azure ML Classes . You will use these classes to operationalize the model in Azure ML. This list is defnitely not exchaustive, but 80% of the time you will be working with these classes in Azure ML. I encourage you to read the Microsoft documentation for each of these. . Feature Description Class . Workspace | Foundational resource in the cloud to manage experiments, models | Workspace(..) | . Compute Instance | Fully managed development environment (DVSM) | ComputeInstance(..) | . Compute Cluster | Fully managed multi-node, scaleable compute | ComputeTarget(..) | . Datastore | Azure Data storage | Datastore(..) | . Dataset | Abstracted File or Tabular data stored in Datastore | Dataset(..) | . Experiment | ML Experiment folder | Experiment(..) | . Run | An instance of an experiment with artifacts | Run(..) | . Log | Log metrics, artifacts related to run | Environment(..) | . Environment | Package environment and dependencies | .log(..) | . ScriptRunConfig | Configuration to run experiments | ScriptRunConfig(..) | . Model | Manage, register, deploy models in the cloud | Model(..) | . Webservice | Containerized packages for deployment, Endpoints | Webservice(..) | . Workspace . You may have different workspaces for different teams, projects etc. In fact, it&#39;s recommended to create different resource groups so all the project data, metadata, artifacts remain in that workspace. Especially if you are just trying Azure ML so you can just delete that resource without incurring any charges for any resources in the future. To create or access a workspace, we use Workspace() class. The easiest way is to download the config.json file from the resource group to your working directory. It has all the tenant, subscription information to connect to that workspace. You will be prompted to authenticate your credentials. . . from azureml.core import Workspace print(&quot;Connecting to the Workspace....&quot;, end=&quot;&quot;,sep=&#39; n&#39;) ws = Workspace.from_config() print(&quot; nWorkspacename:&quot;,ws.name, &quot;, nWorkspace location:&quot;, ws.location) . Connecting to the Workspace.... Workspacename: demows , Workspace location: centralus . We are connected to the workspace, now we can access the assets and artifacts in this workspace . Datastore . When the ML resource was created, Azure automatically created and attached a Blob storage to this workspace. That&#39;s the power of managed resources ! You can always attach other Blob, ADLSg2 storage accounts as needed. Let&#39;s access this default datastore. . # List all datastores registered in the current workspace datastores = ws.datastores for name, datastore in datastores.items(): print(name, datastore.datastore_type) . workspacefilestore AzureFile workspaceblobstore AzureBlob . We have two blog storage accounts in this workspace. Let&#39;s conenct to the default datastore. . datastore = ws.get_default_datastore() datastore . { &#34;name&#34;: &#34;workspaceblobstore&#34;, &#34;container_name&#34;: &#34;azureml-blobstore-dca32a5a-2be2-43c0-a924-9f9d9b7c7789&#34;, &#34;account_name&#34;: &#34;demows8142312183&#34;, &#34;protocol&#34;: &#34;https&#34;, &#34;endpoint&#34;: &#34;core.windows.net&#34; } . Remember this is the &#39;datastore&#39;. We haven&#39;t accessed any datasets in this datastore yet. Your data engineering team, for example, can do ETL using ADF, Synapse Analytics, Power Query etc. for you and register a dataset in this datastore.You can also add other datasets to this datastore. We will register the current bank marketing data to this datastore. Once registered, your other team members can access this dataset by just pointing to that dataset. . Dataset . Sometimes you may find it easier to use the GUI in the Azure ML studio to register a dataset. The GUI is more interactive and can also generate dataset profile. . Although it may not seem like a big deal, but being able to register, track, version the datasets seamlessly is one of the most important steps in creating reliable machine lerning models. In the model creation process, you will generate different versions of the data. By versioning and tracking, you will be able to trace which dataset was used for the training the deployed model and thus debug the models in production. . Don&#39;t take my work for it. See what renowned ML researchers, Andrew Ng and Francois Chollet (creator of Tensfor Flow), say about importance of data collection, versioning. Ref . . Following DataOps ptactices will pay rich dividends and avoid many headaches when yu have to debug and re-train models in the future. . The dataset you create in Azure ML is actually an abstraction/reference to the stored data and its metadata (ref). The datasets are lazily evaluated, which means: . No additional storage cost | Data versioning | No risk of changing original data | . Register a dataset . from azureml.core import Dataset from azureml.data.dataset_factory import DataType #First create a dataset object ds1 = Dataset.Tabular.from_delimited_files(path=path) #Register this dataset to the datastore print(&quot;Registering dataset to the cloud...&quot;, end=&quot;&quot;) ds1 = ds1.register(workspace = ws, name= &quot;bankmarketing&quot;, description = &quot;This is the original data&quot;) print(&quot; n nData registration successful n n&quot;, Dataset.get_all(ws) ) . Registering dataset to the cloud... Data registration successful { &#39;bankmarketing&#39;: DatasetRegistration(id=&#39;5845bcb9-fde0-4419-bba8-55b9e8296da8&#39;, name=&#39;bankmarketing&#39;, version=1, description=&#39;This is the original data&#39;, tags={})} . We have registered the original dataset to the default datastore. . Create a new version of the dataset . If you noticed in the training data used above, I dropped the duration column. We will create another version of the same dataset ds1 by dropping the duration column and call it ds2. . ds2 = ds1.drop_columns(&#39;duration&#39;) ds2 = ds2.register(workspace = ws, name= &quot;bankmarketing&quot;, description = &quot;Duration column dropped&quot;, create_new_version=True) . Dataset.get_all(ws) . {&#39;bankmarketing&#39;: DatasetRegistration(id=&#39;9980d47e-df35-45ec-972a-b68a8dd64bf6&#39;, name=&#39;bankmarketing&#39;, version=2, description=&#39;Duration column dropped&#39;, tags={})} . Note that we still have only 1 dataset in the Datastore. We just replaced the first dataset with the new version. Notice the version number version=2 above. We can retrieve any version when needed. We have only 1 file in the datastore not 2. This is how, by creating abstraction, we are able to save storage cost. If needed, you can add properties, tags, description to the dataset for future reference. In fact it&#39;s a good practice to do so for tracebility. . By default when you reference a dataset, it will always pull the latest version, unless specified. . We are still not done with the dataset. For training, we cleaned the data and split it into train/test. Those also need to be registered to the datastore. We don&#39;t have to but that&#39;s good DataOps/MLOps practice. Also, anytime you create a cross-validation folds for your final model training/validation, register those in the datastore too for reproducibility. To keep things simple, I am going to upload the train and test data created above to the datastore as one single csv file. Also note that you can directly register a pandas dataframe as a dataset. . #Create one single file with training and testing data #Add a column to label which data is train and test #This way we can keep the data in one single file # Besure sure to drop the &#39;data&#39; columns before training and testing. train = x1.copy() train[&#39;target&#39;] = y1 train[&#39;data&#39;] = &#39;train&#39; test = x2.copy() test[&#39;target&#39;] = y2 test[&#39;data&#39;] = &#39;test&#39; train_test_data = train.append(test) . # Register the pandas dataframe as a dataset # Add tags for traceability from azureml.data.dataset_factory import TabularDatasetFactory ds3 = (TabularDatasetFactory .register_pandas_dataframe( train_test_data, target=(datastore,&#39;bank_train_test&#39;), name=&#39;bank_train_test&#39;, tags = {&#39;Author&#39;:&#39;Sandeep&#39;,&#39;Project&#39;:&#39;Bank Marketing&#39;}, show_progress=True) ) . Method register_pandas_dataframe: This is an experimental method, and may change at any time.&lt;br/&gt;For more information, see https://aka.ms/azuremlexperimental. . Validating arguments. Arguments validated. Successfully obtained datastore reference and path. Uploading file to bank_train_test/e9360b10-63b9-4d6b-be50-53add1436e99/ Successfully uploaded file to datastore. Creating and registering a new dataset. Successfully created and registered a new dataset. . for dataset in Dataset.get_all(ws): print(dataset) . bank_train_test bankmarketing . Just for illustration purposes, if we want to retrieve a dataset by name, we can use the get_by_name() method. We can also see the id (i.e unique id) for the dataset. We will log this as an artifact during model building so we can trace the exact train/test used for future reference. . Dataset.get_by_name(ws, &#39;bank_train_test&#39;) . { &#34;source&#34;: [ &#34;(&#39;workspaceblobstore&#39;, &#39;bank_train_test/e9360b10-63b9-4d6b-be50-53add1436e99/&#39;)&#34; ], &#34;definition&#34;: [ &#34;GetDatastoreFiles&#34;, &#34;ReadParquetFile&#34;, &#34;DropColumns&#34; ], &#34;registration&#34;: { &#34;id&#34;: &#34;7b81a6c0-1e72-4948-86bb-ddac0e4e5d77&#34;, &#34;name&#34;: &#34;bank_train_test&#34;, &#34;version&#34;: 1, &#34;tags&#34;: { &#34;Author&#34;: &#34;Sandeep&#34;, &#34;Project&#34;: &#34;Bank Marketing&#34; }, &#34;workspace&#34;: &#34;Workspace.create(name=&#39;demows&#39;, subscription_id=&#39;4cedc5dd-e3ad-468d-bf66-32e31bdb9148&#39;, resource_group=&#39;1-f4dcfa62-playground-sandbox&#39;)&#34; } } . ds_uid = &#39;7b81a6c0-1e72-4948-86bb-ddac0e4e5d77&#39; . Datastore has two datasets now which can be accessed anytime or versioned. . Compute . We can train the model locally and deploy it to the cloud. But if you want to scale-up the process by parallelizing model training, you can use the compute cluster. There two types of compute: . Compute Instance: This is like a managed VM with R,Python, Jupyter installed. You can use it for remote training but can also be accessed from the Studio for development. . | Compute Cluster: This is a scalable multi-node compute, meaning if your training requires lot of compute power (e.g. 12 machines with 24 cores each) you can push the training to the compute cluster to do that. This can also be used for batch-inferencing. . | . For example purposes, I will show how to create it but won&#39;t use it. Compute is expensive. Companies often create compute quota to limit cost and use remote compute for hyperparameter tuning or large jobs. Note that if you are using Azure ML pipelines, you have to use Compute instance/cluster and local training is not available. . I generally prefer creating compute using GUI because you can see the cost of each compute option. . from azureml.core.compute import ComputeTarget, AmlCompute compute_name = &quot;DS12V2&quot; try: vm = ComputeTarget(ws, compute_name) print(f&quot;{compute_name} exists already&quot;) except: compute_config = AmlCompute.provisioning_configuration(vm_size=&quot;Standard_D2_V2&quot;, max_nodes=4) vm = ComputeTarget.create(ws, compute_name, compute_config) vm.wait_for_completion(show_output=True) . DS12V2 exists already Running . Experiment . This is the heart of machine learning and where all the magic happens. When you are working on a machine learning project, it&#39;s rarely a linear process as we discussed above. You try many different algorithms, debug them, understand how they work, try different preprocessing steps, feature engineering, data augmentation etc,, which means you will end up creating thousands of models per project. To keep track of all these experimental runs, Azure ML provides the Experiment class. . Think of Experiment() as a big giant folder where you save the model runs and the artifacts associated with that experiment. At the end of your experiment, you will see how each model performed based on selected metric and choose the right model for your project. The steps you will follow for each experimental run: . Create Experiment object | Start run | Log metrics | Get run/experiment details | . Just for demonstration purposes, I will create a Demo Experiment and log values 1,2,3 for metric called demo_metric. . from azureml.core import Experiment exp1 = Experiment(workspace=ws, name=&quot;Demo_Experiment&quot;) exp1 . NameWorkspaceReport PageDocs Page . Demo_Experiment | demows | Link to Azure Machine Learning studio | Link to Documentation | . If you click on the above link, it will take you directly to the Azure ML Studio Experiment page. We will created the Experiment, i.e folder. Now, we run some experiments . #Start run from azureml.widgets import RunDetails demo_run = exp1.start_logging() #Start Logging demo_run.log(&#39;demo_metric&#39; , 1) demo_run.log(&#39;demo_metric&#39; , 2) demo_run.log(&#39;demo_metric&#39; , 3) #Stop logging demo_run.complete() RunDetails(demo_run).show() . Remember to use run.complete() to stop the run. A better and easier way is to use with as follows. When the run is complete, it will be completed automatically. . For the bank marketing project, we created a random forest model using default hyper params. To demonstrate how create experiments, we will train four RF models by changing the max_depth parameter. When max_depth is None, it&#39;s just a stump of a tree. As we grow the depth, features are split and will identify non-linear patterns in the data. We will try max_depth = [None, 5,7,9]. In a real project, you will perform hyperparameter optimization using RandomSearch, Baysian Optimization using SKLearn, HyperOpt, HyperDrive etc. . #Create another experiment called bank from azureml.core import Run #Define new experiment bank = Experiment(workspace=ws, name=&quot;Bank_Marketing&quot;, ) #Define Hyperparameter to tune max_depth=[None,5,7,9] #Run the experiment for depth in max_depth: with bank.start_logging() as run: #snapshot only the snapshot directory snapshot_directory = &#39;snapshot&#39; #Log max_depth run.log(&#39;Model&#39;, &#39;Random Forest&#39;) run.log(&#39;Dataset&#39;, ds_uid) run.log(&#39;max_depth&#39;, int(0 if depth is None else depth)) run.log_list(&quot;input_columns&quot;, list(x1.columns)) #train the pipeline pipe2 = Pipeline([ (&#39;ct&#39;, ct), (&#39;rf&#39;, RandomForestClassifier(oob_score=True, random_state=0, class_weight = &#39;balanced&#39;, max_depth = depth )), ]) pipe2.fit(x1,y1) rf2 = pipe2[-1] #Log model details run.log(&#39;oob_score&#39;, &#39;True&#39;) run.log(&#39;class_weight&#39;, &#39;balanced&#39;) oob_pred2 = np.argmax(rf2.oob_decision_function_,axis=1) auc2 = metrics.roc_auc_score(y1, oob_pred2) #Log metrics run.log(&#39;oob_auc&#39;, auc2) print(&quot;max_depth: &quot;,depth,&quot; , oob_auc: &quot;, np.round(auc2,3)) RunDetails(run).show() . max_depth: None , oob_auc: 0.626 max_depth: 5 , oob_auc: 0.74 max_depth: 7 , oob_auc: 0.746 max_depth: 9 , oob_auc: 0.746 . By increasing the max_depth, AUC increased from 62% to 74% ! . In the experiment above, we logged the model class, dataset used, hyper parameter, input columns, AUC score etc. After the experiment is complete, you can visit the Studio to see the output and/or interact with the model artifacts. . Note that by default, when you create an experiment, Azure ML will take a snapshot of the working folder. See below. Depending on your needs this is a good/bad thing. You may not want to snapshot all the files and folder. You can specify an amlingnore or gitignore file to indicate which files/folders to ignore. Another option is to specify which directory to snapshot. For example, above I specified start_logging(snapshot_directory = &#39;snapshot&#39;) to snapshot the snapshot folder. This helps reproducibility. You can save data, yaml, config files etc so you or your colleagues can reproduce the results months later. The maximum snapshot limit is 300MB. If your directory exceeds that the run will fail. You can increase the limit but you will incur storage costs. Also, directories ./output and ./logs are special. They will always be automatically uploaded as snapshot. . I recommending &#39;snapshotting&#39; only the required model artifacts and specifying which folder to snapshot. . . Model Packaging - (Registration &amp; Deployment) . We ran some experiments with max_depth hyperparameters and found that using max_depth = [5,7,9] will improve the results significantly. Let&#39;s use &#39;one-standard error&#39; rule (Ref: ESL, pp61) to pick a parsomonious model. We will pick max_depth=5 for create a pickle file and deploy it in service. . Model accuracy is not the only metric, in fact it shouldn&#39;t be, to select a model. Focus should be on selecting simple, parsimonious models that are interpretable &amp; explainable. Watch my interpretability presentation for more details. For now, we will assume this is the right model for us. . final_model = Pipeline([ (&#39;ct&#39;, ct), (&#39;rf&#39;, RandomForestClassifier(oob_score=True, random_state=0, class_weight = &#39;balanced&#39;, max_depth = 5)), ]) final_model.fit(x1,y1) joblib.dump(final_model, &#39;bank_model.pkl&#39;) . [&#39;bank_model.pkl&#39;] . test_final_model = joblib.load(&#39;bank_model.pkl&#39;) . test_final_model.predict(x1.iloc[0:]) . array([0, 0, 0, ..., 1, 1, 1]) . print(&quot;AUC on Test set: ,&quot;, np.round(metrics.roc_auc_score(y2, test_final_model.predict(x2)),2)) . AUC on Test set: , 0.74 . Excellent, OOB score is same as the test (very rare!). . There are actually multiple ways to register and deploy a model as webservice. Typically, you will first create training script, register an environment, create inference schema, register model, create deployment config etc. But there is a shorter way to do all of that in one single step. Usually you will go through everything step-by-step but for demonstration purposes, I will roll these steps into one by using ResourceConfiguration class. Also, note that this is for real-time inferencing using Azure Container Instance. You should always deploy the model locally first for debugging, testing before deploying it to the cloud. For batch-inferencing, follow these steps. . You can also register and deploy using the interface in the Studio. . We will also save the sample features and labels for future reference and model debugging. . np.savetxt(&#39;features.csv&#39;, np.array(x1), delimiter=&#39;,&#39;, fmt=&#39;%s&#39;) np.savetxt(&#39;labels.csv&#39;, y1, delimiter=&#39;,&#39;) . datastore.upload_files(files=[&#39;./features.csv&#39;, &#39;./labels.csv&#39;], target_path=&#39;sample_data/&#39;, overwrite=True) input_dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, &#39;sample_data/features.csv&#39;)]) output_dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, &#39;sample_data/labels.csv&#39;)]) . Uploading an estimated of 2 files Uploading ./features.csv Uploaded ./features.csv, 1 files out of an estimated total of 2 Uploading ./labels.csv Uploaded ./labels.csv, 2 files out of an estimated total of 2 Uploaded 2 files . from azureml.core import Model from azureml.core.resource_configuration import ResourceConfiguration model = (Model.register(workspace = ws, model_name = &quot;bank_model&quot;, #name for the model model_path = &#39;./bank_model.pkl&#39;, #Specify the .pkl file model_framework=Model.Framework.SCIKITLEARN, #This will automatically create environment &amp; schema sample_input_dataset=input_dataset, #Sample input sample_output_dataset=output_dataset, #Sample output resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5), #ACI config to use description=&#39;Bank Marketing model to predict of customer will sign up&#39;, tags = {&#39;Author&#39;:&#39;Sandeep&#39;, &#39;Date&#39;:&#39;2/18/2021&#39;, &#39;Model&#39;:&#39;RandomForest&#39;, &#39;Dataset&#39;:ds_uid} )) . Registering model bank_model . print(&#39;Name:&#39;, model.name) print(&#39;Version:&#39;, model.version) . Name: bank_model Version: 1 . service = Model.deploy(ws, &quot;service3&quot;, [model]) service.wait_for_deployment(show_output=True) . Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes. Running............................................................................................ Succeeded ACI service creation operation finished, operation &#34;Succeeded&#34; . service . AciWebservice(workspace=Workspace.create(name=&#39;demows&#39;, subscription_id=&#39;4cedc5dd-e3ad-468d-bf66-32e31bdb9148&#39;, resource_group=&#39;1-f4dcfa62-playground-sandbox&#39;), name=service3, image_id=None, compute_type=None, state=ACI, scoring_uri=Healthy, tags=http://ec1b6ac7-1a4a-4d2e-8f98-ed8f8628ba44.centralus.azurecontainer.io/score, properties={}, created_by={&#39;hasInferenceSchema&#39;: &#39;True&#39;, &#39;hasHttps&#39;: &#39;False&#39;}) . service.state . &#39;Healthy&#39; . Webservcie status is Healthy and ready to be used in production for inferencing. . Create an input data to test inferencing . We will send 10 sample observations to test if the service is responding and returning expected output. You can also test this using the scoring_uri. . import json input = json.dumps({&#39;data&#39;:x1.iloc[:10,:].to_dict(&#39;list&#39;),&#39;method&#39;: &#39;predict&#39;}) headers = {&#39;Content-Type&#39;: &#39;application/json&#39;} . output = service.run(input) . output . {&#39;predict&#39;: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]} . We got the response back with predictions. Service is running and ready for action. If you are trying this example, be sure to delet the service and the weorkspace to avoid charges. . service.delete() . Monitor the Webservice . To monitor the performance of the webservice and the deployed model, we need to do few things: . Stress test the model for distribution shifts and loads | Collect webservice performance metric using Application Insights. This will help us collect: . Responses | Request rates, response time, failure rates | Exceptions | . | Monitor Concept Drift . Performance of the ML model will likely degrade over time due to change in distribution of the input data | By monitoring drift, we can measure the drift and decide when to re-train the model | . | Collect Model Interpretability data during inferencing . This is to track how model is creating predictions and if predictions are fair | . | . This is big topic and will require a separate presentation. But just know that with Azure ML service, you can monitor the model performance in production environment. . Next Steps . This was just the introduction to give you flavor for how to use Azure ML sdk. There are more advanced methods available depending on the project needs. I would encourage you to research those on your own from MS Docs and MS Learn. . Azure ML Pipelines | Azure ML HyperDrive | Azure Auto ML | Azure ML Studio Designer | . Resources . Microsoft Learn | Microsoft Documentation | . Thank you ! I hope you found this helpful. As always, feel free to get in touch if you have any questions. .",
            "url": "https://pawarbi.github.io/blog/azureml/sdk/mlops/python/dataops/2021/02/22/introduction-to-azureml-sdk-azure-saturday-mlops.html",
            "relUrl": "/azureml/sdk/mlops/python/dataops/2021/02/22/introduction-to-azureml-sdk-azure-saturday-mlops.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Quantized Discrete Parameter Sampling in AzureML HyperDrive",
            "content": "HyperDrive . Azure ML offers an automated hyperparameter tuning capability at scale using its &#39;HyperDrive&#39; package. The key word here is &#39;at scale&#39;. It works similar to Scikit-Learn&#39;s hyperparameter tuning but HyperDrive can be run on remote compute clusters with multiple nodes to obtain the optimized parameters faster. If your model pipeline has multiple models with different preprocessing steps, you will ikely end up with thousands of parameter combinations. This can take significant amount of time depending on your model pipeline architecture. With HyperDrive, you can submit the run to a managed remote cluster to run all the experiments and obtain the tuned parameters. Depending on the sampling method used, these runs can also be parallelized to run several experiments concurrently. . You can follow this example notebook to learn more about how to setup the HyperDrive experiment. I will share my more complex example hopefully in the next couple of blogs. . Parameter Search Space . The parameter search space can be defined using discrete or continuous distributions. . Discrete parameters are integers or strings (e.g. 1, 50, &#39;liblinear&#39; etc.), while continuous are floats (e.g. 1.2,0.5). You can read more about it here on MS Docs. One of the ways you can define the discrete hyperparameters is by using the quantized continuous distributions. These are the parameter expressions that start with &#39;q&#39;, e.g. quniform(),qloguniform(), qnormal() and qlognormal(). Note that the &#39;q&#39; parameters cannot be used for with GridParameterSampling() and BayesianParameterSampling() only supports quniform(). . Microsoft documentation doesn&#39;t give examples of the search space these &#39;q&#39; parameters create, so I thought I would provide some examples. . 1. quniform . quniform(low, high, q) creates uniform distriution between low and high values, separated by spacing q. For example, to create a discrete distribution of values between (10,30) that are 2 values values apart you would define : . from azureml.train.hyperdrive import normal, uniform, choice, quniform, qloguniform, qnormal, qlognormal import numpy as np import matplotlib.pyplot as plt %matplotlib inline import pandas as pd { &quot;param&quot; : quniform(10, 30, 2) } . {&#39;param&#39;: [&#39;quniform&#39;, [10, 30, 2]]} . This is what the search space will look like: . #with q = 2 param = np.sort(np.unique((np.round(np.random.uniform(low = 10, high = 30, size=10)/2)*2))) param . array([12., 18., 22., 28.]) . #with q = 3 param = np.sort(np.unique(np.round(np.random.uniform(low = 10, high = 30, size=10)/3)*3)) print(param) sns.histplot(param, kde=True); . [12. 15. 18. 24. 27. 30.] . As you can see above, qith q=2 the values are spaced apart by 2 values and with q=3, the values are 3 units apart. Note that the values are randomly distributed so you may not see all the values between the specified low and high. You could specify the same search space by using choice(12, 15, 17,27). But if the search space is large, it&#39;s easier to use the quantized version. . 2. qloguniform . Defined as : . { &quot;param&quot; : qloguniform(0.01, 5, 3) } . {&#39;param&#39;: [&#39;qloguniform&#39;, [0.01, 5, 3]]} . #with q = 3 param = np.sort(np.unique(np.round(np.exp(np.random.uniform(0.01,5,30)))/3)*3) print(param) sns.histplot(param, kde=True) . [ 1. 2. 3. 4. 5. 7. 9. 10. 11. 17. 18. 19. 37. 47. 50. 57. 59. 102. 141.] . &lt;AxesSubplot:ylabel=&#39;Count&#39;&gt; . 3.qnormal . Values are normally distributed with specified mean and standard deviation. Defined as: . { &quot;param&quot; : qnormal(300,50, 5) } . {&#39;param&#39;: [&#39;qnormal&#39;, [300, 50, 5]]} . #with mean = 300, std = 50, q = 5 param = np.sort(np.unique(np.round((np.random.normal(300,50,50)))/5)*5) print(param) sns.histplot(param, kde=True) . [196. 199. 200. 236. 241. 255. 257. 258. 260. 261. 274. 277. 278. 280. 284. 285. 286. 287. 291. 296. 298. 300. 302. 303. 305. 306. 307. 310. 312. 314. 317. 319. 323. 324. 331. 338. 341. 345. 350. 372. 395.] . &lt;AxesSubplot:ylabel=&#39;Count&#39;&gt; . This would be a good choice for, say n_estimators() in tree-based algorithms. You can always adjust the q parameter to define how close the values can be. . 4. qlognormal . Values are lognormally distributed with the specified mean and standard deviation. . { &quot;param&quot; : qlognormal(10,0.5, 2) } . {&#39;param&#39;: [&#39;qlognormal&#39;, [10, 2, 2]]} . #with mean = 300, std = 50, q = 2 param = np.sort(np.unique(np.round(np.exp(np.random.normal(10,0.5,25))/2)*2)) print(param) sns.histplot(param, kde=True); . [ 7426. 9054. 9668. 10800. 10816. 10886. 12758. 14232. 14346. 14838. 15376. 17612. 18320. 19152. 20512. 22676. 25370. 26078. 26398. 32758. 37050. 41412. 43732. 54636. 58332.] . This would be helpful for creating non-uniform distributin of the parameters that are right-skewed . References . https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters | https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py#azureml_train_hyperdrive_parameter_expressions_uniform . | https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.bayesianparametersampling?preserve-view=true&amp;view=azure-ml-py . |",
            "url": "https://pawarbi.github.io/blog/azureml/hyperdrive/parameter%20expression/qnormal/quniform/2021/02/01/hyperdrive-parameter-sampling.html",
            "relUrl": "/azureml/hyperdrive/parameter%20expression/qnormal/quniform/2021/02/01/hyperdrive-parameter-sampling.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "",
            "content": "Forecasting in Power BI . I have written and talked about Time Series Forecasting previously, especially on how to create forecasts in Power BI. You can refer to my previous blog posts below: . Time series Forecasting in Python &amp; R, Part 1 (EDA): Link This blog covers exploratory data analysis techniques for time series data before creating the forecast. . Time series Forecasting in Python &amp; R, Part 2 (Forecasting): Link Here I go into theoretical and practical aspects of creatings forecasts using many different classical forecasting methods . Time series Forecasting in Power BI: Link This blog shows how to create forecasts in Power BI, its strengths, limitations, and my recommendations. . I recently presented on this topic at Global AI Bootcamp, Singapore. Here is the recording of that presentation. It summarizes my findings from the blog above. . .",
            "url": "https://pawarbi.github.io/blog/2022/10/14/2021-01-20-timeseries-forecasting-in-powerbi.html",
            "relUrl": "/2022/10/14/2021-01-20-timeseries-forecasting-in-powerbi.html",
            "date": " • Oct 14, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "Video Recording- Time Series Forecasting in Power BI",
            "content": "Forecasting in Power BI . I have written and talked about Time Series Forecasting previously, especially on how to create forecasts in Power BI. You can refer to my previous blog posts below: . Time series Forecasting in Python &amp; R, Part 1 (EDA): Link This blog covers exploratory data analysis techniques for time series data before creating the forecast. . Time series Forecasting in Python &amp; R, Part 2 (Forecasting): Link Here I go into theoretical and practical aspects of creating forecasts using many different classical forecasting methods . Time series Forecasting in Power BI: Link This blog shows how to create forecasts in Power BI, its strengths, limitations, and my recommendations. . I recently presented on this topic at Global AI Bootcamp, Singapore. Here is the recording of that presentation. It summarizes my findings from the blog above. . .",
            "url": "https://pawarbi.github.io/blog/power%20bi/timeseries/forecasting/2021/01/20/powerbi-forecasting.html",
            "relUrl": "/power%20bi/timeseries/forecasting/2021/01/20/powerbi-forecasting.html",
            "date": " • Jan 20, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "Video Recording- Machine Learning Model Interpretability Using Azure ML",
            "content": "Model Interpretability . I covered below topics in my presentation: . What is Model Interpretability? | Why do we need to build interpretable models? | Accuracy Fallacy : Accurate model does not mean correct model | How to create interpretable glassbox models using Explainable Boosting Machine ? | How Explainable Boosting Machine models work? | How to use interpret-ml library to obtain globally and locally important features? | Case Study on probing, de-bugging, comapring two different models using interpret-ml | Using Microsoft&#39;s &#39;Design Probe Thinking&#39; | . References &amp; Resources . How to explain models with InterpretML | GAMUT | Using Interpret API | Science Behind InterpretML | FICO Challenge | Interpretability using Python | Stop explaining Black Box Models | .",
            "url": "https://pawarbi.github.io/blog/azureml/interpretml/explainableboostingmachine/ga2m/interpretability/xai/2021/01/20/machine-learning-model-interpretability-azureml-interpretml.html",
            "relUrl": "/azureml/interpretml/explainableboostingmachine/ga2m/interpretability/xai/2021/01/20/machine-learning-model-interpretability-azureml-interpretml.html",
            "date": " • Jan 20, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "Migrating Power BI Dataflows Across Workspaces",
            "content": "Migrating Dataflow . I was recently involved in a project to migrate multiple dataflows across workspaces. While exporting the dataflow by creating a .json file is known, I didn&#39;t find any references on how to fix the Power BI Desktop report to point to the migrated dataflows. In this post I will describe how to do that easily. If you prefer, you can watch the video at the end of the blog post. . Steps: . First and foremost you will need to setup the enterprise data gateway, add the required data sources and authenticate them. Before following any of the steps below, ensure that the gateway is setup correctly and its running. . | In Power BI service, browse to the dataflow you want to export . | Click on the ellipses (three dots) and select Export .json. The json file contains the data and the metadata associated with the dataflow for all the queries within it. . | . . In Power BI service, browse to the second workspace to which you want to migrate the saved dataflow and select Import Model. Import the saved the json file. | . . You will be prompted to Edit Credentials. If your data source needs credentials, enter the credentials under Data source credentails in Settings . | Refresh the newly imported dataflow to make sure you are able to pull the data and run the queries . | Select the imported dataflow and copy the URL in the browser. The URL will be in following format: . https://app.powerbi.com/groups/{workspace_id}/dataflows/{dataflow_id} . | . Copy the workspace id &amp; the dataflow id, we need those to reconfigure the queries in Desktop. If you have multiple dataflows, you may find it easier to use the API . | Open the report in Power BI desktop, create two parameters for workspace_id and the dataflow_id. Use the above copied strings as the default values. You can watch the below video if you don&#39;t know how to create the parameters . | Open the M code for each query in the dataflow and replace the workspaceId=&quot;{original_workspaceid}&quot; with the workspaceId=workspace_id_parameter . | . . Repeat the same thing for the dataflow id, replace dataflowId=&quot;{original_dataflowid}&quot; with dataflowId=dataflow_id . | Repeat this for each dataflow query. Since you created the parameters, you only need to replace the id strings with the respective parameters . | Refresh all queries to ensure they are working . | Delete the original dataflow from the first workspace as we don&#39;t need it anymore . | . Video . . References . Dataflow | Workspaceid | .",
            "url": "https://pawarbi.github.io/blog/powerbi/dataflow/migration/workspaces/json/2021/01/05/migrating-dataflow.html",
            "relUrl": "/powerbi/dataflow/migration/workspaces/json/2021/01/05/migrating-dataflow.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "DAX Formulae For Creating Statistical Distributions in Power BI",
            "content": "Why create statistical distributions? . As a simulation engineer, one of my responsibilities is to create statistical experiments to simulate various scenarios and understand the outcomes. These outcomes have probabilities associated with them which help the decision makers to identify key risks, uncertainties and make informed decisions to leverage risk. For example, if the company wants to invest a million dollars in a capital project, there are many uncertainties involved such as labor costs, scheduling uncertainty, project delays, technical risks etc. A statistical simulation experiment, i.e Monte Carlo simulation, will assign probabilities with these risks and simulate thousands of outcomes to compute the probability of success. The company can then decide if it wants to invest the $1M or what decisions it can take proactively to de-risk the uncertainties. . There are many statistical packages and Excel add-ons available to create such experiments but none that I know of in Power BI. In Power BI, only uniform distribution and normal distributions are available natively in DAX. In this blog post I share the DAX codes I use to create different distibutions. . If you are new to Monte Carlo / Discrete Event simulation, these distributions may not make sense. In a future blog post I will show when to use these distributions and how to create sophisticated simulations in Power BI. The beauty of creating it in Power BI vs Excel is that the stakeholders can interact with the simulation parameters and understand the probable outcomes much more clearly. The DAX codes given here can be used to create calculated tables for simulations. . If you are interested in this topic, please also take a look at my other blog on generating random numbers in Power BI . Uniform Distribution . Uniform distribution to create values between 100 &amp; 400. . DAX: . Uniform = RANDBETWEEN(100,400) . Normal Distribution . Normal distribution with mean 10, std dev of 1. . DAX: . `Normal = NORM.INV(RAND(),10,1)` ` . LogNormal Distribution . LogNormal distribution with mean = 80, variance = 225 . DAX: . LogNormal = VAR _m = 80 // mean VAR _v = 225 // variance (not std deviation) VAR _phi = SQRT ( _v + _m ^ 2 ) VAR _mu = LN ( ( _m ^ 2 ) / _phi ) // scaled mean VAR _sigma = SQRT ( LN ( ( _phi ^ 2 ) / _m ^ 2 ) ) RETURN // scaled s.d EXP ( NORM.INV ( RAND (), _mu, _sigma ) ) . Beta PERT . Beta PERT with: Min likely value = 100 Med likely value = 300 Max likely value = 800 . DAX: . Beta Pert = VAR _a = 100 VAR _b = 300 VAR _c = 800 VAR _alpha = ( 4 * _b + _c - 5 * _a ) / ( _c - _a ) VAR _beta = ( 5 * _c - _a - 4 * _b ) / ( _c - _a ) RETURN BETA.INV ( RAND (), _alpha, _beta, _a, _c ) . Discrete . Discrete with: . Probability of Choice 1 = 50% , Probability of Choice 2 = 30% , Probability of Choice 3 = 20% . DAX: . Discrete = IF ( &#39;Table&#39;[random_chocie] &lt;= 0.5, &quot;Choice 1&quot;, IF ( ( &#39;Table&#39;[random_chocie] &gt; 0.5 &amp;&amp; &#39;Table&#39;[random_chocie] &lt; 0.8 ), &quot;Choice 2&quot;, IF ( ( &#39;Table&#39;[random_chocie] &gt; 0.8 ), &quot;Choice 3&quot; ) ) ) . Exponential Distribution . Exponential with lambda = 120 . DAX: . Exponential = VAR lambda = 120 RETURN // Exponential distribution with a mean of lambda -1 * lambda * LN ( 1 - RAND () ) . Poisson Distribution . Poisson distribution with mean = 120 . DAX: Poisson = . VAR lambda = 120 RETURN // Poisson dist with a mean of lambda NORM.INV ( RAND (), lambda, SQRT ( lambda ) ) . Truncated Normal Distribution . Truncated normal distribution with: lower limit = 1 higher limit = 4 mean = 3 s.d = 0.9 . DAX: truncatednormal = . VAR _a = 1 VAR _b = 4 VAR _mu = 3 VAR _sigma = 0.9 RETURN NORM.INV ( NORM.DIST ( _a, _mu, _sigma, TRUE ) + RAND () * ( NORM.DIST ( _b, _mu, _sigma, TRUE ) - NORM.DIST ( _a, _mu, _sigma, TRUE ) ), _mu, _sigma ) . Weibull Distribution . Weibull distribution with: shape parameter, alpha = 2 scale parameter, beta = 10 . DAX . Weibull = VAR _alpha = 2 // shape parameter VAR _beta = 10 // scale parameter RETURN _beta * ( - LN ( 1 - RAND () ) ) ^ ( 1 / _alpha ) . Logistic Distribution . Logistic distribution with: location parameter, a = 2 scale parameter, k = 10 . DAX . Logistic = VAR _a = 2 // Location Parameter VAR _k = 10 // Scale Parameter RETURN _a + _k * LN ( ( RAND () / ( 1 - RAND () ) ) ) . Laplace Distribution . Laplace distribution with: mean = 10 std dev = 2 . DAX . Laplace = VAR _rand = &#39;Table&#39;[random_chocie] VAR _mu = 10 VAR _sigma = 2 RETURN IF ( _rand &lt;= 0.5, _mu + LN ( 2 * _rand ) / _sigma, 10 - LN ( 2 - 2 * _rand ) / _sigma ) . Gumbel Distribution / Extreme Value Distribution . Gumbel/EV Distribution with: mean = 10 std dev = 2 . DAX . Gumbel = VAR _mu = 10 VAR _sigma = 2 RETURN _mu - ( _sigma * LN ( - LN ( RAND () ) ) ) . You may find below visual helpful in determining which distribution to use: . .",
            "url": "https://pawarbi.github.io/blog/power%20bi/statistics/distribution/pert/beta/normal/uniform/lognormal/logistic/weibull/2020/12/24/Statistical-distributions-powerbi.html",
            "relUrl": "/power%20bi/statistics/distribution/pert/beta/normal/uniform/lognormal/logistic/weibull/2020/12/24/Statistical-distributions-powerbi.html",
            "date": " • Dec 24, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Importing Google Form Responses in Power BI",
            "content": "Importing Data from Google Form . In this short blog post, I will show how to import responses received in Google Form in Power BI. This will require you to publish the responses to web. Note that this does not mean the data could be searched via a search engine. But if someone has a link to the published data, they will be able to see it (not edit it) and use it. You can always disable the publishing by clicking Stop Publishing in Publish to Web option. . With this method you can refresh the Power BI dashboard and pull the latest responses from Google Form. . Note that Microsoft has Microsoft Form which is free but as far as I know it does not allow publishing the form response data to web. If you have confidential data, Microsoft Form is a better option as Power BI will need the access to Microsoft Form to be authenticated before importing the data. However, in my case, I was working with a nonprofit organization to analyze their survey responses so Microsoft Form was not an option. For more complex workflows ,and large data, I advise using Microsoft Form, Power Automate and SharePoint together. . Steps . Open the Google Form in edit mode and click on Responses | . Click on View Responses in Sheets | . File &gt; Publish to the Web | . Either create a new sheet or select an existing sheet. | . Under Link, select Form Responses 1, select Commas separated values .csv. Select Publish.Copy the URL generated and inspect it to make sure it ends with gid=123456789&amp;single=true&amp;output=csv* | . . Open Power BI Desktop and select Text/CSV and enter the URL generated above | . Import the data, build the dashboard and publish to Power BI service | . To schedule the refresh, for Data Source Credentials, choose Authentication Method as Anonymous &amp; Privacy Level as None | . Submit a new test response and refresh the dataset in Power BI service to make sure dashboard is pulling the latest data. |",
            "url": "https://pawarbi.github.io/blog/power%20bi/google%20form/2020/12/14/Importing-Google-Form-Data-PowerBI.html",
            "relUrl": "/power%20bi/google%20form/2020/12/14/Importing-Google-Form-Data-PowerBI.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Accessing Power BI Datasets via XMLA Endpoint in Python Jupyter Notebook",
            "content": "Power BI recently announced availability of a new license &#39;Premium Per User&#39; which will allow small to medium size companies to access Premium features at a lower price point (as of 12/11/2020 price has not been published) . I was given a priority access to it and have been exploring all the premium features for the last few weeks. One of the key benefits of Premium is the ability to consume published datasets via XMLA endpoint. Instead of me describing it, you can read Microsoft&#39;s documentation for detailed description here. . To use XMLA endpoint, you will need to have a Premium workspace and enable the &#39;XMLA Endpoint&#39; in capacity settings. . Capacity Settings &gt; Power BI Premium &gt; Workloads &gt; XMLA Endpoint . . In this blog post I will show how to setup connection to the XMLA endpoint and access Power BI datasets &amp; measures within those datasets using Python in a Jupyter notebook. This blog post is written in a Jupyter notebook. I use Jupyter notebooks for data exploration, visualization and building machine learning models. Ability to consume published datasets in a notebook will be immensely helpful in my own workflow. . I took inspiration from David Elderveld&#39;s amazing 4-part blogposts on &#39;Jupyter as an External Tool for Power BI Desktop&#39;. It really opened up lot of options that I didn&#39;t know existed, thanks David! I highly encourage you to read all the four posts to learn more about using Python with Power BI. . Secondly, none of this would be possible without the python-ssas module by Josh Dimarsky which enables TOM connection to Analysis Services. . To get started, you will need three things: . install the pythonnet library (pip install pythonnet) | Make sure you have Microsoft.AnalysisServices.Tabular.dll and Microsoft.AnalysisServices.AdomdClient.dll dlls. If you use Excel, Power BI, you will most likley already have them. As Josh mentions in his documentation, they are usually installed in C: Windows Microsoft.NET assembly GAC_MSIL | copy the ssas_api.py file from here and save it in your Python libraries folder or Jupyter working directory. | . import pandas as pd import ssas_api as ssas import seaborn as sns . Connect to Premium Workspace . First we want to connect to our Power BI service account &amp; the Premium workspace. You will need to find the workspace connection string URL. It will be in this format powerbi://api.powerbi.com/v1.0/[tenant name]/[workspace name]. You can obtain it from your workspace settings (see below) . First go to your Premium Workspace and click on Settings. Notice that I have three datasets in this workspace, viz Dataset1, dataset2 and dataset2_2. . . Next, copy the server address from Workspace Connection: . . Create variables for your tenant username and password . server = &#39;workspace_connection_from_above&#39; username = &#39;name@email.com&#39; password = &#39;password&#39; . Setup connection string by specifying the server name, username &amp; password. Note that I have left the database name (db_name) blank here because I am assuming that we don&#39;t know the names of the datasets that are available in this workspace. We will explore the workspace and get a list of all the datasets. If you already know the dataset name, you can enter it between the single quotations. . conn1 = ssas.set_conn_string( server=server, db_name=&#39;&#39;, username=username, password=password ) . Import libraries for Tabular Object Model (TOM) and setup the TOM server. As long as your username and password are valid, you will connect to your workspace using TOM. If it fails, it means either your credentials are incorrect or you do not have access to the workspace. You will not be prompted for interactive authentication. . global System, DataTable, AMO, ADOMD import System from System.Data import DataTable import Microsoft.AnalysisServices.Tabular as TOM import Microsoft.AnalysisServices.AdomdClient as ADOMD try: TOMServer = TOM.Server() TOMServer.Connect(conn1) print(&quot;Connection to Workspace Successful !&quot;) except: print(&quot;Connection to Workspace Failed&quot;) . Connection to Workspace Successful ! . Access Datasets . Let&#39;s now get a list of all the avaible datasets in this workspace and their metadata such as compatibility level, database id, size in megabytes, creation date and last update date. . datasets = pd.DataFrame(columns=[&#39;Dataset_Name&#39;, &#39;Compatibility&#39;, &#39;ID&#39;, &#39;Size_MB&#39;,&#39;Created_Date&#39;,&#39;Last_Update&#39; ]) for item in TOMServer.Databases: datasets = datasets.append({&#39;Dataset_Name&#39; :item.Name, &#39;Compatibility&#39;:item.CompatibilityLevel, &#39;Created_Date&#39; :item.CreatedTimestamp, &#39;ID&#39; :item.ID, &#39;Last_Update&#39; :item.LastUpdate, &#39;Size_MB&#39; :(item.EstimatedSize*1e-06) }, ignore_index=True) datasets . Dataset_Name Compatibility ID Size_MB Created_Date Last_Update . 0 Dataset1 | 1465 | a28e52a8-d6b3-4b5c-8e61-e107fbd52748 | 1.916094 | 8/21/2018 3:34:53 PM | 12/11/2020 11:24:21 AM | . 1 dataset2_2 | 1520 | a93244fe-860d-45c2-9d36-b44fb284c54d | 0.454222 | 12/11/2020 10:01:57 AM | 12/11/2020 11:20:50 AM | . I have three datasets in this workspace but I only see 2! dataset2 and dataset2_2 are identical except dataset2 was created by uploading the csv file to the worspace while the other two datasets were published to the workspace from Power BI Desktop. Also note that their compatibility levels are different. Dataset1 was published with the old version of Power BI Desktop with &#39;Enhanced Metadata&#39; option turned off that&#39;s why its compatibility level is not 1520. Since September 2020 release, Power BI Desktop automatically converts the datasets to Enhanced Metadata. . Now let&#39;s choose the dataset we want to connect to. In this case I want to access dataset2_2. I will copy the database id from the ID column from the above table. Then we will find the names of all the tables that are available in that dataset. . ds = TOMServer.Databases[&#39;a93244fe-860d-45c2-9d36-b44fb284c54d&#39;] for table in ds.Model.Tables: print(table.Name) . dataset2 . In the dataset2_2 dataset, the only available table is dataset2. Let&#39;s connect to this table. We will create a new connection string by specifying the db_name this time. . conn2 = (ssas.set_conn_string( server=server, db_name=&#39;dataset2_2&#39;, username = &#39;name@email.com&#39;, password = &#39;password&#39; )) . You can write any valid DAX query against this table and get the results back as a Pandas dataframe. . dax_string = &#39;&#39;&#39; //Write your DAX Query here EVALUATE dataset2 &#39;&#39;&#39; df_dataset2 = (ssas .get_DAX( connection_string=conn, dax_string=dax_string) ) df_dataset2.head() . dataset2[vendorID] dataset2[passengerCount] dataset2[tripDistance] dataset2[hour_of_day] dataset2[day_of_week] dataset2[day_of_month] dataset2[month_num] dataset2[normalizeHolidayName] dataset2[isPaidTimeOff] dataset2[snowDepth] dataset2[precipTime] dataset2[precipDepth] dataset2[temperature] dataset2[totalAmount] . 0 2 | 1 | 2.62 | 2 | 6 | 19 | 6 | None | False | 0.0 | 1 | 0 | 23.116071 | 11.80 | . 1 2 | 1 | 0.42 | 17 | 6 | 19 | 6 | None | False | 0.0 | 1 | 0 | 23.116071 | 4.30 | . 2 2 | 1 | 1.78 | 18 | 6 | 19 | 6 | None | False | 0.0 | 1 | 0 | 23.116071 | 12.96 | . 3 2 | 1 | 1.26 | 16 | 6 | 19 | 6 | None | False | 0.0 | 1 | 0 | 23.116071 | 9.36 | . 4 2 | 1 | 2.37 | 22 | 6 | 19 | 6 | None | False | 0.0 | 1 | 0 | 23.116071 | 10.80 | . sns.boxplot(df_dataset2[&#39;dataset2[totalAmount]&#39;]) . &lt;AxesSubplot:xlabel=&#39;dataset2[totalAmount]&#39;&gt; . Access Measures . We can also get a list of all the measures and their DAX expressions. As seen below, in this table we have only one measure called totalAmount average per vendorID with its DAX. . table = ds.Model.Tables.Find(&#39;dataset2&#39;) for measure in table.Measures: print(measure.Name, &quot;: n&quot;, measure.Expression) . totalAmount average per vendorID : AVERAGEX( KEEPFILTERS(VALUES(&#39;dataset2&#39;[vendorID])), CALCULATE(SUM(&#39;dataset2&#39;[totalAmount])) ) . Based on this Microsoft documentation, I was under the impression that only the datasets with Compatibility level 1500 or higher could be accessed via XMLA. That&#39;s not the case. Dataset1 has a compatibility of 1465 and can still be accessed via XMLA endpoint as seen below. . conn3 = (ssas.set_conn_string( server=server, db_name=&#39;Dataset1&#39;, username = &#39;name@email.com&#39;, password = &#39;password&#39; )) dax_string2 = &#39;&#39;&#39; //Write your DAX Query here EVALUATE KeyInfluencers_Predictors &#39;&#39;&#39; df_dataset1 = (ssas .get_DAX( connection_string=conn3, dax_string=dax_string2) ) df_dataset1.head() . KeyInfluencers_Predictors[Feature Name] KeyInfluencers_Predictors[Feature Type] KeyInfluencers_Predictors[Feature Importance] . 0 col6 | Numeric | 0.417869 | . 1 col0 | Numeric | 0.257106 | . 2 col4 | Numeric | 0.106939 | . 3 col3 | Numeric | 0.090845 | . 4 col1 | Numeric | 0.066334 | . Datasets in Power BI service can currently be accessed via number of different tools (SSDT, SSMS, PowerShell, Tabular Editor, DAX Studio, ALM Toolkit) which are all database modeling tools. But Data Scientists now can easily access the datasets for exploring the data and building machine learning models in Jupyter Notebook or VSCode using Python. . This method can also be used to export multiple refreshed dataset(s) on a schedule and on demand. . There are still few things I am not sure about and need to explore more. . Can I use service principal, key etc so I don&#39;t have to write username/password in my code | How to write back, create dataset using Python and push it to the workspace | Can I create/modify measures? | What are the limitations ? | . Please feel free to post your comments below or reach out to me if you find any errors or have suggestions. . References: . https://dataveld.com/2020/07/20/python-as-an-external-tool-for-power-bi-desktop-part-1/ | https://powerbi.microsoft.com/en-us/blog/power-bi-premium-per-user-public-preview-now-available/ | https://github.com/yehoshuadimarsky/python-ssas/ | https://docs.microsoft.com/en-us/power-bi/admin/service-premium-connect-tools | https://docs.microsoft.com/en-us/analysis-services/tom/introduction-to-the-tabular-object-model-tom-in-analysis-services-amo?view=asallproducts-allversions | https://docs.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.measure?view=analysisservices-dotnet | .",
            "url": "https://pawarbi.github.io/blog/ppu/xmla/powerbi_premium/premium/python/jupyter_notebook/2020/12/11/Accessing-Power-BI-Datasets-via-XMLA-Endpoint-in-Python-Jupyter-Notebook.html",
            "relUrl": "/ppu/xmla/powerbi_premium/premium/python/jupyter_notebook/2020/12/11/Accessing-Power-BI-Datasets-via-XMLA-Endpoint-in-Python-Jupyter-Notebook.html",
            "date": " • Dec 11, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Azure Data Scientist Certification (DP-100) Resources & Tips",
            "content": "DP-100 . The DP-100 exam (DP-100: Designing and Implementing a Data Science Solution on Azure) measures your ability to set up Azure ML workspace, create ML experiments and deploy them in service. I took the exam on Nov 23, 2020. Note that the exam contents will change on December 8, 2020 so if you are going to take the exam after Dec 8, 2020, please visit the exam website for updated content. . This exam covers 4 main areas of Azure ML: . Set up an Azure ML workspace (30-35%) | Run, track, manage, train ML models (20-25%) | Optimize models, AutoML (20-25%) | Deploy, monitor, consume models (20-25%) | . Before you do anything, make sure to check the official exam page first on MS Certifications page for the latest exam as MS often retires exams and replaces them as technology evolves. . The exam fee is $165 but I got it for free with MS Ignite Voucher. Keep an eye on MS online events for free vouchers. Exam length is 180 minutes and you are asked 58 questions covering the above topics. . Resources . Here are the resources I used to learn Azureml ML Service and prepare for the exam. . First create a free Azure account with $200 in credit. You will need it for practice | If you are new to Python or Machine Learning, start with this MS Learn track. If you are already familiar with Python &amp; ML, you can skip it. | Start simple to get hang of the no-code ML offering from Azure and create scaleable ML models using the designer in Azure ML Studio. Link. Note that the exams will have questions on the Designer and not the deprecated Azure ML Studio Classic. The two are different. This should also familiarize you with Azure ML workspace. | Take the Build AI Solutions with Azure ML course on MS learn. This is the most important resource you will want to study. This covers all the DP-100 topics really well. | As you go through each module in MS Learn, open the corresponding page on MS Documentation and read it. Many of the topics will be same but the documentation page adds more details &amp; examples that I found very helpful. | Labs: These labs will give you hands-on experience with Azure ML SDK. These are very important and highly recommend you complete all of them. | Lab Videos: These videos are supplementary to the labs above. Not very helpful as there is no audio. | If you like to follow along with someone instead of doing it on your own, I recommend taking the DP-100 course by Ginger Grant. The course is completely free and covers everything you need for DP100. Below are the links Module 1, Day 1 | Module 2, Day 2 | Module 3, Day 3 | Module 4, Day 4 | . | Pluralsight: Pluralsight, in partnership with Microsoft, offers free instroctor led DP-100 courses. I watched most of them, I found Ginger&#39;s course above more thorough. | Cloudacademy also offers DP-100 course (not free). I created a free 7 day trial account to check out the content and it&#39;s very bad. It&#39;s basically just commentary on the examples from the labs above. Don&#39;t waste your time. If you already have a cloud academy membership, you can explore but don&#39;t pay for it. | Udacity: Udacity has a Azure ML Engineer program in collaboration with Microsoft. I won the scholarship for that course. It covers all the DP-100 topics and plus some more. If you are enrolled in this course, you should be able to pass DP-100 comfortably. I wouldn&#39;t take the course just to prepare for the exam. The content in the course is good but very similar to Ginger&#39;s videos above. | If you want to learn more advanced Azure ML sdk, this is a great repo with all the tutorials and examples. This is above and beyond what&#39;s required for DP-100 and I recommend completing above labs first before using this. | Tips . Don&#39;t skip the labs. You will be asked questions on Azure ML SDK and you need to know it very well | You will be asked questions on regression, classification modeling and metrics so you need to know machine learning. | Almost all the courses/resources above (except some courses on Pluralsight) assume that you know how to create regression, classification models. If you don&#39;t have any experience with machine learning, use the MS Learn track from above. It&#39;s minimum and you should start with that first. | Learn the mlflow api and how to use it with Azure ML sdk: link | If you have not taken any Azure certifications before, I recommend taking an easier exam first (e.g. AZ900) to get a feel for it first. | Start the exam check-in process at least 30 minutes prior to your schedule. It takes time and sometimees you may face technical difficulties so give yourself ample time before the exam. | You can always reschedule the exam (I did) if you think you need more time to prepare or have other commitments. | If you create a free Azure account, create a new resource with Azure ML workspace so it will be easier for you to delete it later. Azure ML compute is expensive so be sure to keep an eye on the usage. Don&#39;t forget to stop the compute target when not in use, otherwise you will be shocked when you get the invoice form Azure. Don&#39;t ask me how I know this ;). If you do incur charges, reach out to Azure support team. They will often waive the charges. | You will be able to complete most labs (except Pipelines &amp; deployment) using your local compute. | As mentioned above, MS often gives away exam vouchers or discounts so keep an eye on their events page. | Connect with me on twitter (@PawarBI) if you have questions | Good luck ! .",
            "url": "https://pawarbi.github.io/blog/certification/2020/11/24/dp100-azure-data-scientist-certification-exam-resources-tips-preparation.html",
            "relUrl": "/certification/2020/11/24/dp100-azure-data-scientist-certification-exam-resources-tips-preparation.html",
            "date": " • Nov 24, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Intuitive Explanation of Penalized Regression",
            "content": "Video . . Previous Blog &amp; Video . Link .",
            "url": "https://pawarbi.github.io/blog/regression/explanation/ridge_lasso/2020/08/17/intuitive-penalized-regression.html",
            "relUrl": "/regression/explanation/ridge_lasso/2020/08/17/intuitive-penalized-regression.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Penalized Linear Regression in Azure ML Studio Designer",
            "content": "Video . You can read the summary below or watch the video . . Linear Regression . Linear regression is the simplest and the most basic form of regression. Many textbooks and videos cover it in detail. To predict an outcome y using features x1,x2,x3..., we take a weighted sum of all the features (plus an intercept term). The weight assigned to each feature is obtained by minimizing the sum of squared errors (SSE). The larger the weight, the higher it contributes to predicting y. The main advantages of using linear regression are that it is fairly easy to interprete, model and doesn&#39;t require lot of data. The drawbacks are that it requires features to be uncorrelated, features need to be numerical, model often underfits the data and it cannot be used for complex non-linear relationships. If the feautures are correlated they can reduce the accuracy of the predictions and results become less interpretable. . . The weights calculated using the min(SSE) criterion have high bias and low variance. We can fix this by adding more non-linear additive terms, thereby increasing the bias but lowering the variance. As the model becomes relatively more complex, the weights obtained for these features also increase and can lead to larger variance. To tackle this problem, a penalty term is added to SSE that penalizes the features that have large weights. If the penalty is 0, resulting model is just OLS but as the penalty increases, the weights have to shrink to satisfy min(SSE) criterion. As the weights become very large, they almost shrink to zero. If we can identify the right penalty, we can achieve lower variance sacrificing bias. Such model will generalize better and will be less susciptible to effects from multicollinear features. . The penalty can be assigned to the absolute sum of the weights (L1 norm) or sum of squared weights (L2 norm). Linear regression using L1 norm is called Lasso Regression and regression with L2 norm is called Ridge Regression. . Azure ML Studio offers Ridge regression with default penalty of 0.001. As mentioned above, if the penalty is small, it becomes OLS Linear Regression. This penalty can be adjusted to implement Ridge Regression. Typically the penalty is calculated using k-fold cross-validation for a range of penalty values either by random search or grid search. Unfortunately Azure ML Studio does not offer parameter sweep / grid search for tuning the L2 penalty in the Studio, so we will have to find the optimal penalty in the notebook and use it in the designer. Note that the penalty depends on the data, so if the data change significantly, the value will have to be re-calculated. As the penalty increases, bias increases but variance reduces (because model complexity reduces due to penalty on weights) and after an optimal value of the penalty, variance increases again. . . Import Libraries . #collapse-hide import pandas as pd from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, QuantileTransformer, Normalizer, PowerTransformer from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error from sklearn.model_selection import train_test_split, cross_validate, cross_val_score from sklearn.pipeline import make_pipeline, Pipeline from sklearn.compose import ColumnTransformer from statsmodels.tools.eval_measures import rmse from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) from nimbusml.linear_model import FastLinearRegressor . . #collapse-hide def corr_heatmap (df, target = &#39;Salary&#39;,threshold = 0.5 ,annot = False, absolute = False): &#39;&#39;&#39; Sandeep Pawar Create a triangular correlation plot and highlight strong correlations based on specified threshold import seaborn as sns &#39;&#39;&#39; plt.figure(figsize=(12,9)) mask = np.triu(np.ones_like(df.corr(), dtype=np.bool)) cmap = sns.diverging_palette(220, 20, n=10, sep = 80) if absolute == True: df = df.corr().abs().round(1).sort_values(by=target) else: df = df.corr().round(1).sort_values(by=target) sns.heatmap(df, annot=annot, cmap=cmap, mask=mask , vmax = threshold, center = 0, vmin = -threshold, square = True); . . Data . I am going to use data of baseball hitters. The data has stats for each player and their salary. The goal is to predict a player&#39;s salary using their stats. This data has 3 categorical features and 16 numerical features. I have dropped the player&#39;s name from the data. . path = &quot;https://gist.githubusercontent.com/keeganhines/59974f1ebef97bbaa44fb19143f90bad/raw/d9bcf657f97201394a59fffd801c44347eb7e28d/Hitters.csv&quot; df = (pd.read_csv(path).dropna().drop(&quot;Unnamed: 0&quot;,axis=1) ) categoricals = [&#39;League&#39;,&#39;Division&#39;,&#39;NewLeague&#39;] numericals = [&#39;AtBat&#39;, &#39;Hits&#39;, &#39;HmRun&#39;, &#39;Runs&#39;, &#39;RBI&#39;, &#39;Walks&#39;, &#39;Years&#39;, &#39;CAtBat&#39;, &#39;CHits&#39;, &#39;CHmRun&#39;, &#39;CRuns&#39;, &#39;CRBI&#39;, &#39;CWalks&#39;,&#39;PutOuts&#39;, &#39;Assists&#39;, &#39;Errors&#39; ] df[categoricals]=df[categoricals].astype(&#39;category&#39;) df.head(3) . AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague . 1 315 | 81 | 7 | 24 | 38 | 39 | 14 | 3449 | 835 | 69 | 321 | 414 | 375 | N | W | 632 | 43 | 10 | 475.0 | N | . 2 479 | 130 | 18 | 66 | 72 | 76 | 3 | 1624 | 457 | 63 | 224 | 266 | 263 | A | W | 880 | 82 | 14 | 480.0 | A | . 3 496 | 141 | 20 | 65 | 78 | 37 | 11 | 5628 | 1575 | 225 | 828 | 838 | 354 | N | E | 200 | 11 | 3 | 500.0 | N | . I won&#39;t spend much time here on EDA as the purpose is to show how to implement penalized regression in Azure mL Studio. But as the correlation matrix below shows, the features show very high collinearity with each other but moderate correlation with the target variable Salary. . I will include all the features in the model to show how Ridge Regression improves the results of Linear Regression despite highly correlated features. . corr_heatmap(df, threshold = 0.9, absolute=False, annot=True, target =&#39;Salary&#39;) . Above heatmap shows, many features are highly correlated with each other and have low-medium correlation with the target variable Salary. . #Creating train test split with 20% test X = df.drop(&#39;Salary&#39;, axis=1) y = df[&#39;Salary&#39;].values x1,x2,y1,y2 = train_test_split(X,y,test_size=0.2, random_state=123 ) print(&quot;Train Size:&quot;,len(x1)) print(&quot;RTest Size:&quot;,len(x2)) . Train Size: 210 RTest Size: 53 . Pipeline . Scikit-Learn . For linear regression, all the features need to be numericals and need to be scaled. While scaling is not a requirement, for Ridge Regression the features should be scaled to make sure feature scales don&#39;t influence the weights. Categorical features will be one-hot encoded and numerical features will be scaled using StandardScaler(). . PipelinePipeline(steps=[(&#39;Transform&#39;, ColumnTransformer(transformers=[(&#39;Categoricals&#39;, Pipeline(steps=[(&#39;OHE&#39;, OneHotEncoder())]), [&#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;]), (&#39;Numericals&#39;, Pipeline(steps=[(&#39;Scaler&#39;, StandardScaler())]), [&#39;AtBat&#39;, &#39;Hits&#39;, &#39;HmRun&#39;, &#39;Runs&#39;, &#39;RBI&#39;, &#39;Walks&#39;, &#39;Years&#39;, &#39;CAtBat&#39;, &#39;CHits&#39;, &#39;CHmRun&#39;, &#39;CRuns&#39;, &#39;CRBI&#39;, &#39;CWalks&#39;, &#39;PutOuts&#39;, &#39;Assists&#39;, &#39;Errors&#39;])])), (&#39;LinearRegression&#39;, LinearRegression())]) . Transform: ColumnTransformerColumnTransformer(transformers=[(&#39;Categoricals&#39;, Pipeline(steps=[(&#39;OHE&#39;, OneHotEncoder())]), [&#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;]), (&#39;Numericals&#39;, Pipeline(steps=[(&#39;Scaler&#39;, StandardScaler())]), [&#39;AtBat&#39;, &#39;Hits&#39;, &#39;HmRun&#39;, &#39;Runs&#39;, &#39;RBI&#39;, &#39;Walks&#39;, &#39;Years&#39;, &#39;CAtBat&#39;, &#39;CHits&#39;, &#39;CHmRun&#39;, &#39;CRuns&#39;, &#39;CRBI&#39;, &#39;CWalks&#39;, &#39;PutOuts&#39;, &#39;Assists&#39;, &#39;Errors&#39;])]) . Categoricals[&#39;League&#39;, &#39;Division&#39;, &#39;NewLeague&#39;] . OneHotEncoderOneHotEncoder() . Numericals[&#39;AtBat&#39;, &#39;Hits&#39;, &#39;HmRun&#39;, &#39;Runs&#39;, &#39;RBI&#39;, &#39;Walks&#39;, &#39;Years&#39;, &#39;CAtBat&#39;, &#39;CHits&#39;, &#39;CHmRun&#39;, &#39;CRuns&#39;, &#39;CRBI&#39;, &#39;CWalks&#39;, &#39;PutOuts&#39;, &#39;Assists&#39;, &#39;Errors&#39;] . StandardScalerStandardScaler() . LinearRegressionLinearRegression() . #Create a quick pipeline cat_ohe = OneHotEncoder() cat_steps = [(&#39;OHE&#39;, cat_ohe)] cat_pipe = Pipeline(cat_steps) ss=StandardScaler() num_steps = [(&#39;Scaler&#39;, ss)] num_pipe = Pipeline(num_steps) transformers = [(&#39;Categoricals&#39;, cat_pipe, categoricals), (&#39;Numericals&#39;, num_pipe, numericals)] ct = ColumnTransformer(transformers) x1_scaled = ct.fit_transform(x1) x2_scaled = ct.transform(x2) #Linear Regression lr = LinearRegression() steps = [(&#39;Transform&#39;, ct),(&#39;LinearRegression&#39;, lr)] pipe_final = Pipeline(steps) pipe_final.fit(x1,y1) print(&quot;OLS Train RMSE:&quot;,rmse(y1,pipe_final.predict(x1))) print(&quot;OLS Test RMSE:&quot;,rmse(y2,pipe_final.predict(x2))) print(&quot;&quot;) #Ridge Regression rr = Ridge(alpha=402) rr.fit(x1_scaled,y1) rr.predict(x2_scaled) print(&quot;Ridge Train RMSE:&quot;,rmse(y1,rr.predict(x1_scaled))) print(&quot;Ridge Test RMSE:&quot;,rmse(y2,rr.predict(x2_scaled))) print(&quot;&quot;) #Lasso Regression lasso = Lasso(alpha=15) lasso.fit(x1_scaled,y1) lasso.predict(x2_scaled) print(&quot;Lasso Train RMSE:&quot;,rmse(y1,lasso.predict(x1_scaled))) print(&quot;Lasso Test RMSE:&quot;,rmse(y2,lasso.predict(x2_scaled))) print(&quot;&quot;) #Decision Tree dt = DecisionTreeRegressor(random_state=123, max_depth=3, min_samples_leaf=0.10) dt.fit(X=x1_scaled,y=y1) dt.predict(x2_scaled) print(&quot;DT Train RMSE:&quot;,rmse(y1,dt.predict(x1_scaled))) print(&quot;DT Test RMSE:&quot;,rmse(y2,dt.predict(x2_scaled))) print(&quot;&quot;) #Random Forest rf = RandomForestRegressor() rf.fit(X=x1_scaled,y=y1) rf.predict(x2_scaled) print(&quot;RF Train RMSE:&quot;,rmse(y1,rf.predict(x1_scaled))) print(&quot;RF Test RMSE:&quot;,rmse(y2,rf.predict(x2_scaled))) . OLS Train RMSE: 300.6194521415407 OLS Test RMSE: 388.68624678093903 Ridge Train RMSE: 342.07177961040986 Ridge Test RMSE: 327.5827363090764 Lasso Train RMSE: 323.78659234667606 Lasso Test RMSE: 345.58566621397665 DT Train RMSE: 295.640673916212 DT Test RMSE: 248.6086248329667 RF Train RMSE: 109.41741807765071 RF Test RMSE: 235.4703983064576 . As the results show, with Linear Regression, RMSE is 300 and 388 on train &amp; test, resp. With Ridge Regression, I tuned the parameter to alpha=402 and obtained RMSE of 342/327. Thus using Ridge regression, the RMSE on training set increased (higher bias) but the RMSE on test set decreased (lower variance). In comparison, Lasso Regression was slightly worse than Ridge regression. Decision Tree and Random Forest were significantly better without tuning any parameters ! . . Tip: Always tune hyperparameters within CV. For simplification and demonstration purposes I calculated it on training set alone . Feature Weights . coef=pd.DataFrame() n = list(np.arange(len(rr.coef_))) for i in n: coef=coef.append({&quot;Feature&quot;:i, &quot;Linear Reg&quot;:lr.coef_[i], &quot;Ridge Reg&quot;:rr.coef_[i], &quot;Lasso Reg&quot;: lasso.coef_[i]},ignore_index=True) coef.set_index(&#39;Feature&#39;).sort_values(by=&#39;Ridge Reg&#39;, ascending=False).plot.bar(grid=False, figsize=(15,8)); . Lasso Reg Linear Reg Ridge Reg . Feature . 17.0 128.7 | -20.5 | 34.5 | . 16.0 126.7 | 219.5 | 34.5 | . 14.0 0.0 | 906.5 | 33.6 | . 19.0 75.3 | 84.7 | 32.2 | . 15.0 0.0 | 134.3 | 31.3 | . 13.0 0.0 | -804.1 | 28.0 | . 11.0 43.4 | 105.2 | 26.3 | . 7.0 45.0 | 68.6 | 22.0 | . 18.0 -0.0 | -95.5 | 21.6 | . 9.0 0.0 | -19.8 | 18.6 | . 10.0 0.0 | 35.3 | 16.9 | . 6.0 0.0 | -106.0 | 15.9 | . 2.0 63.1 | 57.4 | 13.5 | . 12.0 -17.3 | -85.9 | 10.6 | . 1.0 0.0 | 43.1 | 6.1 | . 5.0 0.0 | -14.4 | 5.4 | . 8.0 -0.0 | -37.4 | 5.1 | . 20.0 0.0 | 42.7 | 0.2 | . 21.0 -0.0 | -18.9 | -3.0 | . 4.0 -0.0 | 14.4 | -5.4 | . 0.0 -2.9 | -43.1 | -6.1 | . 3.0 -0.0 | -57.4 | -13.5 | . The chart and table above show weight on each feature used to train the model. In all cases, the weights have decreased with Ridge Regression compared to Linear Regression. That&#39;s the effect of using a high penalty. In comparison, Lasso regresion shrinks the weights to exactly zero for some of the features. Lasso has an advantage of feature selection. Out of the 21 features used, only 8 features have non-zero weights and are important for predicting the outcome! . Azure ML Studio . In Azure ML designer the L2 penalty did not produce same results despite using the same data and parameters. I suspected that this is because Azure ML Studio uses the FastLinearRegressor() instead of Ridge() from scikit-learn. So I tuned the parameters using FastLinearRegressor() from nimbusml. The optimal L2 was 0.04 to 0.06. This showed improvement over the OLS Regression in AzureML Studio. RMSE over test set dropped from 355 to 345. Not a significant drop but if linear regresison is needed, Ridge will generalize better than Linear Regression. . . OLS vs Ridge Results in Studio . Top are training and bottom are test scores . Linear . L2: 0.001 . Training RMSE: 299 . Test RMSE: 355 . . Ridge . L2: 0.05 . Training RMSE: 302 . Test RMSE: 345 . . My take-aways are : . Azure ML Studio offers Ridge Regression which can provide better results over Linear Regression when features have multi-collinearity | Azure ML Studio does not offer tuning or grid-searching the L2 penalty. This has to be done either by running multiple experiments in the Studio or in external notebook. | FastLinearRegressor() does not offer Lasso Regression directly but by adjusting the l1_threshold parameter we can run Lasso or ElasticNet Regression. This will achieve a compromise between Lasso and Ridge. Lasso or ElasticNet are not available in the Studio. | By default FastLinearRegressor() normalizes the data, it&#39;s not clear from the documentation if thats the default behaviour in Studio when L2 penalty is used. | The hyperparameters tuned in sklearn do not directly work in Studio. Be sure tune them in Studio. | .",
            "url": "https://pawarbi.github.io/blog/azureml/regression/penalized/ridge/lasso/azureml_studio/2020/07/12/penalized-regression-L2-azureml.html",
            "relUrl": "/azureml/regression/penalized/ridge/lasso/azureml_studio/2020/07/12/penalized-regression-L2-azureml.html",
            "date": " • Jul 12, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "Missing Value Analysis & Imputation in Azure ML Designer",
            "content": "Missing Values . Missing values in a dataset is one of the most common problems and cleaning/preprocessing the data can affect the data analysis, ML model predictions. Missing values can come in many forms, e.g. . Data are completely missing | Values appear as N/A, Null, -, &quot; . &quot; etc. Data Analyst/Engineer/Scientist will have to code these values as missing | In some cases 0 may indicate a valid value while 0 may also indicate missing value. In such cases, domain knowledge helps to identify missing values. | | . Missing Data Mechanism . Missing Completely At Random, MCAR: . There is no explanation as to why the data are missing and probability of missing is uniformly distributed. Missingness cannot be attributed to any other variables that are in the data or not included in the data. e.g. you have installed 100 sensors to measure temperature of a machine and few of the sensors ran out of battery or are just bad or not properly installed etc recorded no data for a brief amount of time. There is no systematic pattern to it and is just by luck. In this case, missingness is due to MCAR. Typically missingness by MCAR is few values. If missing data are large, then there is high probability that there are some variables causing the missingness. . Missing At Random, MAR: If we can attribute missing to other observed variables in the data, then the data are missing by MAR. For example, out of those 100 sensors, 25 were made by company A and usually the missingness is observed to be after 1000-1200 hours of installation when ambient temperature is between 90-110F. This indicates that the life of sensors by company A perhaps follow some weibull/exponential distribution dependent on duration and ambient temperature. Thus, there is high correlation between missing values and these factors (company, duartion and ambient temperature). This is more common and can be identified by exploratory data analysis. In general, it&#39;s good practice to assume the data are missing by MAR and try to identify the correlating factors. . Missing Not At Random, MNAR: If the data are missing neither by chance or by influence from observed data, then it is MNAR. This is the hardest mechanism to identify and address. This just means that missingness is caused by factors that are unknown and are external. Imagine some of the thermocouples in the example above are installed close to a vibrating part which vibrates excessively only when the load on the machine increases significantly, which causes the thermocouple wires to disconnect momentarily. Since there are multiple factors here that are not recorded, root cause is harder to pinpoint and can only be identified by experimentation and sensitivity (what-if analysis). While this may seem similar to MCAR, the difference is that MNRA has some probability of occurence (some pattern) while MCAR does not. . The mechanism of missingness informs which imputation strategy to use. . Missing Value Quantification &amp; Visualization . import statsmodels.api as sm import numpy as np air = sm.datasets.get_rdataset(&quot;airquality&quot;).data air.replace(&#39;NaN&#39;, np.nan, inplace=True) print(&quot;Missing_Values_count: &quot;, air.isna().sum()) print(&quot;Missing_Values_%: &quot;, (air.isna().mean() * 100).round(1)) . Missing_Values_count: Ozone 37 Solar.R 7 Wind 0 Temp 0 Month 0 Day 0 dtype: int64 Missing_Values_%: Ozone 24.2 Solar.R 4.6 Wind 0.0 Temp 0.0 Month 0.0 Day 0.0 dtype: float64 . missingno is a great library to visualize patterns in missing data . import missingno as miss miss.matrix(air); . Barplot . miss.bar(air); . Common Imputation Strategies . Listwise deletion : Delete all rows that have missing values. This is the easiest and often used strategy. This will will work only when the missingness is due to MCAR because the number of values are few (&lt;1%) and are unlikely to affect the accuracy as it produces unbiased estimates under MCAR assumption. The drawbacks are : . Size of observed data is reduced. This will affect non-linear models (e.g. tree based models) which benefit from more observations. Linear models (linear regression, logistic regression, SVM etc.) benefit from more features so may not be as much affacted. | If the data are not MCAR, this will bias the population estimates. | . In general if the missing data are few and can be attributed to MCAR, deletion is the most straightforward approach. . | Single Value Imputation: Replace the missing data with population estimates such as mean, median, mode, percentile etc. This is by far the most used method of imputation. If the feature is symmatrically distributes, either mean or median will work but if it is skewed, median should be used to replace values. Drawbacks are: . This is a problem if the data re MAR or MNAR as using a single value will definitely bias the estimates. Imagine in the example above, we replace the missing temperature with average temperature of 75F without giving regard to the ambient temperature. A non-linear model will capture this noise as a signal and will predict incorrect values. | It reduces the variability in the feature, and thus predictive power of that feature | ALWAYS split the data first, calculate the mean/median etc for the training set and use the SAME mean/median for the test/evaluation set. Apply scaling and other transformations after imputing. | . | Forward/Backward Fill/Interpolation: This is typically used in time series analysis when there is high autocorrelation in the data, i.e values are correlated to its past/future. We would either carry forward the last value to fill the missing value or calculate moving average (centrak or expanding window) and then fill the value. The main drawback is that it doesnt work with highly non-linear features or when the data have no autocorrelation. Interpolation will work by interpolating over the data. We can choose linear, quadratic, cubic etc. This will require some domain knowledge to choose the right interpolation method. | Indicator Variable: This is typically used for missing categorical data. This can be thought of as feature engineering. Replace the missing category with a category (e.g. &quot;missing&quot; or &quot;N/A&quot; etc.) and add another dummy column that indicates the category was missing. The ML algorithm will treat it as a feature and if you have large enough dataset, will learn the missingness pattern. This will work best for non-linear algorithms. | MICE ( Multiple Imputation By Chained Equation): MICE does multivariate regression analysis that is multiple times to converge at values. This is a very useful technique, especially when the data are missing by MAR. It is a good practice to compare summary statistics of the missing variable before and after applying MICE. MICE is a very robust imputation method. | KNN Imputation: K-nearest Neighbor can be used to find samples in the training set that are closest to the missing values and average the nearby points to predict the missing value. The purpose of imputation is not only to find possible true value but also to keep the original data structure. It&#39;s been shown that kNN imputation achieves both very efficiently and is a very effective technique Ref. | While sklearn/statsmodels/pandas offers all of the above imputation techniques, Azure ML does not. Azure ML Studio Classic had MICE and Probabilistic PCA but they are absent from the Designer. I discuss some of the above techniques in the video below. . To demonstrate some of the options avialable, I will use the following example table I created. This dataset . Index Missing_1 Missing_2 Missing_6 Misisng_8 Missing_Cat1 Missing_Ca2 Missing_2-2 . 0 1 | 320.0 | 249.0 | NaN | 224.0 | Gold | Gold | 26 | . 1 2 | 613.0 | 251.0 | NaN | NaN | Blue | Blue | 74 | . 2 3 | 260.0 | 294.0 | 590.0 | NaN | Silver | Silver | 174 | . 3 4 | 419.0 | NaN | NaN | NaN | Red | Red | 0 | . 4 5 | 485.0 | 415.0 | 386.0 | NaN | Blue | Blue | 488 | . 5 6 | NaN | 655.0 | NaN | NaN | Silver | NaN | 435 | . 6 7 | 61.0 | 37.0 | NaN | NaN | Red | Red | 588 | . 7 8 | 41.0 | NaN | NaN | NaN | Yellow | Yellow | - | . 8 9 | 607.0 | 254.0 | 369.0 | NaN | Blue | Blue | 363 | . 9 10 | 423.0 | 407.0 | 80.0 | 495.0 | Blue | Blue | 512 | . Update: I forgot to show in the video how single value imputation reduces variability. Consider the column Missing_2. With missing values, the standard deviation of the column is 178. But if we replace the 2 missing values with the mean of the column, 320, the standard deviation is 157. The more values we replace, the lower the variability. If the variance reduces significantly, it will not be an effective predictor of the outcome. Note that the mean remains the same after imputation. . print(&quot;Mean :,&quot;,data[&quot;Missing_2&quot;].mean()) print(&quot;Std :&quot;, data[&quot;Missing_2&quot;].std()) data[&quot;Missing_2_imp&quot;] = data[&quot;Missing_2&quot;].replace(np.nan,data[&quot;Missing_2&quot;].mean()) . Mean :, 320.25 Std : 178.7821899087586 . print(&quot;Mean_after:&quot;,data[&quot;Missing_2_imp&quot;].mean()) print(&quot;Std_after:&quot;,data[&quot;Missing_2_imp&quot;].std()) . Mean_after: 320.25 Std_after: 157.67107111536558 . data[[&quot;Missing_2&quot;,&quot;Missing_2_imp&quot;]].plot.box(); . Video . . References . https://stefvanbuuren.name/fimd/ |",
            "url": "https://pawarbi.github.io/blog/azureml/designer/missing_value/2020/07/12/missing_value-imputation.html",
            "relUrl": "/azureml/designer/missing_value/2020/07/12/missing_value-imputation.html",
            "date": " • Jul 12, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Creating Custom Summary Statistics Table in Power Query",
            "content": "Introduction . A Reddit user posted a question on r/PowerBI forum few days ago about &quot;... Table.Profile that respects slicers and deals with strings...&quot; I answered the question by noting that it would be a easier to do it with DAX using SWITCH(). But coincidentally, I had a similar requirement for a project where the user wanted to get summary statistics for each numerical column and filterable by a slicer. The summary statistics in my case also included some custom functions. I could have created calculated columns, but Power Query offers a faster &amp; memory-efficient solution. The reddit user had question about strings but I am going to focus on numerical columns only for simplicity. This is a quick way to get summary statistics for exploratory data analysis or checking data quality. . Table.Profile() . As this user mentioned in the question, the easiest and fastest way to get descriptive statistics is to wrap the table with Table.Profile(). Read the documentation here. By default, Table.Profile() returns min, max, average, standard deviation, count, null count, distinct count. But you can add more aggregates or custom functions by specifying the second parameter additionalAggregates as nullable list. Lars Schreiber has an excellent blog post on using that second parameter. . In the example below, I added median and a custom function to count number of outliers in each column. This second parameter can be specified as ColumnName . Custom Function to Count Outliers . #hide_output (mylist as list, optional limit as number) =&gt; let // Function to count number of outlier value based on how far it is from the average. // By default values that are 2 standard deviations away form the mean are counted as outliers // Sandeep Pawar // PawarBI.com // Date: 7/2/2020 mean = List.Average(mylist), sd = List.StandardDeviation(mylist), limit = 2, outliers = List.Select( mylist, each _ &gt; (mean + limit*sd)), outcount = List.Count(outliers) in outcount . To use this custom function fxoutlier(list,limit) in Table.Profile(), I can write: . #hide_output Table.Profile(TableName, {{&quot;Outliers&quot;, each Type.Is(_, nullable number), each fxoutlier(_,null)}}) . Video . .",
            "url": "https://pawarbi.github.io/blog/powerbi/powerquery/statistics/transformation/table.profile/2020/07/01/powerquery-descriptive-statistics.html",
            "relUrl": "/powerbi/powerquery/statistics/transformation/table.profile/2020/07/01/powerquery-descriptive-statistics.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Exporting Data From Power BI to SQL Server Using Python",
            "content": "Exporting to SQL . Watch the video below for exporting data from Power Query to SQL Server. Below are the links to the blogs and packages mentioned in the video: . Exporting to SQL server using R | Exporting a CSV using Python | SQL Server Management Studio | SQLAlchemy | Recommendations for Using Python/R in Power BI | Writing M Custom Function Documentation | . Python Code . Install Python, Sqlalchemy | Setup Python in Power BI | Temporarily set the data privacy to Public. Be sure to change it back to the original privacy level | Delete the export query after you are done exprting to SQL server | . #hide_output import sqlalchemy from sqlalchemy import create_engine, MetaData, Table, select from six.moves import urllib params = urllib.parse.quote_plus(&#39;DRIVER=ODBC Driver 17 for SQL Server;SERVER=localhost;DATABASE=pbi;trusted_connection=yes&#39; ) engine = sqlalchemy.create_engine(&#39;mssql+pyodbc:///?odbc_connect=%s&#39; % params, echo=False) connection = engine.raw_connection() dataset.to_sql(name=&#39;from_pbi2&#39;, con=engine, index=False, if_exists=&#39;append&#39;) // or &#39;replace&#39; . PowerQuery Custom Function . Copy everything between the dotted lines ( ) and paste it in the &#39;Advanced Editor&#39;. . #hide_output &quot;&quot;&quot; Start Copy-- let fn = (source as table, Driver as text, database as text, table as text, server as text, if_exists as text) as table =&gt; let Source = source, // Script to Export Power BI to SQL database // Author: Sandeep Pawar // Visit www.pawarbi.com for details and // Version: 0.1 // Date: Jun 23,2020 Run_Python = Python.Execute(&quot;# &#39;dataset&#39; holds the input data for this script#(lf)# install pyodbc , sqlalchemy#(lf)import sqlalchemy#(lf)#(lf)from sqlalchemy import create_engine, MetaData, Table, select#(lf)#(lf)from six.moves import urllib#(lf)#(lf)# change SERVER name with your server name &amp; database name with your database name#(lf)# For Driver, search odbc in Windows &gt; Driver Use the driver installed there#(lf)params = urllib.parse.quote_plus(&quot;&quot;DRIVER=&quot;&amp;Text.From(Driver)&amp;&quot;;SERVER=&quot;&amp;Text.From(server)&amp;&quot;;DATABASE=&quot;&amp;Text.From(database)&amp;&quot;;trusted_connection=yes&quot;&quot;)#(lf)#(lf)engine = sqlalchemy.create_engine(&quot;&quot;mssql+pyodbc:///?odbc_connect=%s&quot;&quot; % params, echo=False) #(lf)#(lf)connection = engine.raw_connection()#(lf)#(lf)#engine.connect() #(lf)# suppose df is the data-frame that we want to insert in database#(lf)#(lf)#if_exists: Use replace if you want to replace an existing table with named &quot;&quot;from_pbi&quot;&quot; You can use any table name#(lf)dataset.to_sql(name=&quot;&amp;Text.From(table)&amp;&quot;,con=engine, index=False, if_exists=&quot;&amp;Text.From(if_exists)&amp;&quot;)&quot;,[dataset=Source]) in Run_Python, documentation = [ Documentation.Name = &quot; Export to SQL&quot;, Documentation.Description = &quot; Power BI to SQL export Using Python &quot;, Documentation.Category = &quot; Table &quot;, Documentation.Source = &quot;www.PawarBI.com &quot;, Documentation.Author = &quot; Sandeep Pawar PawarBI.com &quot;, Documentation.Examples = {[Description = &quot;Use this function to export a dataset imported in Power Query to a SQL Server database. You will need to install Python, Pandas, pyodbc and sqlalchemy for this code to work. If you get firewall error, set the data privacy to public for the purposes of exporting the data and then you can revert it back to chosen data privacy setting. Visit www.pawarbi.com or my youtube channel &#39;PawarBI&#39; for details&quot;, Code = &quot;Enter SQL Driver: SQL Server Driver 17, database: pbi, table: &#39;from_pbi&#39;, if_exists: &#39;replace&#39; or &#39;append&#39; &quot;, Result = &quot; The script will export the data to the specified SQL server, under &#39;pbi&#39; database in table named &#39;from_pbi&#39; &quot;]}] in Value.ReplaceType(fn, Value.ReplaceMetadata(Value.Type(fn), documentation)) End Copy-- &quot;&quot;&quot; . .",
            "url": "https://pawarbi.github.io/blog/powerbi/python/sql/export/2020/06/23/export-powerbi-sql-python.html",
            "relUrl": "/powerbi/python/sql/export/2020/06/23/export-powerbi-sql-python.html",
            "date": " • Jun 23, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Handy Keyboard Shortcuts in Power BI",
            "content": "Shortcuts . I am sure there are many more, but these are the ones I use frequently. . Alt + Drag to toggle snap-on off and move objects freely | Up/Down arrows to move objects in small steps &amp; SHIFT+ Arrow to move them large steps | Ctrl + Shift + Arrow to make shapes bigger/smaller | Shift + Resize to keep the aspect ratio | Ctrl + G to group and Ctrl + Shift + G to ungroup. Align, Distribute and Group and then resize to scale all the shapes/cards at the same time | Ctrl + Arrow to quickly cycle through visuals/shapes on the canvas | .",
            "url": "https://pawarbi.github.io/blog/powerbi/2020/06/17/powerbi-keyboard-shortcuts.html",
            "relUrl": "/powerbi/2020/06/17/powerbi-keyboard-shortcuts.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Random Number Generation in Power BI",
            "content": "Random Number Generation . A quick post on creating random numbers in Power BI. While it&#39;s very easy to use, there are couple of things to keep in mind to get correct results. Typically the random numbers aren&#39;t needed in Power BI reports but I use them very often for simulations, statsitical functions and synthetic data generation. . What are random numbers? . As series of values is said to contain random numbers if all the values have equal probability of occurence and there is no correlation, i.e. the numbers are uniformly distributed. . How to generate random numbers? . In DAX, you can use RAND() function to generate random numbers. Similarly, to generate random numbers between two numbers, use RANDBETWEEN(). . In Power Query, List.Random() creates a list of random numbers, while Number.Random() creates a single random number. You can add an index column before using the Number.Random() to get different random numbers on each row. You can delete the index column after the Number.Random() step. There isn&#39;t a List.RandomBetween() but Number.RandomBetween() can be used along with index column to generate numbers between two specified values. . Tip 1: DAX and M RandomBetween behave differently . If you generate random numbers between (1,100) using DAX RANDOMBETWEEN(1,100), it will generate integer/whole numbers between (1,100) and will include numbers 1 and 100. In constrast to that, in Power Query Number.RandomBetween(1,100) will generate decimal numbers between 1 and 100 and will exclude 100. Thus, if you are using M and want to include 100, use Number.RandomBetween(1,101). . To generate integer random values using M, first add an index column and then create a custom column using Int16.From(Number.RandomBetween(1,100)). This includes numbers 1 and 100. . Tip 2: Multiple Random numbers in Power Query . To create multiple columns containing random numbers, add an index column each time before using the Number.Random(). You can delete the columns later. . You can alse wrap the step before the Number.Random() step with Table.Buffer() to generate different number on each row. For example, if you added a custom column in the step previous to adding random number column, your step would look like Table.Buffer(Table.AddColumn(...)). In this case you will not need to add an index column. If you need to add multiple random number columns, you can buffer the previous step each time. But be careful of the performance and transformation implications of Table.Buffer(). Read more abou it here. Thanks to Gil Raviv for this tip.. . Tip 3: You can use RAND() to create decimal random numbers between two numbers . As RANDBETWEEN() creates integers, you can use RAND()*N to create decimal numbers between (0,N). In this case, the numbers will not include 0 and N. . Similarly, to generate decimal random numbers between (N1,N2) use N1 + RAND() *(N2-N1). N1 &amp; N2 are not included. . You can divide RANDBETWEEN() by 10^i, to create numbers with &#39;i&#39; decimal digits . Tip 4: IMPORTANT Be careful with using ROUND() with RAND() . As mentioned above, all random numbers should have equal probability of occurence otherwise it violates the uniform distribution assumption. If you use ROUND() with RAND() ( e.g.ROUND(RAND()*50,0)) to generate integers between 0 and 50, the end numbers will have half the probability of occurence compared to rest of the numbers. This is because numbers greater than or equal to 0.5 will be rounded to the next integer. e.g. 49.25 will be rounded down to 49 whereas 49.68 will be rounded up to 50. Thus, the end values have half the probability of occurence. The correct way is to use INT() i.e INT(RAND()*50). While the error resulting from this will be minimal, it&#39;s better to use the correct formula when creating synthetic data to avoid bias. . Tip 5: DAX &amp; M do not allow random seed . In Python and R, you can specify the seed number to generate reproducible random numbers. Both in DAX &amp; M do not allow seed, so with each refresh new random numbers are generated. In many cases, this is not desirable. A work around for that is to generate random numbers in Power Query using M, indexing those and disabling the refresh for that query. . Create an index column and random number column using . Table.FromColumns( { {1..n_samples}, List.Random(n_samples)}, {&quot;Index&quot;, &quot;NRandom&quot;} ) . where n_samples &gt;&gt;&gt; number of random values you need. For example, if you are going to need 3 sets of random numbers 1000 each, use n_samples &gt;&gt; 3000 ,say 5000. . | Uncheck &quot;Include in report refresh&quot; for that query | Now you can use this list by using FILETR() in DAX by passing Table[Index] &lt; 1001 and so forth | . Tip 6: Randomly subsample the data by using SAMPLE() in DAX . I use this often to check robsutness/sensitivity of the simulation results with respect to the features used. For example, some columns may show correlaton or importance when used entirely but when sub-sampled may or may not. . To sample a column, use SAMPLE(sample_size, &#39;Table&#39;, &#39;Table&#39;[Column]) . Ruth provides a good overview of the SAMPLE() function below: . Tip 7: Randomly generate letters or sequence of letters . You can use UNICHAR() and RANDBETWEEN() to generate random letters . To generate upper case latters: UNICHAR(RANDBETWEEN(65,90) . To generate lower case latters: UNICHAR(RANDBETWEEN(97,122) . To generate upper &amp; lower case latters: UNICHAR(RANDBETWEEN(97,122) &amp; UNICHAR(RANDBETWEEN(65,90) . In addition you can nest them with RANDBETWEEN() to generate a random words, passwords, symbols etc. For a list of unicode chart, see this . Similarly, to generate characters using M, first create an index column and then use: . Character.FromNumber(Int16.From(Number.RandomBetween(65,90))) for upper case letters. Thanks to Alex Powers for the tip. . Tip 8: Changing column type to decimal . When you create the custom column with random numbers using Number.Random() in Power Query, it will be mixed type (123/ABC). If you convert it to Decimal, randomness is lost and all the numbers in the column will be the same. Solution to that is just add another index column ! You can always delete it later. . Tip 9: Limit the use of calculated tables and columns with RAND() and RANDBETWEEN() . The DAX Maestro Marco Russo has a blog on &quot;volatile&quot; DAX functions. These are the DAX functions that return differrent values when the model is refreshed. Using too many of these volatile functions in the model can cause slower refreshes. Use the RAND() AND RANDBETWEEN() but use them judicously and be aware of the potential performance impact. . Tip 10: Generating random dates . To generate random dates, we can convert the random numbers generated by RAND() AND RANDBETWEEN() to date. For example, to generate random dates between 2000 and 2030, we can use CONVERT(RANDBETWEEN(36526,45658), DATETIME). Note that in DAX 0 is 12/30/1899 so just calculate accordingly. In Power Query, the equivalent would be Date.From (Number.RandomBetween(36526,45658)). .",
            "url": "https://pawarbi.github.io/blog/powerbi/dax/m/2020/06/03/powerbi-random-numbers.html",
            "relUrl": "/powerbi/dax/m/2020/06/03/powerbi-random-numbers.html",
            "date": " • Jun 3, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "Recommendations For Using Python/R in Power BI",
            "content": "Overview . Using Python and R in Power BI, when used correctly, can allow quick experimentation, data/calculation validation, statistical analysis/inferencing of the data. However, before you use Python/R in Power BI keep the performance and data privacy issues in mind. I personally use Python in my report development, model development process but use it as a tool for experimentation rather than replacement to native Power BI tools. Below are my recommendations / tips: . Always read the documentation Python and R for known limitations, official guidelines | . Uncheck “Enhanced meta-data” if you are using Python. Power BI released it as a preview feature in March 2020. Unfortunately, I learned it the hard way. If you use Python in Power Query with this option selected and save the file, it will currupt your file and you cannot recover it back (well, not entirely). Enhanced metadata allows read/write XMLA (Premium feature). This is a serious bug that MS needs to fix. If you need to use this option, use R. | . . Keep the data privacy implications in mind. Data Privacy must be set to &quot;Public&quot; . While that does not mean the report/dashboard/data are made public, do not use Python /R if you are working with sensitive data. When the data privacy is set to &quot;Public&quot;, Microsoft stores it in less restrictive storage but the data is safe. | . Always create and use virtual environment ,e.g. use virtualenv/pipenv, conda. Conda may not always work if multiple Python copies are installed and is a known issue. More on Python virtual enviroment here. Use the directory of the virtual environment as Python home directory. | . . Only install the packages you need. Power BI needs pandas and matplotlib installed at minimum. I personally only use Pandas, Numpy, Statsmodels, Scipy, Scikit-learn, Matplotlib, Seaborn in my Power BI virtual environment. If you find yourself needing more packages, you should consider using the IDE and deploying Python/R outside of Power BI. | . Only import the packages you need, e.g. from scipy.stats import lognorm instead of from scipy import * to the reduce execution time | . By default, Power BI loads pandas, matplotlib and os, no need to re-import it (import pandas as pd). If you do need to alias the package, create a variable, pd=pandas and then you can use pd.read_csv(..) | . Change date column to text before running Python and change it back to Date format after running the script. If you keep it as Date, all Date values will be returned as &quot;Microsoft.OldDb.Date&quot;. You can also change the column type to Date/Time and parse it back but it&#39;s esier to convert text. | . Duplicate the original query and apply Python /R on duplicated query, instead of the original. | . Apply Python / R as the last step or when the rows have been filtered, unnecessary columns have been removed to reduce the *.csv file Power BI creates. The temporary csv file must be &lt;250 MB, so it&#39;s best to reduce the data to keep the file size down. | . Use only one Python / R script in a query, if you must use Python / R. | . Always run Query Diagnostics to make sure Python/R script isn’t the bottleneck, query execution time is &lt;&lt; 5min. Query will time out if the execution time is &gt;30 minutes. Run the script in your IDE to time the execution. If you are using Python, vectorize the data to improve efficiency, avoid for loops, use apply(), avoid intermediate dataframes. | . Power Query is powerful and preferred, so don&#39;t use Python / R as a replacement. Use it for quick experimentation, data validation, spot/sanity checking, calculation validation, statistical analysis only. I use it often in my workflow for finding patterns, anamolies, outliers, correlation etc. using Power BI + Jupyter (blog to come. ). I have intentionally left out Machine Learning here. I do not think an ML model should be deployed as a Python / R script because of the limitations and it&#39;s a bad deployment practice in general. ML models need to be revised, monitored for performance, data drift etc and Power BI does not allow that. Do not deploy ML models in Power Query | . Use Power Query Parameter in Python /R if you need to reuse a script or need a quick way to change arguments,params. You can pass &amp;Text.From([param]) or &amp;Number.From([param]) in any Python / R script to make it more dynamic. | . Apply correct summarization for visuals (No summarization, sum, average etc.), always add an index/unique key to make sure rows are not removed. | . Create Python/R visuals after you have finalized column/measure names. Renaming column/measure will throw an error . | Use native Power BI visuals for production reports and Python/R visuals for getting more insights and for experimentation. Power/R visuals are very slow to refresh and do not allow cross-filtering, highlighting. . | If you do use Python/R visual in production report, run the Performance Analyzer to check execution &amp; refresh time. If it&#39;s too slow, consider using native visuals or build one in charticulator. I rarely use custom visuals to avoid dependency and data security reasons. . | . David Eldersveld provides an excellent overview in his video and blog . Reference: . https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-python-scripts | https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-python-in-query-editor | https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-r-scripts | https://dataveld.com/2018/11/10/5-performance-tips-for-r-and-python-scripts-in-power-bi/ |",
            "url": "https://pawarbi.github.io/blog/powerbi/r/python/2020/05/15/powerbi-python-r-tips.html",
            "relUrl": "/powerbi/r/python/2020/05/15/powerbi-python-r-tips.html",
            "date": " • May 15, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Time series Forecasting in Power BI",
            "content": "Overview . In Part 1 I covered the exploratory data analysis of a time series using Python &amp; R and in Part 2 I created various forecasting models, explained their differences and finally talked about forecast uncertainty. In this post, I hope to provide a definitive guide to forecasting in Power BI. I wanted to write about this because forecasting is critical for any business and the documentation on this is very thin, and it&#39;s easy to (mis)use it. . If you do not have experience in forecasting, I would encourage you to read the above two blogs to learn more about forecasting in general. . I will use the same dataset I used in the earlier blog posts and compare the Power BI forecast with the Python models I created. . How to create a forecast in PowerBI? . It&#39;s very easy! Parker Stevens gives a nice overview of it in the clip below. . . To create a forecast you need: . A continuous date column | A numerical column with the numbers you want to forecast . Drag and drop the dates in &quot;Axis&quot; field | Drag and drop the numbers in the &#39;Values&#39; field | Click on the down arrow in the &#39;Date&#39; field and apply the required hierarchy (month, quarter, week etc.) or remove &#39;Date Hierarchy&#39; if you do not want hierarchy. If removed, it will plot the data for each date rather than the hierarchy. | In the Format options, make sure the X Axis type is &#39;Continuous&#39; | Go to &#39;Analytics&#39; pane, Forecast &gt; +Add &gt; Enter the Forecast Length | | . That&#39;s it ! We have a forecast. You can hover over the line chart to get the forecast predictions along with confidence interval. Very easy. . But how do we know: . if the forecast is accurate | What model(s) was used to create the forecast? | what assumptions were made to make the forecast? | what&#39;s the forecast uncertainty? | how do we display the forecasts? | what are the limitations? | can we improve the forecast? | when is it appropriate or no appropriate to use it? | . Let&#39;s first take a look at the documentation from the Power BI team and see if we can aswer some of these questions. . How does Power BI create the forecast? . I found couple of official blog posts on Power BI&#39;s website that were written in 2014 and 2016. The blog written in 2014 was for Power View which has been deprecated but the post still shows up under Power BI&#39;s blog. Other than that, I couldn&#39;t find anything. Given the lack of information, I will assume these posts still describe the current forecasting procedure in Power BI. I will use Python to follw the same procedure and see if we can understand it better. . Which forecasting model is used? . According to this blog, Power BI uses the ETS(AAA) and ETS(AAN) models to model seasonal and non-seasonal data, respectively. I used and described these models in Part 2. But here is a quick non-mathematical recap: . ETS stands for Error, Trend, Seasonality. It is an exponential smoothing model which gives exponential weightage to the historical data to predict the future values. | The data is first decomposed into level, trend, and seasonality. Error is obtained by subtracting the level, trend and and seasonality from the actual values. | Level is the average value over the observed period | Trend is the change in level over this period | Seasonality is the behaviour of the observed quantity from one season to next. e.g. if you have a monthly data for toy sales, sales would be up around holidays from Nov-Jan and then down in the following months. | It&#39;s called exponential smoothing because exponential weights are applied to successive historical values. | Trend, Seasonality and the Error terms can be combined in additive, multiplicative or mixed fashion. . Additive = (Level+Trend) + Seasonality + Error . | . Multiplicative = (Level * Trend) * Seasonality * Error . In addition, the Trend component can be &quot;Damped&quot;. i.e. we &#39;derate&#39; the growth of the trend | . The ETS models follow ETS(XYZ) nomenclature: . X: Error Term. It can be Additive (A), Multiplicative (M) . | Y: Trend Term. It can Additive (A), Multiplicative (M) or Damped (Ad), or No trend (N) . | Z: Seasonality. Additive (A) or Multiplicative(M), or No seasonality (N) . | . | . As the illustration below shows, if the trend is linear, it&#39;s &quot;additive&quot;. If it shows exponential growth, multiplicative model would fit better. | Similarly, if the quantity being studied varies in a stable way relative to average levels, seasonality can be modeled as &quot;additive&quot;. If the change from one season to next, relative to average, can be expressed in terms of % of average, then &quot;multiplicative&quot; model would be a better fit. Read Part 2 to get better understanding of this. There are 36 possible combinations depending on A,M,Ad,N. Only some of these are practical. . | Out of 15 or so possible practical ETS models, Power BI uses two models ETS(AAN), ETS(AAA). You can read more about ETS here . | In general, ETS often provides the most accurate forecasts, even better than the complex neural network models Ref | . . ETS(AAN) . This is &quot;Additive&quot; error, &quot;Additive&quot; trend and &quot;No&quot; seasonality model used when there is no seasonality in the time series. If you are familiar with Holt-Winter&#39;s exponential models, this is Holt&#39;s linear model. | You would use this model when you see a linear trend in the data and no seasonality pattern. | If you are familiar with ARIMA models, this is equivalent to ARIMA(0,2,2) | This is not single exponential smoothing (SES). SES (i.e ETS(ANN)) is used when the data doesn&#39;t have any trend either. | See an example below. It shows Google&#39;s stock performance over time. It has a positive trend but no seasonality. The forecast with an ETS (AAN) model is just a straight line that extends the trend into the future. . . | . ETS(AAA) . This model should be used when there is linear trend and seasonality. | This is the Holt-Winter&#39;s triple exponential smoothing model | See the example below. This data shows quarterly milk production. It has an obvious positive, linear trend and seasonality. Notice how the production jumps up in certain quarters of the year and then drops in following seasons. the height of the peaks in seasons remains same relative to the trend. &quot;Additive&quot; seasonality shows this behaviour. | . . The blog from 2016 written by MS employee mentions that &quot;.. Power BI uses assembly forecaster..Power BI picks the algorithm..forecaster includes exponential smoothing ..&quot;. Not entirely sure if that means if in addition to above ETS models, Power BI uses any other models. . Another possibility is that I know Microsoft uses nimbusml python/mx.net module in its various products including Power BI for machine learning. NimbusML has ssaForecaster() class which uses Single Spectrum Analysis for forecasting. It&#39;s a powerful forecating method but hasn&#39;t been used widely in the industry because of some limitations. . The 2014 blog also mentions that Power View (now in Power BI) does not use ARTXP or ARIMA methods. ARXTP is Autoregressive tree model with cross-prediction. I have no clue what that is. I have discussed ARIMA in part 2. . How does Power BI create the forecast? . Fortunately the 2014 blog sheds some light on how Power BI creates the forecast. To get the better understanding of the methodology, I am going to try to recreate it in Python. . Data: . I am using the same dataset I used in the previous blogs. This data shows quarterly sales of a French retailer. I have divided the numbers by 1,000 to make them easier to read. We have data for 24 quarters and the goal is to forecast sales for the next 4 quarters and total sales for FY2018. I am going to load the dataset in python. . As the plot below shows we have a clear seasonal data with positive trend. So we should be able to use the forecast tool in Power BI. . #collapse_hide #Author: Sandeep Pawar #Version: 1.0 #Date Mar 27, 2020 import pandas as pd import numpy as np import itertools #Plotting libraries import matplotlib.pyplot as plt import seaborn as sns import altair as alt plt.style.use(&#39;seaborn-white&#39;) pd.plotting.register_matplotlib_converters() %matplotlib inline #statistics libraries import statsmodels.api as sm import scipy from scipy.stats import anderson from statsmodels.tools.eval_measures import rmse from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import month_plot, seasonal_plot, plot_acf, plot_pacf, quarter_plot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing from statsmodels.stats.diagnostic import acorr_ljungbox as ljung from statsmodels.tsa.statespace.tools import diff as diff from statsmodels.tsa.statespace.sarimax import SARIMAX import pmdarima as pm from pmdarima import ARIMA, auto_arima from scipy import signal from scipy.stats import shapiro from scipy.stats import boxcox from scipy.special import inv_boxcox from sklearn.preprocessing import StandardScaler from scipy.stats import jarque_bera as jb from itertools import combinations import fbprophet as Prophet #library to use R in Python import rpy2 from rpy2.robjects import pandas2ri pandas2ri.activate() %load_ext rpy2.ipython import warnings warnings.filterwarnings(&quot;ignore&quot;) np.random.seed(786) pd.plotting.register_matplotlib_converters() def MAPE(y_true, y_pred): &quot;&quot;&quot; %Error compares true value with predicted value. Lower the better. Use this along with rmse(). If the series has outliers, compare/select model using MAPE instead of rmse() &quot;&quot;&quot; y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 def residcheck(residuals, lags): &quot;&quot;&quot; Function to check if the residuals are white noise. Ideally the residuals should be uncorrelated, zero mean, constant variance and normally distributed. First two are must, while last two are good to have. If the first two are not met, we have not fully captured the information from the data for prediction. Consider different model and/or add exogenous variable. If Ljung Box test shows p&gt; 0.05, the residuals as a group are white noise. Some lags might still be significant. Lags should be min(2*seasonal_period, T/5) plots from: https://tomaugspurger.github.io/modern-7-timeseries.html &quot;&quot;&quot; resid_mean = np.mean(residuals) lj_p_val = np.mean(ljung(x=residuals, lags=lags)[1]) norm_p_val = jb(residuals)[1] adfuller_p = adfuller(residuals)[1] fig = plt.figure(figsize=(10,8)) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2); acf_ax = plt.subplot2grid(layout, (1, 0)); kde_ax = plt.subplot2grid(layout, (1, 1)); residuals.plot(ax=ts_ax) plot_acf(residuals, lags=lags, ax=acf_ax); sns.kdeplot(residuals); #[ax.set_xlim(1.5) for ax in [acf_ax, kde_ax]] sns.despine() plt.tight_layout(); print(&quot;** Mean of the residuals: &quot;, np.around(resid_mean,2)) print(&quot; n** Ljung Box Test, p-value:&quot;, np.around(lj_p_val,3), &quot;(&gt;0.05, Uncorrelated)&quot; if (lj_p_val &gt; 0.05) else &quot;(&lt;0.05, Correlated)&quot;) print(&quot; n** Jarque Bera Normality Test, p_value:&quot;, np.around(norm_p_val,3), &quot;(&gt;0.05, Normal)&quot; if (norm_p_val&gt;0.05) else &quot;(&lt;0.05, Not-normal)&quot;) print(&quot; n** AD Fuller, p_value:&quot;, np.around(adfuller_p,3), &quot;(&gt;0.05, Non-stationary)&quot; if (adfuller_p &gt; 0.05) else &quot;(&lt;0.05, Stationary)&quot;) return ts_ax, acf_ax, kde_ax def accuracy(y1,y2): accuracy_df=pd.DataFrame() rms_error = np.round(rmse(y1, y2),1) map_error = np.round(np.mean(np.abs((np.array(y1) - np.array(y2)) / np.array(y1))) * 100,1) accuracy_df=accuracy_df.append({&quot;RMSE&quot;:rms_error, &quot;%MAPE&quot;: map_error}, ignore_index=True) return accuracy_df def plot_pgram(series,diff_order): &quot;&quot;&quot; This function plots thd Power Spectral Density of a de-trended series. PSD should also be calculated for a de-trended time series. Enter the order of differencing needed Output is a plot with PSD on Y and Time period on X axis Series: Pandas time series or np array differencing_order: int. Typically 1 &quot;&quot;&quot; #from scipy import signal de_trended = series.diff(diff_order).dropna() f, fx = signal.periodogram(de_trended) freq=f.reshape(len(f),1) #reshape the array to a column psd = fx.reshape(len(f),1) # plt.figure(figsize=(5, 4) plt.plot(1/freq, psd ) plt.title(&quot;Periodogram&quot;) plt.xlabel(&quot;Time Period&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.tight_layout() path = &#39;https://raw.githubusercontent.com/pawarbi/datasets/master/timeseries/ts_frenchretail.csv&#39; #Sales numbers are in thousands, so I am dividing by 1000 to make it easier to work with numbers, especially squared errors data = pd.read_csv(path, parse_dates=True, index_col=&quot;Date&quot;).div(1_000) data.index.freq=&#39;Q&#39; data.head() train = data.iloc[:-6] test = data.iloc[-6:] train_log = np.log(train[&quot;Sales&quot;]) data.head(10) . . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . 2013-06-30 409.0 | . 2013-09-30 498.0 | . 2013-12-31 387.0 | . 2014-03-31 473.0 | . 2014-06-30 513.0 | . Let&#39;s create the forecast for this data in Power BI first. . Power BI Forecast . . Observations: . Power BI was able to capture the trend and seasonality very well. | I left the &quot;Seasonality&quot; field blank and Power BI still detected quarterly seasonality. | Power BI shows the 95% &quot;Confidence Interval&quot; as gray band by default | You can inspect the forecast by selcting &quot;Show as table&quot; and can also &quot;Export data&quot; | Power BI does not show the forecast values. You have to hover over the line chart to know the values. | Forecast values are not accessible for further calculations or use in measures/calculated columns. | Now let&#39;s see how Power BI creates the forecast as described in the blog. . Creating Validation Window . First a validation window is created. Here is what the blog says: . ...The classical Holt-Winters method finds the optimal smoothing parameters by minimizing the mean sum of squares of errors for predictions in the training window, looking only at predictions that are one-step ahead. However, the errors you get from looking just one step ahead might not be representative of the errors you get when you want a longer horizon forecast. Therefore, to improve long-range forecasting error, we introduced a validation window, which contains the last few points of the training window. Within this validation window, we do not adjust the state at each and every step, but instead, compute the sum of squares of prediction errors for the window as a whole. This has the effect of dampening variation and preserving trend across the validation window... . What this means: In classical forecasting, the parameters of the model are optimized by using all the given data, making forecasts one step at a time and then minimizing the mean sum of square errors (SSE). SSE is calculated by subtracting the 1-step forecasts from the actual values and then squaring them. Errors could be positive or negative so if we add them as is, they may add upto 0. Squaring solves that problem. The issue with this approach is we typically want a long term forecast and using the above approach we cannot assess the accuracy of the forecast for long horizon. To overcome this, in a machine learning model, the data is split into training and test (i.e validation) and model parameters are obtained by using only the training set. The test set is used to assess the accuracy of the model fit. This also helps prevent overfitting (good fit on the training set and poor on the test). . I will split the data into train and test. I will use the first 18 values for training (blue line) and the last 6 for validation (orange line). . Preprocessing the data . It is important that the preprocessing is done after the train/test split and the same preprocessing parameters/steps are applied on both the sets to prevent data leakage. . 1. Missing values: If Power BI detects that any missing values, it will automatically fill in the values using linear interpolation, i.e taking mean of the values before and after the missing value. Power BI performs the imputation as long as the missing values are fewer than 40% of the total observations. . This is a reasonable approach but could potentially be a problem if: . the missing values cannot be imputed with interpolation | missing values/nulls are actually 0&#39;s indicating no sales/production etc. (intermittent time series) | . You should check the data for missingness before doing the forecasting. Also if the data (trend) is not linear, interpolation will result in erroneous data. . To test, I intentionally deleted some of the observations and Power BI still performed the forecasting but did not show the interpolated values in the table. It would be good to know what values were used. If the missing values are close to the end, it will definitely affect the forecast as exponential smoothing gives higher weightage to the recent data. . 2. Detecting Seasonality: Seasonality is the number of seasons in one full time period. If you have a quarterly data, it has 4 seasonal periods in a year. Similarly, monthly has 12, yearly 1 and weekly has 52 seasonal periods. In Python or R, you have to specify the number of seasonal periods. But in Power BI, seasonality is detected automatically for you. As we saw above, I did not have to enter the Seasonality. Below are the steps and description of each step in identifying the seasonality. . De-trending: Trend is the positive or negative change in the level of the series over the observed period. When trend is present, the series is called &quot;non-stationary&quot;. Some forecasting methods such as ARIMA require the time series to be stationary before the method can be applied. ETS method can be used on stationary and non-stationary data. While Power BI does not apply ARIMA model, series is de-trended so we only have seasonality (and error) left in the series. Presence of trend can affect the periodogram (we will see that below). Series can be de-trended by differencing the previous observation. e.g. in our case, the first 3 observations are [362, 385, 432..]. We de-trend the series by subtracting 362 from 385, 385 from 431 and so on. There is no values before 362 so it becomes null. We lose one observation after differencing. New series will be [null, 23, 47,..] | Here are the data before and after differencing. After differencing there is no trend in the data and it only shows the seasonality. . Z-Normalization: Irrespective of the original and de-trended data distribution, data is z-normalized (i.e. standardized) to make the mean 0 and standard deviation 1. This will make the data normally distributed. Note that normality of data is not an essential condition for forecasting neither does it guarantee improvement in mean forecast. Standardization is necessary in many machine learning methods (especially in regression, neurel net based methods, clustering etc.) to make the features scale independent but is not required in forecasting. Normalized data may lead to narrower prediction interval due to stabilized variance. Another benefit of standardization is that forecasting errors are also scaled. Irrespective of the scale of the original data (100 vs 100,000), after normalization Power BI can compare the error metric with an internal benchmark and further refine the model. I don&#39;t know if Power BI does that but it&#39;s a possibility. | We will test the de-trended data and z-normalized data for normality, calculate mean &amp; standard deviation . De-trended data: . - Jarque Bera p-value: 0.22 , Data is Normal - Mean: Sales 20.29 dtype: float64 , - Std Deviation: Sales 77.01 dtype: float64 . Observations . De-trended data actually shows bi-modal normal distribution. Jarque Bera test confirms normality. | The trend is almost flat, except in the first few observations. This actually means we need higher order differencing to remove trend but for the purposes of our analysis, this is good. | De-trended data has a mean of 20.2 and standard deviation of 77. | . Transformed Data . Transform the data with z-normalization to make mean 0, std 1 and remove trend component . - Jarque Bera p-value: 0.2 , Data is Normal - Mean: Sales_X 0.0 dtype: float64 , - Std Deviation: Sales_X 1.0 dtype: float64 . Observations . Transformed data still show the 2 peak bi-modal distribution with a mean of 0 and standard deviation of 1 | . Identify candidate periods from the power spectrum: . So far we have observed the data in time domain but we can also see it in frequency domain to identify prominent frequencies. It&#39;s based on the assumption that it is made up of sine and cosine waves of different frequencies. This helps us detect periodic component of known/unknown frequencies. It can show additional details of the time series that can be easily missed. We do it with a Periodogram . | Rank candidate periods: . Pearson and Spearman auto-correlations are computed for the transformed data. Significant lags in auto-correlation are matched with those found in the frequency analysis. If multiple peaks are found, the lags are arranged and peak with the highest correlation value is used for seasonality. . Auto-correlation is the correlation of the time series with its past values. e.g. to calculate the correlation at 1st lag, time series is shifted by 1 observation and correlation is calculated. Process is repeated for many past versions of itself. If auto-correlation is significant at a particular lag value &#39;k&#39;, it shows that the observations from k periods ago affect the time series significantly. You can read more here. . In the ACF plot any value that is outside the blue band shows significant lag. As we can see below, only the lag at k=4 is significant (~0.75). This matches with the observation from periodogram. . Thus, Power BI can now confirm that this series has seasonality of 4, phew!. I am glad Power BI does all this automatically and fast. . | Notes about seasonality: . Power BI recommends not using very long data as higher order lags have less information and can affect Pearson correlation calculation. Pearson correlation shows linear relationship between two variables. If the data is very log, the linear assumption may not remain valid. | Power BI recommends at least 3-4 seasons worth of data for seasonality detection to work properly. This means for quarterly data &gt;12 values, &gt;36 for monthly, &gt;156 for weekly data | You can also specify the seasonality manually. I highly recommend entering the seasonality manually because I have found that automatic seasonality may not always work as intended. I will show an example below. | . I randomly grabbed a monthly time series from the M3 competition dataset. M3 competition dataset is often used in research as a benchmark for testing various forecasting methods. M3 has 3003 time series of various seasonalities. For this time series, seasonality = 12 and the goal is to forecast next 12 months. I first created the forecast without specifying the seasonality. As you can see in the clip below, the forecast looks weird and definitely not what you would expect. After specifying seasonality =12, the forecast looks much more reasonable. Reducing the length of the series, as recommended by Power BI, did not help either. Automatic seasonality detection is not as robust as we would like. It&#39;s also possible that Power BI&#39;s algorithm is thrown off by the sudden peak in year 1985 which could be an outlier. . Forecasting . To compare the forecasting accuracy, I created a forecast in Power BI based on first 18 values (training set) and forecasted the last 6 values (validation). To be consistent, I entered 4 as seasonality. Table below shows the actual values, forecast, 95% upper and lower CI. . . Sales forecastValue confidenceHighBound confidenceLowBound . Date . 2016-09-30 773 | 813.0 | 864.1 | 761.8 | . 2016-12-31 592 | 636.1 | 689.4 | 582.9 | . 2017-03-31 627 | 725.5 | 797.7 | 653.4 | . 2017-06-30 725 | 785.5 | 873.8 | 697.1 | . 2017-09-30 854 | 900.7 | 1012.3 | 789.1 | . 2017-12-31 661 | 702.9 | 797.0 | 608.9 | . Forecast Accuracy: . There are many forecast model accuracy metrics. The two most common are RMSE (Root Mean Square Error) and % MAPE (Mean Absolute Percentage Error). You can read more about it in &quot;Evaluation Metric&quot; section in Part 2. In general, if the data has no outliers and 0 values, RMSE is a good metric to use. It can be thought of as the avg error in the mean forecast. While there are more robust and scale independent measures that can be used, we will use RMSE for comparing &amp; evaluating the performance. The smaller the RMSE and MAPE the better. . e.g. for Power BI forecast, RSME = sqrt (avg[ (773-813)^2 + (592-636.1)^2 + (627-725.5)^2 + (854-900.7)^2 + (661-702)^2)]) . . %MAPE RMSE . 0 8.1 | 58.9 | . Observations: . Power BI did an excellent job of capturing the trend and seasonality in the data. | Power BI forecast runs parallel to the actual values by almost the same margin, this may indicate some bias in the forecast | %MAPE is 8% and RMSE is 59. Thus, Power BI forecast, on average, in +/-8% of actual values or in terms of numbers +/- 59. For comaprison purposes, the standard deviation of the data is 111 so this is a really good forecast. | Actual value is outside the CI band in Q1 2017. I will discuss CI little later in the post, first we focus on mean forecasts. | ETS model in Python . Pyhton (and R) has two implimentations of the exponential smoothing methods. Holt-Winter&#39;s (HW) and ETS. ETS has a statistical framework, HW doesnt and HW can be thought of as a subset of ETS. . For HW in Python, you use statsmodels.tsa.holtwinters.ExponentialSmoothing() class and for state space ETS methods you use statsmodels.tsa.statespace.exponential_smoothing.ExponentialSmoothing() . In R, for HW use hw() and for ETS use ets() from forecast library. . Note: In general, although the ETS(AAA) model is equivalent to Holt-Winter&#8217;s additive linear model, and should give same answers, ETS almost always performs better because of how the parameter initialization and optimization works. You should experiment with HW but ETS more than likely will give more accurate results. . I will create the ETS(AAA) model in Python. . ETS(AAA) . ets1=(sm.tsa.statespace.ExponentialSmoothing( train, #Using training set , first 18 values) trend=True, #Trend is present initialization_method= &#39;concentrated&#39;, seasonal=4, #Quarterly data, seasonality=4 damped_trend=False). #Turn of Damped fit()) py_fc1 = ets1.forecast(6) #forecast next 6 values . %MAPE RMSE . 0 6.4 | 47.0 | . Observations: . ETS(AAA) forecast created in Python tracks the actual values much more closely | RMSE is 47, smaller than Power BI (58.9). Avg MAPE also better by 2.4 pct (6.4% vs. 8%) | One of the observations I noted earlier was that the trend looks more exponential than linear. The &#39;additive&#39; trend component can only capture the linear trends and hence not suitable in this case. The more accurate model would be ETS(MAM). Also, if you closely look at the first plot (top of the page) where I plotted the data with 4Q rolling mean, you can clearly see that the orange line is levelling off towards the end. So we not only have an exponential trend it is slowing at the end. . We can make the exponential trend linear by taking a &#39;log&#39; of the observations and setting damped_trend=True. New model is ETS(A,Ad,A) . ETS(A,Ad,A) . ets_model=sm.tsa.statespace.ExponentialSmoothing(np.log(train), trend=True, initialization_method= &#39;heuristic&#39;, seasonal=4, damped_trend=True).fit() py_fc2 = ets_model.get_forecast(6) results_df=(py_fc2.summary_frame( alpha=0.05).apply( np.exp)[[&quot;mean&quot;,&quot;mean_ci_lower&quot;,&quot;mean_ci_upper&quot;]]) . %MAPE RMSE . 0 5.1 | 40.9 | . Observations: . ETS(A,Ad,A) matches the actual values even better than the earlier ETS(AAA) model | RMSE (40.9) and %MAPE (5%) are better than the ETS(AAA) and Power BI forecast. | Recommendations . The main-take away from the above exercise is that Power BI can create fairly accurate forecasts but has limitations: . The trend and seasonal components have to be linear. If you have exponential and/or damped trend, you may not get accurate results | Automatic detection of seasonality can be spotty. It&#39;s best to manually enter the seasoanlity in the Forecast dialog box in Power BI | Power BI uses two ETS models (AAN) and (AAA). It chooses the method based on its own algorithm. | ETS method has its own limitations. While it can be more robust to outliers compared to ARIMA, location of the outlier (close to the end of the series vs earlier) can throw off the forecast. Power BI detects outliers by monitoring local trend and automatically adjusting forecast accordingly. | ETS cannot be used for high frequency time series such as daily, sub-daily (minute, hourly) data. In fact, weekly data is also pushing the envelope a little bit. The reason for that is in high-frequency data such as weekly you may have multiple seasonalities. For example, if you have weekly sales data, it&#39;s possible that sales may be higher closer to month end (&#39;week of the month&#39;), plus some months may have higher sales than other (&#39;month of year&#39; seasonality). We cannot enter mutiple seasonality values. Thus do not use Power BI forecast for anything other than monthly, quarterly, semi-annual, yearly data, you can use weekly data but with caution. If you have high frequency data you will need to try TBATS, deep-learning, Facebook Prophet models (see part 2). If you do use Power BI forecast for high-frequency data, it will likley use the (AAN) model and give you a straight line with a trend. | Use data with at least 3-4 full seasonal cycles. That translates to &gt;36 for monthly, 12 for quarterly data. | Do not use too much data. While Power BI doesn&#39;t mention what&#39;s too much, I would recommend using only the relevant 5 cycle data if available. | Power BI does not provide model evaluation metric. You can &#39;estimate&#39; the model accuracy by the method described above. Let&#39;s say your forecast horizon is 12 months. Enter 12 in the &#39;Ignore Last&#39; and create a forecast for 12 months. Use the RSME &amp; %MAPE to evaluate the forecast accuracy. While it doesn&#39;t guarantee true forecast accuracy, it at least gives you an estimation. If you don&#39;t have enough data, use the Cross-Validation approach I described in Part 2 | Power BI imputes missing values by linear iterpolation. If your data has non-linear trend, too many missing values (&gt;40%), missing values close to the end of the time series, it&#39;s better to clean up the data yourself than letting Power BI do it automatically. | If your data has many 0&#39;s (no sales, no production etc) which is usually the case for new products or slow moving products (i.e intermittent demand), do not use Power BI forecast. ETS should not be used for intermittent time series. Croston&#39;s method, deep-learning models can be used in that case. | ETS cannot use additional variables (exogenous variables) to improve the forecast. For example, if you sell more products when it&#39;s sunny, warm outside, on weekends, holidays, sport events etc. If the time series has high correlation with such events, you can include them in the forecast model for better accuracy. SARIMAX, deep-learning, gradient boosting models can be used for that. AutoML in Azure ML has built in holiday calendar and can include all these variables in the forecast. | An important and essential part of any statistical/machine learning model is model diagnostics. We want to make sure the model is valid, accurate, doesn&#39;t overfit and has extracted all the information from the available data. It is usually done by residual diagnsotics. Power BI doesn&#39;t provide any model diagnostics or parameter values etc. I am going to skip that part here but you can read more in Part 2. | Forecast Uncertainty, Confidence Interval or Prediction Interval . The forecast we obtain in Power BI is the mean forecast. Time series is a statistical process and thus has probability distribution. When we create a forecast, we are estimating the mean of that forecast distribution, i.e 50% probability values. What about the other percentiles? . Power BI lets you choose the Confidence Interval (CI). You can choose from 75%, 80%, 85%, 95%, 99%. The 2014 blog talks about using confidence interval for assessing forecast accuracy. . &#39;...The shaded area shows you the range of predicted values at different confidence levels...&#39; . I think Power BI is calculating the Prediction Interval (PI) based on above description and not CI. You are actually choosing the confidence values for the PI. Let me explain the difference between the two and why it is important. . When you calculate CI, you are creating a confidence interval around the mean forecast. For example, the first mean forecast above is 813, upper bound is 864 and lower bound is 762. If it is CI, it does not mean the forecast will be between 864 and 762, 95% of the time. What it means is that based on the optimized parameters used in the model, limited sample size etc., the mean will be between 864 and 761. It&#39;s the error in estimating the mean. It&#39;s true that the band gives you an estimation of the range of mean values but it&#39;s not the range of the predicted values if it is CI. Prediction intervals (PI) are wider than CI. CI&#39;s are used to estimate error in a parameter of the population , e.g. mean, median, percentile, even PI values. It&#39;s not a range of predicted values. . I will give you an example, let&#39; say you work in an office where 1000 people work and you are asked what&#39;s the average work experience in years for the company. You go and ask 25 people about their years of experience, take an average and come up with 12.5 and standard dev of 2.3. Because you only asked 25 people, you know there is an error. You can estimate that error using CI and it is [11.6, 13.4] at 95%. This means the &quot;mean&quot; will be between 11.6 and 13.4 95% of the time but the actual &quot;range&quot; of experience would be far greater depending on the distribution. You might have people with 1 month experience to 30+ years experience, that&#39;s PI. Note that PI can be close to CI but not always. CI doesn&#39;t take model uncertainty into account and has little to no value in practical forecasting. . . Based on the 1 line description in the blog and the tests I have done, I think Power BI is calculating the PI, at least I hope so. This is why Power BI should release more documentation on this topic. . How is PI calculated? . Hyndman has provided calculations for estimating the PI. It assumes the residuals are normal, which may not always be the case. Simulations can be used for better results as it doesn&#39;t assume distribution of the residuals. I calculated the PI using simulation in Part 2 as [923, 691] which is slightly wider than the CI from Power BI [864,762]. . Also note that the PI &amp; CI grow with the forecast horizon. The farther you are into the future, the more uncertainty you have in the estimates. . Below I calculated the PI using R. As you can see they are slightly wider than Power BI&#39;s range but close. . %%R -i train -o fets library(fpp2) r_train &lt;- ts(train$Sales, start=c(2012,01), frequency=4) fets &lt;- r_train %&gt;% ets() %&gt;% forecast(h=6, simulate=TRUE, level= 95) %&gt;% summary() . Point Forecast Lo 95 Hi 95 . 0 799.7 | 733.7 | 863.9 | . 1 631.1 | 563.9 | 702.3 | . 2 724.0 | 635.9 | 821.3 | . 3 771.0 | 665.4 | 883.7 | . 4 878.5 | 745.6 | 1023.9 | . 5 691.8 | 578.6 | 817.8 | . Implementation in Power BI with Python . Power BI created a reasonable forecast but it can be improved. If you want to further improve this forecast, you can use the &quot;Ensemble Forecast&quot; discussed in Part 2. . Perhaps the biggest limitation of forecasting in Power BI is not being able to access the forecast values for further calculations or reporting. It doesn&#39;t even show the forecast on the line chart. . We can use this Python code in Power BI for forecasting. Watch the video below. I have uploaded this Power BI file to my Github for your reference. . You need statsmodels, numpy, pandas libraries installed. . #hide_output # &#39;dataset&#39; holds the input data for this script import numpy as np from statsmodels.tsa.statespace.exponential_smoothing import ExponentialSmoothing h = 4 #forecast horizon model=ExponentialSmoothing(np.log(dataset[&quot;Sales&quot;]),trend=True,initialization_method= &#39;heuristic&#39;,seasonal=4,damped_trend=True).fit() fc=np.exp(model.predict(0,len(dataset)+(h-1))) dates=pandas.date_range(start=&#39;2012-03-31&#39;, periods= len(dataset)+h , freq=&#39;Q&#39;) df=pandas.DataFrame({&#39;dates&#39;:dates,&#39;Actual&#39;:dataset[&quot;Sales&quot;],&#39;Forecast&#39;:fc}) . We are using the same Python model we used earlier but we have to create a new dataframe for the additional values created by the forecast. Note that in this case, I am also obtaining the fittedvalues to inspect the fit over the data. If you only care about the forecast, change the code to model.forecast(4) . DAX for extracting forecast Sales . %%html Forecast_Sales&amp;nbsp;=&lt;br&gt;&lt;span class=&quot;Keyword&quot; style=&quot;color:#035aca&quot;&gt;CALCULATE&lt;/span&gt;&lt;span class=&quot;Parenthesis&quot; style=&quot;color:#808080&quot;&gt;&amp;nbsp;(&lt;/span&gt;&amp;nbsp;&lt;span class=&quot;Keyword&quot; style=&quot;color:#035aca&quot;&gt;VALUES&lt;/span&gt;&lt;span class=&quot;Parenthesis&quot; style=&quot;color:#808080&quot;&gt;&amp;nbsp;(&lt;/span&gt;&amp;nbsp;forecast[Forecast]&amp;nbsp;&lt;span class=&quot;Parenthesis&quot; style=&quot;color:#808080&quot;&gt;)&lt;/span&gt;,&amp;nbsp;forecast[type]&amp;nbsp;==&amp;nbsp;&lt;span class=&quot;StringLiteral&quot; style=&quot;color:#D93124&quot;&gt;&quot;Forecast&quot;&lt;/span&gt;&amp;nbsp;&lt;span class=&quot;Parenthesis&quot; style=&quot;color:#808080&quot;&gt;)&lt;/span&gt;&lt;br&gt; . Forecast_Sales&nbsp;=CALCULATE&nbsp;(&nbsp;VALUES&nbsp;(&nbsp;forecast[Forecast]&nbsp;),&nbsp;forecast[type]&nbsp;==&nbsp;&quot;Forecast&quot;&nbsp;) . Note: If you have not installed or used Python in Power BI before, read the documentation first. Also read these performance tips by David Eldersveld . . Tip: I recommend creating virtual environments and using (e.g. pynenv) for Power BI specific virtual environment . . Python in Power BI Limitations: . If you are not familiar with Python or don&#39;t have access to Python at work, this obvisouly won&#39;t work for you | For Python scripts to work properly in Power BI service, all data sources need to be set to &quot;Public&quot;. That&#39;s a BIG NO. | If you use Pyhon script in Power Query, you have to use a personal gateway. That may not be a problem but if you are using dataflow in your report, this solution won&#39;t work as dataflow needs Enterprise Gateway | Python script cannot be used on merged queries, you will get an error. You can merge after the Python script but not before. | The exponential_smoothing() can resturn confidence interval (see Part 2) but as we discussed above, it&#39;s of no practical use. It does not calculate prediction interval. We can use simulation to get prediction interval but it takes few minutes so can&#39;t practially be used as a Python script in Power BI. | Using R in Power BI . Leila Etaati has covered forecasting using R in great details, so I won&#39;t cover it here. But personally I find the forecast(), fpp2(), fable() libraries in R to be much faster, easier to work with and they do return prediction interval. Plus, unlike Python,ets() can be used for &quot;multiplicative&quot; models. . Using Excel . Excel has FORECAST.ETS() formula which uses ETS(AAA) method to create the forecast, just like Power BI. Power BI could be using the same algorithm under the hood because the options and results are very identical. Forecast Sheet under &quot;Data&quot; can be used for creating forecast with a UI. You can import this excel in Power BI and create a line chart almost exactly same as Power BI. . ETS can&#39;t be used for high-frequency data anyway so you would only need to update the Excel sheet once a month, quarter, year etc. so it shouldn&#39;t be a big problem. You can also use Power Automate to refresh the Excel on a schedule. . If you are ok with the limitations of ETS(AAA) model discussed above or find that ETS(AAA) can produce accurate results for your data, I think this is the easiest method to show forecast in Power BI. . . Other Options . SQL: If you are using SQL as a datasource, you can use RevoScalePy/RevoScaleR in SQL to serve any machine learning model including forecasting . KNIME: KNIME is an open-source tool used for productionizing data science projects. It&#39;s free, easy to use, can be used for ETL and the best part is that it has a Power BI integration. You can create a forecasting model (it&#39;s own ETS, ARIMA nodes or R/Python) and push the results to Power BI. If you need a more complex model, this is a great option. I will cover this in a future blog post. . Azure ML: Azure ML Service has a direct integration with Power BI. You can create the model in Azure Notebook, Designer or AutoML. You can see an example here. In the next blog I will cover this in more detail. . Custom Visuals: There are some custom visuals in the Apps gallery but I generally never use custom visuals for data privacy and dependency reasons. . Summary . It&#39;s easy to create a forecast in Power BI but it is severely limited | You cannot show the forecasted values on the line chart | You do not know what preprocessing Power BI may have applied to the data (imputation, outlier removal etc.) | You cannot plot multiple columns or use second Y axis when Forecast is used | You cannot use the &#39;Legend&#39; in the line chart with Forecast. Only works in Line Chart and not in &#39;Line &amp; Stacked column chart&#39; | You cannot extract the forecasted values for use in measures or calculated columns | Forecast can be exported as an excel file, re-imported to use the forecast but that would defeat the purpose of automatic forecasting | Power BI uses two ETS methods (AAN) and (AAA) which can be used for additive components but not when the trend, seasonality are non-linear | Power BI forecast should not be used on high-frequency data such as daily, hourly (even weekly if it exhibits multiple seasonalities) | Use at least data worth 3-4 seasons (&gt;12 for quarterly, &gt;36 for monthly data) | Power BI should provide more documentation on confidence interval and clarify if it is confidence interval or prediction interval. Until then, use it with caution. | If you do use Power BI&#39;s forecast tool, create a forecast first for time greater than or equal to your forecast horizon, use the same number in the &#39;Ignore Last&#39; points, assess the fit and calculate RMSE. If the fit looks good, you can use it for final forecast. | Always enter the seasonality manually | Do not use Power BI forecast on intermittent data with several 0&#39;s. | Excel might provide the easiest way to create an ETS(AAA) forecast. But has the same limitations as discussed above for ETS in general. | You can use Pyhton and R for generating forecasts in and outside of Power BI. R can give mean forecast and the prediction interval. For Pyhton, use simulations to generate PI. | References: . Forecasting: Principles and Practice, by Prof. Hyndman | Time Series Analysis and its Applications, by Robert Shumway | Time Series Analysis and Forecasting, by Montgomery &amp; Jennings | Introduction to Time Series and Analysis, by Brockwell | Practial Time Series Forecasting with R, by Galit Shmueli 6. https://homepage.univie.ac.at/robert.kunst/pres09_prog_turyna_hrdina.pdf |",
            "url": "https://pawarbi.github.io/blog/forecasting/python/powerbi/forecasting_in_powerbi/2020/04/24/timeseries-powerbi.html",
            "relUrl": "/forecasting/python/powerbi/forecasting_in_powerbi/2020/04/24/timeseries-powerbi.html",
            "date": " • Apr 24, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "Time series Forecasting in Python & R, Part 2 (Forecasting )",
            "content": "Overview . In Part 1, I did exploratory data analysis of sales time series of a French retailer. In this blog I will apply various time series models in Python and R to forecast sales for the next 4 quarters. The forecasting methods I will cover are: . Seasonal Naive | Triple Exponential Smoothing (Holt Winter&#39;s) | ETS (State Space Error Trend Seasonality) | SARIMA | Facebook Prohet | Ensemble Forecast | . For each of these models, I will provide a short description for intuitive understanding of these methods and give references for more academic explanation. For any forecasting model, the general steps are as below. . Forecasting Steps . EDA | Forecast on test set | Evaluate the forecast Use appropriate evaluation metric (%MAPE, RMSE, AIC) | Plot the forecast against train and test data set | . | Check residuals. . Plot residuals, plot ACF/PACF and Q/Q plots | Conditions A, B below are essential and C,D are useful. Residuals should be: . Uncorrelated | Have zero (or close to zero) mean | Constant variance | Normally distributed | | First two ensure that there is no more information that can be extracted from the data, while the bottom two keep the variability in the point forecast narrow . | . | Select model(s) Forecast future series | Prediction Interval | . | Evaluation Metric . We evaluate the forecasting model by comparing the fitted &amp; predicted values against the actual values in training and test sets. Note that residuals are the difference between training data and fitted values, while forecast error is the difference between test data and predicted values. We use residuals to check performance of the model while errors for checking accuracy/uncertainty of the future forecast. . As a general rule, if the data has no outliers RMSE (Root Mean Square Error) is a good metric to use. %MAPE (Mean Absolute Percentage Error) provides a more inutitive understanding as it is expressed in percentage. We do not use %MAPE if the series is intermittent to avoid division by zero. Note that both these measures are not scale independent but to keep things simple I will use RSME and MAPE. . . Importing libraries . #collapse-hide #Author: Sandeep Pawar #Version: 1.0 #Date Mar 27, 2020 import pandas as pd import numpy as np import itertools #Plotting libraries import matplotlib.pyplot as plt import seaborn as sns import altair as alt plt.style.use(&#39;seaborn-white&#39;) pd.plotting.register_matplotlib_converters() %matplotlib inline #statistics libraries import statsmodels.api as sm import scipy from scipy.stats import anderson from statsmodels.tools.eval_measures import rmse from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import month_plot, seasonal_plot, plot_acf, plot_pacf, quarter_plot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing from statsmodels.stats.diagnostic import acorr_ljungbox as ljung from statsmodels.tsa.statespace.tools import diff as diff from statsmodels.tsa.statespace.sarimax import SARIMAX import pmdarima as pm from pmdarima import ARIMA, auto_arima from scipy import signal from scipy.stats import shapiro from scipy.stats import boxcox from scipy.special import inv_boxcox from sklearn.preprocessing import StandardScaler from scipy.stats import jarque_bera as jb from itertools import combinations import fbprophet as Prophet #library to use R in Python import rpy2 from rpy2.robjects import pandas2ri pandas2ri.activate() import warnings warnings.filterwarnings(&quot;ignore&quot;) np.random.seed(786) . . Library versions . #Printing library versions print(&#39;Pandas:&#39;, pd.__version__) print(&#39;Statsmodels:&#39;, sm.__version__) print(&#39;Scipy:&#39;, scipy.__version__) print(&#39;Rpy2:&#39;, rpy2.__version__) print(&#39;Numpy:&#39;, np.__version__) . Pandas: 0.25.0 Statsmodels: 0.11.0 Scipy: 1.4.1 Rpy2: 2.9.4 Numpy: 1.18.2 . Various functions used . Below are some of the custom functions I wrote for forecast accuracy, gridsearching, residual diagnostics. . def MAPE(y_true, y_pred): &quot;&quot;&quot; %Error compares true value with predicted value. Lower the better. Use this along with rmse(). If the series has outliers, compare/select model using MAPE instead of rmse() &quot;&quot;&quot; y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 def HWGrid(train, test, seasonal_periods): &quot;&quot;&quot; Author: Sandeep Pawar twitter: @PawarBI Functions returns a dataframe with parameters of the Holt-Winter&#39;s method and corresponding train &amp; test evaluation scores. It also does a quick check of the residuals using Ljung-Box test and Shapiro test for normality. Residuals must be uncorrelated. train: (pandas series) - Training data test: (pandas series) - Test data Seasonal_periods: int - No of seasonas in the time period. e.g. 4 for Quarterly, 12 for Monthly, 52 for Weekly data &quot;&quot;&quot; trend = [&#39;add&#39;,&#39;mul&#39;] seasonal = [&#39;add&#39;,&#39;mul&#39;] damped = [False, True] use_boxcox = [False, True, &#39;log&#39;] params = itertools.product(trend,seasonal,damped,use_boxcox) result_df = pd.DataFrame(columns=[&#39;Trend&#39;, &#39;Seasonal&#39;, &#39;Damped&#39;, &#39;BoxCox&#39;,&#39;AICc Train&#39;, &#39;%MAPE_Train&#39;, &#39;RMSE_Train&#39;, &#39;%MAPE_Test&#39;, &#39;RMSE_Test&#39;, &quot;Resid_LJ&quot;, &quot;Resid_Norm&quot;,&quot;Resid_mean&quot; ]) for trend,seasonal,damped,use_boxcox in params: model = ExponentialSmoothing(train, trend=trend, damped=damped, seasonal=seasonal, seasonal_periods=seasonal_periods).fit(use_boxcox=use_boxcox) mape1=MAPE(train,model.fittedvalues) rmse1=rmse(train,model.fittedvalues) mape2=MAPE(test,model.forecast(len(test))) rmse2=rmse(test,model.forecast(len(test))) aicc1 = model.aicc.round(1) lj_p_val = np.mean(ljung(x=model.resid, lags=10)[1]) norm_p_val = jb(model.resid)[1]#shapiro(model.resid)[1] lj = &quot;Uncorrelated&quot; if lj_p_val &gt; 0.05 else &quot;Correlated&quot; norm = &quot;Normal&quot; if norm_p_val &gt; 0.05 else &quot;Non-Normal&quot; result_df = result_df.append({&#39;Trend&#39;:trend , &#39;Seasonal&#39;: seasonal , &#39;Damped&#39;:damped , &#39;BoxCox&#39;:use_boxcox , &#39;%MAPE_Train&#39;:np.round(mape1,2) , &#39;RMSE_Train&#39;:np.round(rmse1,1) , &#39;AICc Train&#39;:aicc1 , &#39;%MAPE_Test&#39;:np.round(mape2,2) , &#39;RMSE_Test&#39;:np.round(rmse2,1) , &#39;Resid_LJ&#39; :lj , &#39;Resid_Norm&#39;:norm , &#39;Resid_mean&#39;:np.round(model.resid.mean(),1)} , ignore_index=True, sort=False) return result_df.sort_values(by=[&quot;RMSE_Test&quot;, &quot;%MAPE_Test&quot;,&quot;RMSE_Train&quot;,&quot;%MAPE_Train&quot;]).style.format({&quot;%MAPE_Train&quot;: &quot;{:20,.2f}%&quot;, &quot;%MAPE_Test&quot;: &quot;{:20,.2f}%&quot;}).highlight_min(color=&#39;lightgreen&#39;) . Calculating cross-validation score for Holt-Winter&#39;s method in Python . def hw_cv(series, seasonal_periods, initial_train_window, test_window): from statsmodels.tools.eval_measures import rmse import warnings warnings.filterwarnings(&quot;ignore&quot;) &quot;&quot;&quot; Author: Sandeep Pawar Date: 4/15/2020 Ver: 1.0 Returns Rolling and Expanding cross-validation scores (avg rmse), along with model paramters for Triple Exponential Smoothing method. Expanding expands the training set each time by adding one observation, while rolling slides the training and test by one observation each time. Output shows parameters used and Rolling &amp; Expanding cv scores. Output is in below order: 1. Trend 2. Seasonal 3. Damped 4. use_boxcox 5. Rolling cv 6. Expanding cv Requirements: Pandas, Numpy, Statsmodels, itertools, rmse series: Pandas Series Time series seasonal_periods: int No of seasonal periods in a full cycle (e.g. 4 in quarter, 12 in monthly, 52 in weekly data) initial_train_window: int Minimum training set length. Recommended to use minimum 2 * seasonal_periods test_window: int Test set length. Recommended to use equal to forecast horizon e.g. hw_cv(ts[&quot;Sales&quot;], 4, 12, 6 ) Output: add add False False R: 41.3 ,E: 39.9 Note: This function can take anywhere from 5-15 min to run full output &quot;&quot;&quot; def expanding_tscv(series,trend,seasonal,seasonal_periods,damped,boxcox,initial_train_window, test_window): i = 0 x = initial_train_window t = test_window errors_roll=[] while (i+x+t) &lt;len(series): train_ts=series[:(i+x)].values test_ts= series[(i+x):(i+x+t)].values model_roll = ExponentialSmoothing(train_ts, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods, damped=damped).fit(use_boxcox=boxcox) fcast = model_roll.forecast(t) error_roll = rmse(test_ts, fcast) errors_roll.append(error_roll) i=i+1 return np.mean(errors_roll).round(1) def rolling_tscv(series,trend,seasonal,seasonal_periods,damped,boxcox,initial_train_window, test_window): i = 0 x = initial_train_window t = test_window errors_roll=[] while (i+x+t) &lt;len(series): train_ts=series[(i):(i+x)].values test_ts= series[(i+x):(i+x+t)].values model_roll = ExponentialSmoothing(train_ts, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods, damped=damped).fit(use_boxcox=boxcox) fcast = model_roll.forecast(t) error_roll = rmse(test_ts, fcast) errors_roll.append(error_roll) i=i+1 return np.mean(errors_roll).round(1) trend = [&#39;add&#39;,&#39;mul&#39;] seasonal = [&#39;add&#39;,&#39;mul&#39;] damped = [False, True] use_boxcox = [False, True, &#39;log&#39;] params = itertools.product(trend,seasonal,damped,use_boxcox) for trend,seasonal,damped,use_boxcox in params: r=rolling_tscv(data[&quot;Sales&quot;], trend, seasonal, 4, damped, use_boxcox, 12,4) e=expanding_tscv(data[&quot;Sales&quot;], trend, seasonal, 4, damped, use_boxcox, 12,4) result = print(trend, seasonal, damped, use_boxcox,&quot; R:&quot;, r,&quot; ,E:&quot;, e) return result . Function for residual diagnostics . def residcheck(residuals, lags): &quot;&quot;&quot; Function to check if the residuals are white noise. Ideally the residuals should be uncorrelated, zero mean, constant variance and normally distributed. First two are must, while last two are good to have. If the first two are not met, we have not fully captured the information from the data for prediction. Consider different model and/or add exogenous variable. If Ljung Box test shows p&gt; 0.05, the residuals as a group are white noise. Some lags might still be significant. Lags should be min(2*seasonal_period, T/5) plots from: https://tomaugspurger.github.io/modern-7-timeseries.html &quot;&quot;&quot; resid_mean = np.mean(residuals) lj_p_val = np.mean(ljung(x=residuals, lags=lags)[1]) norm_p_val = jb(residuals)[1] adfuller_p = adfuller(residuals)[1] fig = plt.figure(figsize=(10,8)) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2); acf_ax = plt.subplot2grid(layout, (1, 0)); kde_ax = plt.subplot2grid(layout, (1, 1)); residuals.plot(ax=ts_ax) plot_acf(residuals, lags=lags, ax=acf_ax); sns.kdeplot(residuals); #[ax.set_xlim(1.5) for ax in [acf_ax, kde_ax]] sns.despine() plt.tight_layout(); print(&quot;** Mean of the residuals: &quot;, np.around(resid_mean,2)) print(&quot; n** Ljung Box Test, p-value:&quot;, np.around(lj_p_val,3), &quot;(&gt;0.05, Uncorrelated)&quot; if (lj_p_val &gt; 0.05) else &quot;(&lt;0.05, Correlated)&quot;) print(&quot; n** Jarque Bera Normality Test, p_value:&quot;, np.around(norm_p_val,3), &quot;(&gt;0.05, Normal)&quot; if (norm_p_val&gt;0.05) else &quot;(&lt;0.05, Not-normal)&quot;) print(&quot; n** AD Fuller, p_value:&quot;, np.around(adfuller_p,3), &quot;(&gt;0.05, Non-stationary)&quot; if (adfuller_p &gt; 0.05) else &quot;(&lt;0.05, Stationary)&quot;) return ts_ax, acf_ax, kde_ax . Function for calculating RMSE &amp; %MAPE . def accuracy(y1,y2): accuracy_df=pd.DataFrame() rms_error = np.round(rmse(y1, y2),1) map_error = np.round(np.mean(np.abs((np.array(y1) - np.array(y2)) / np.array(y1))) * 100,1) accuracy_df=accuracy_df.append({&quot;RMSE&quot;:rms_error, &quot;%MAPE&quot;: map_error}, ignore_index=True) return accuracy_df . Importing Data . path = &#39;https://raw.githubusercontent.com/pawarbi/datasets/master/timeseries/ts_frenchretail.csv&#39; #Sales numbers are in thousands, so I am dividing by 1000 to make it easier to work with numbers, especially squared errors data = pd.read_csv(path, parse_dates=True, index_col=&quot;Date&quot;).div(1_000) data.index.freq=&#39;Q&#39; data.head() . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . data.index . DatetimeIndex([&#39;2012-03-31&#39;, &#39;2012-06-30&#39;, &#39;2012-09-30&#39;, &#39;2012-12-31&#39;, &#39;2013-03-31&#39;, &#39;2013-06-30&#39;, &#39;2013-09-30&#39;, &#39;2013-12-31&#39;, &#39;2014-03-31&#39;, &#39;2014-06-30&#39;, &#39;2014-09-30&#39;, &#39;2014-12-31&#39;, &#39;2015-03-31&#39;, &#39;2015-06-30&#39;, &#39;2015-09-30&#39;, &#39;2015-12-31&#39;, &#39;2016-03-31&#39;, &#39;2016-06-30&#39;, &#39;2016-09-30&#39;, &#39;2016-12-31&#39;, &#39;2017-03-31&#39;, &#39;2017-06-30&#39;, &#39;2017-09-30&#39;, &#39;2017-12-31&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, freq=&#39;Q-DEC&#39;) . Train Test Split: . Part 1 on EDA covers this in detail. I will be using both typical train/test split and cross-validation for training &amp; evaluation. . #Split into train and test train = data.iloc[:-6] test = data.iloc[-6:] #forecast horizon h = 6 train_length = len(train) print(&#39;train_length:&#39;,train_length, &#39; ntest_length:&#39;, len(test) ) #Creating BxCox transformed train &amp; test to be used later train_bcox, bcox_lam = boxcox(train[&quot;Sales&quot;]) print(&quot;BoxCox parameter to linearize the series:&quot;, bcox_lam.round(2)) test_bcox = boxcox(test[&quot;Sales&quot;], lmbda=bcox_lam) train_log = np.log(train[&quot;Sales&quot;]) . train_length: 18 test_length: 6 BoxCox parameter to linearize the series: -0.21 . #collapse-hide #Create line chart for Training data. index is reset to use Date column train_chart=alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;Date&#39;, y=&#39;Sales&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]) #Create Rolling mean. This centered rolling mean rolling_mean = alt.Chart(train.reset_index()).mark_trail( color=&#39;orange&#39;, size=1 ).transform_window( rolling_mean=&#39;mean(Sales)&#39;, frame=[-4,4] ).encode( x=&#39;Date:T&#39;, y=&#39;rolling_mean:Q&#39;, size=&#39;Sales&#39; ) #Add data labels text = train_chart.mark_text( align=&#39;left&#39;, baseline=&#39;top&#39;, dx=5 # Moves text to right so it doesn&#39;t appear on top of the bar ).encode( text=&#39;Sales:Q&#39; ) #Add zoom-in/out scales = alt.selection_interval(bind=&#39;scales&#39;) #Combine everything (train_chart + rolling_mean +text).properties( width=600, title=&quot;French Retail Sales &amp; 4Q Rolling mean ( in &#39;000)&quot;).add_selection( scales ) . . Seasonal Naive . Seasonal naive method uses the observations from the corresponding season from last period. For example, forecast Q3 would be sales from Q3 last year. It does not take any trend or previous history into account. This method, as expected, is not the most accurate but helps create a baseline. As we explore more complex models, we want them to perform better than this and compare them with seasonal naive forecast. . . This method is not available in statsmodels library so I wrote a function for it. . def pysnaive(train_series,seasonal_periods,forecast_horizon): &#39;&#39;&#39; Python implementation of Seasonal Naive Forecast. This should work similar to https://otexts.com/fpp2/simple-methods.html Returns two arrays &gt; fitted: Values fitted to the training dataset &gt; fcast: seasonal naive forecast Author: Sandeep Pawar Date: Apr 9, 2020 Ver: 1.0 train_series: Pandas Series Training Series to be used for forecasting. This should be a valid Pandas Series. Length of the Training set should be greater than or equal to number of seasonal periods Seasonal_periods: int No of seasonal periods Yearly=1 Quarterly=4 Monthly=12 Weekly=52 Forecast_horizon: int Number of values to forecast into the future e.g. fitted_values = pysnaive(train,12,12)[0] fcast_values = pysnaive(train,12,12)[1] &#39;&#39;&#39; if len(train_series)&gt;= seasonal_periods: #checking if there are enough observations in the training data last_season=train_series.iloc[-seasonal_periods:] reps=np.int(np.ceil(forecast_horizon/seasonal_periods)) fcarray=np.tile(last_season,reps) fcast=pd.Series(fcarray[:forecast_horizon]) fitted = train_series.shift(seasonal_periods) else: fcast=print(&quot;Length of the trainining set must be greater than number of seasonal periods&quot;) return fitted, fcast . #Before I create the model, I am going to create a dataframe to store all out-of=sample forecasts and the test set predictions = test.copy() . Seasonal Naive Forecast model . #Fitted values py_snaive_fit = pysnaive(train[&quot;Sales&quot;], seasonal_periods=4, forecast_horizon=6)[0] #forecast py_snaive = pysnaive(train[&quot;Sales&quot;], seasonal_periods=4, forecast_horizon=6)[1] #Residuals py_snaive_resid = (train[&quot;Sales&quot;] - py_snaive_fit).dropna() predictions[&quot;py_snaive&quot;] = py_snaive.values predictions . Sales py_snaive . Date . 2016-09-30 773.0 | 681.0 | . 2016-12-31 592.0 | 557.0 | . 2017-03-31 627.0 | 628.0 | . 2017-06-30 725.0 | 707.0 | . 2017-09-30 854.0 | 681.0 | . 2017-12-31 661.0 | 557.0 | . Plot the Forecast . pd.plotting.register_matplotlib_converters() train[&quot;Sales&quot;].plot(figsize=(12,8))#, style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) py_snaive_fit.plot(color=&quot;b&quot;, legend=True, label=&quot;SNaive_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) predictions[&quot;py_snaive&quot;].plot(color=&quot;b&quot;, legend=True, label=&quot;Snaive_fc&quot;); . Model Evaluation: . #Training score accuracy(train[&quot;Sales&quot;].iloc[-len(py_snaive_fit.dropna()):], py_snaive_fit.dropna()) . %MAPE RMSE . 0 13.9 | 80.3 | . Model Accuracy . #Test score accuracy(predictions[&quot;Sales&quot;], predictions[&quot;py_snaive&quot;]) . %MAPE RMSE . 0 9.4 | 92.0 | . Residual Check . residcheck(py_snaive_resid.dropna(),12); . ** Mean of the residuals: 75.21 ** Ljung Box Test, p-value: 0.339 (&gt;0.05, Uncorrelated) ** Jarque Bera Normality Test, p_value: 0.721 (&gt;0.05, Normal) ** AD Fuller, p_value: 0.071 (&gt;0.05, Non-stationary) . observations: . Seasonal naive forecast actually performs quite well considering it&#39;s just a logical forecasting method and there is no statistical procedure involved. | Model captures seasonality and general trend quite well but under forecasts (underfitting) | Training RMSE is 80.3 and test RMSE is 92 which is less than the standard deviation of the training set (111). | Residual analysis shows residuls are not stationary and have non-zero mean. Residual plot clearly shows that the model hasn&#39;t extracted the trend and seasonal behaviour as well as we would like. Though visually the model seems to perform well, it&#39;s not a useful model on its own | Non-zero mean can be fixed by adding the mean back to the forecasts as explained here but in this case the mean is significantly away from zero. | We could perhaps fit an AR model to the residuals to get more out of it. If you look at the ACF plot, it shows no lags are significant but the PACF plot (see below) shows 4,9,10,11,12 are significant. This is an AR process signature. Thus, if we want to capture information and make this model useful, we can fit an AR model to the residuals to create a 2 layer model | This shows importance of always checking the residuals after fitting the model | #PACF of Seasonal Snaive model residuals plot_acf(py_snaive_resid); plot_pacf(py_snaive_resid); . Triple Exponential Smoothing (Holt-Winter&#39;s Method) . Triple Exponential Smoothing (Holt Winter&#39;s method) decomposes the series into level, trend, seasonality. Future values are predicted by combining these systematic factors based on recent history. The intuitive idea here is that the future will behave very similar to recent past, we just have to find how much of the past is relevant. The three systematic components are: . Level, (alpha): Average value around which the series varies. For a seasonal time series, level is obtained by first de-seasonalizing the series and then averaging. Alpha value determines how much of the past to consider and is between [0,1]. alpha=1 means give importance only to the last data point (naive forecast) | Trend, (beta): Trend is how the level changes over time. Similar to alpha, a beta value closer to 1 indicates the model is considering only the recent trend. Trend also has a damping factor (phi) which determines how much of the recent trend to &#39;forget&#39;. Consider it as a de-rating factor on trend. | Seasonality (gamma): This factor models how the series behaves in each time period for full season. Recall that in the previous blog about EDA (Part 1), I calculated the seasonal factors. Gamma is the same thing. | . This method is called &quot;Exponential&quot; because each of the above factors give exponential weightage to the past values. . Additive model = (Level + Trend) + Seasonality . Multiplicative Model = (Level Trend) Seasonality . The Exponentialsmoothing() method in statsmodels finds the optimal alpha, beta, gamma and phi by minizing the errors. . . Additive vs Multiplicative . Depending on the temporal structure of the time series, trend and seasonality can show additive, multiplicative or mix behaviour. . In case of trend, if the time series has linear trend, it&#39;s additive. If it is exponentially increasing (power law), a multiplicative model might fit better. . Seasonality is calculated relative to the level. If a series has additive seasonality, each season varies +/- relative to the level. E.g. in a quarterly series, in Q1 we might add +30 more units, -60 in Q2, +120 in Q3 and +60 in Q4 relative to level. Thus, seasonality peaks have somehwat fix height relative to level everywhere. On the other hand, in a multiplicative seasonality, the quantities will vary by %, i.e +5% in Q1, -7% in Q2, +10% in Q3 and +5 in Q4 relative to trend. As the level increases or decreases, seasonality can vary by %. . For this time series, we identified during EDA (Part 1) that trend is exponential. Seasonality can be modeled as additive or multiplicatives. Note that we can turn a multiplicative component into additive by taking log or using power transform (BoxCox). This is often preferred and may perform better. . Refer to Part 1 where I calculated these seasonality factors. . I replicated an illustration from Everette Gardener&#39;s paper to show this effect below. I highly recommend reading it if you are ineterested in exponential smoothing. . . Grid Searching Triple Exponential Smoothing (Holt-Winter&#39;s Method) . Instead of fitting each model individually, I wrote a custom function HWGrid() to perform uniform, gridsearch using the model parameters. This function also returns statistics on residuals (Ljung Box test, Normality test and mean). You get model evaluation metric and residual metric for 24 models. This may take a while (5-15 min) on your computer. . m=HWGrid(train[&quot;Sales&quot;], test[&quot;Sales&quot;], seasonal_periods=4) . m . Trend Seasonal Damped BoxCox AICc Train %MAPE_Train RMSE_Train %MAPE_Test RMSE_Test Resid_LJ Resid_Norm Resid_mean . 3 add | add | True | False | 170.6 | 3.55% | 20.4 | 10.79% | 81.3 | Uncorrelated | Normal | 4.4 | . 0 add | add | False | False | 151.8 | 2.41% | 18.2 | 10.98% | 82.6 | Uncorrelated | Normal | 4.9 | . 8 add | mul | False | log | 139.7 | 2.29% | 13 | 11.49% | 84 | Uncorrelated | Normal | 0.8 | . 11 add | mul | True | log | 154.3 | 2.29% | 13 | 11.49% | 84 | Uncorrelated | Normal | 0.8 | . 2 add | add | False | log | 138.9 | 2.20% | 12.7 | 11.69% | 84.8 | Uncorrelated | Normal | 0.8 | . 5 add | add | True | log | 153.4 | 2.20% | 12.7 | 11.69% | 84.8 | Uncorrelated | Normal | 0.8 | . 18 mul | mul | False | False | 137.6 | 2.31% | 12.2 | 11.88% | 86.7 | Uncorrelated | Normal | -0.6 | . 12 mul | add | False | False | 154.7 | 3.09% | 19.7 | 12.09% | 91.1 | Uncorrelated | Normal | 5.8 | . 15 mul | add | True | False | 170.5 | 3.56% | 20.4 | 12.15% | 91.4 | Uncorrelated | Normal | 3.2 | . 23 mul | mul | True | log | 153.4 | 2.32% | 12.7 | 12.52% | 92.1 | Uncorrelated | Normal | 0.8 | . 20 mul | mul | False | log | 139 | 2.24% | 12.7 | 12.61% | 92.4 | Uncorrelated | Normal | 0.6 | . 14 mul | add | False | log | 138.1 | 2.16% | 12.4 | 12.76% | 92.9 | Uncorrelated | Normal | 0.6 | . 17 mul | add | True | log | 152.7 | 2.16% | 12.4 | 12.76% | 92.9 | Uncorrelated | Normal | 0.6 | . 1 add | add | False | True | 139.6 | 2.26% | 13 | 12.85% | 94.6 | Uncorrelated | Normal | 0.8 | . 4 add | add | True | True | 154.2 | 2.26% | 13 | 12.85% | 94.6 | Uncorrelated | Normal | 0.8 | . 10 add | mul | True | True | 154.8 | 2.32% | 13.2 | 12.77% | 94.7 | Uncorrelated | Normal | 0.8 | . 7 add | mul | False | True | 140.3 | 2.32% | 13.2 | 12.78% | 94.7 | Uncorrelated | Normal | 0.8 | . 6 add | mul | False | False | 142 | 1.78% | 13.8 | 13.52% | 98 | Uncorrelated | Normal | 3.4 | . 16 mul | add | True | True | 154.1 | 2.22% | 12.9 | 13.34% | 98.1 | Uncorrelated | Normal | 1.4 | . 9 add | mul | True | False | 156.5 | 1.78% | 13.8 | 13.53% | 98.1 | Uncorrelated | Normal | 3.4 | . 19 mul | mul | False | True | 139.7 | 2.32% | 13 | 13.28% | 98.7 | Uncorrelated | Normal | 0.6 | . 13 mul | add | False | True | 139.4 | 2.25% | 12.9 | 13.45% | 99.2 | Uncorrelated | Normal | 0.7 | . 22 mul | mul | True | True | 154.6 | 2.31% | 13.1 | 13.40% | 99.5 | Uncorrelated | Normal | 0.7 | . 21 mul | mul | True | False | nan | nan% | nan | nan% | nan | Correlated | Non-Normal | 469.6 | . Above resulting dataframe is sorted in ascending order showing model with lowest test RMSE and %MAPE at the top. Cells highlighted in green are the lowest numbers in their respective columns. Some observations from this result: . Train RMSE is much smaller than test RMSE, showing all the models perform far better on training set than test set | Models with additive trend and BoxCox=log are at the top. This confirms the finding from the EDA that the trend was more than linear. By taking the &#39;log&#39;, trend is linearized and thus &quot;additive&quot; model can be used. | The top model [&quot;add&quot;, &quot;add&quot;, True, False] performed the worst on the train set. AICc is also the highest. | Top 5 models have used &#39;log&#39; transformation and generally have very similar performance on the test set. | All models except one at the bottom as Uncorrelated residuals. Recall that model with uncorrelated residuals has captured as much information as it can from the available data. | All models are biased (non-zero mean). Ideally we want the model to have zero mean but in this case the means are small and should be added to the forecast to correct the bias. | All models have residuals that are normal. This is a useful but not necessary condition. Having a model with normal residuals can make prediction interval calculations easier. | Model selection should always be done by comparing test evaluation metric and not by comparing residual diagnostic metrics. | Top and fourth model has high AICc. Third and fifth have almost same performance. We want to select a parsimonious and simple model. I will select the model with additive seasonality and trend as it has the lowest AICc in the top 5 models. | hw_model = ExponentialSmoothing(train[&quot;Sales&quot;], trend =&quot;add&quot;, seasonal = &quot;add&quot;, seasonal_periods=4, damped=False).fit(use_boxcox=&#39;log&#39;) hw_fitted = hw_model.fittedvalues hw_resid = hw_model.resid #Adding the mean of the residuals to correct the bias. py_hw = hw_model.forecast(len(test[&quot;Sales&quot;])) + np.mean(hw_resid) predictions[&quot;py_hw&quot;] = py_hw #Holt-Winter Parameters hw_model.params_formatted . name param optimized . smoothing_level alpha | 7.538529e-01 | True | . smoothing_slope beta | 3.568007e-09 | True | . smoothing_seasonal gamma | 0.000000e+00 | True | . initial_level l.0 | 6.094620e+00 | True | . initial_slope b.0 | 3.675604e-02 | True | . initial_seasons.0 s.0 | -2.463472e-01 | True | . initial_seasons.1 s.1 | -2.036851e-01 | True | . initial_seasons.2 s.2 | -9.372556e-02 | True | . initial_seasons.3 s.3 | -3.542245e-01 | True | . Note the optimized alpha, beta and gamma parameters. . alpha: 0.75, i.e 75% weight was given to the last observation . | beta: learning parameter for trend. very small. Not much weight is given to the recent trend and trend is obatained from distant past . | gamma: seasonal factor is 0. gamma is usually very small (&lt;0.2) and we want it to be small. If the gamma is high, it can lead to overfitting becuase it means the model is learning too much from recent the recenet data. 0 here indicates seasonality is learned from the earliest season. . | . #Plotting train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) hw_fitted.plot(color=&quot;b&quot;, legend=True, label=&quot;HW_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) predictions[&quot;py_hw&quot;].plot(color=&quot;b&quot;, legend=True, label=&quot;HW_Forecast&quot;); . Cross-Validation . In the above gridsearch, the training set size was fix and we evaluated the model performance by comparing train AICc, RMSE, %MAPE and test RMSE &amp; %MAPE. Test metrics provides true forecast accuracy and should always be used for model selection. This is the preferred approach when the data size is large. But when the time series is short, cross-validation should be used to make sure the model does not overfit the data. The two common approaches are: . Expanding window cross-validation: We start with some initial training &amp; test sets and with each iteration we add one observation to the training set. Forecast errors are calculated with each iteration and averaged to compare model performance. This simulates the performance of the model as we add more observations. For the final forecast we will using all the available history, so using expanding window gives us a good estimate of the forecast accuracy and uncertainty. . | Rolling Window cross-validation: Similar to Expanding but the training size remains same, instead it moves by one observation each time. Training and test lengths remain same. . | Note that AICc, theoretically, provides the same information because it penalizes complex models that overfit. . . #I would like to perform 5 fold cross validation, want the training size to be at #least 12 and test window = forecast horizon 24 - 4 - 5 = 15. Initial training size should be min 12, max 15. #I will choose 15 hw_cv(data[&quot;Sales&quot;], seasonal_periods=4, initial_train_window=15, test_window=4) . add add False False R: 39.9 ,E: 41.3 add add False True R: 43.4 ,E: 51.0 add add False log R: 40.9 ,E: 36.8 add add True False R: 40.7 ,E: 45.9 add add True True R: 38.2 ,E: 45.4 add add True log R: 33.9 ,E: 39.6 add mul False False R: 35.4 ,E: 39.5 add mul False True R: 42.6 ,E: 50.4 add mul False log R: 44.1 ,E: 40.7 add mul True False R: 38.9 ,E: 40.8 add mul True True R: 37.1 ,E: 45.6 add mul True log R: 37.4 ,E: 41.1 mul add False False R: 44.7 ,E: 43.8 mul add False True R: 47.6 ,E: 49.7 mul add False log R: 46.0 ,E: 39.1 mul add True False R: 163.9 ,E: 90.7 mul add True True R: 292.8 ,E: 70.0 mul add True log R: 487.5 ,E: 39.1 mul mul False False R: 42.6 ,E: 38.9 mul mul False True R: 48.9 ,E: 52.3 mul mul False log R: 43.0 ,E: 39.2 mul mul True False R: 78.4 ,E: nan mul mul True True R: nan ,E: 71.2 mul mul True log R: 3186.8 ,E: 39.9 . Residual Check . residcheck(hw_resid, 12); . ** Mean of the residuals: 0.84 ** Ljung Box Test, p-value: 0.3 (&gt;0.05, Uncorrelated) ** Jarque Bera Normality Test, p_value: 0.419 (&gt;0.05, Normal) ** AD Fuller, p_value: 0.0 (&lt;0.05, Stationary) . Accuracy . accuracy(predictions.Sales,predictions[&quot;py_hw&quot;] ) . %MAPE RMSE . 0 11.8 | 85.7 | . Observations: . In general, cross-validation shows that models with high AICc found in gridsearch have higher cv scores. | Multiplicative trend models have high cv scores | The model I chose from the gridsearch (additive trend, seasonality and logged observations) has the lowest expanding cross-validation score. | Plot shows that the model captures the trend, seasonality very well | This model predicts more sales than the actual (in test). From the gridsearch table, we see that the %MAPE is ~12%. Thus, if we use the this model, we should let the stakeholders know that the forecast error is +12% | Residuals are uncorrelated, normal, stationary with a bias (which we already corrected in the forecast) | Model performed better than seasonal naive approach. | ETS . ETS standards for Error, Trend, Seasonality model (I have also seen some refer to it as ExponenTial Smoothing). It is similar to Holt-Winter&#39;s model above but with a State Space statistical framework. In HoltWinter&#39;s, the time series is decomposed into trend and seasonality and exponential weights are used to make the forecast. In a State Space approach, the underlying statistical process is identified and errors are factored in to make the forecast. Holt, Single Exponential Smoothing, Holt-Winter&#39;s, certain ARIMA models can all be categorised into ETS class models. . ETS models follow a taxonomy of ETS(XYZ) where: . X:Error (aka innovations). It can be Additive (A) or Multiplicative (M) | Y:Trend. Trend component can be No trend (N), additive (A), Multiplicative (M) or damped (Ad) | Z:Seasonality, Null (N), additive (A) or multiplicative (M) | . Thus, ETS(ANN) is an exponential model with additive error, no trend, no seasonality (i.e single exponential smoothing) and ETS(MAM) is analogous to Holt-Winter&#39;s method described above. There can be 24 different ETS models based on above combinations but not all combinations of ETS are stable, especially when error is multiplicative. . statsmodels() has a statespace implementation of exponential smoothing method in its tsa.statespace() class. It only has addditive error, additive &amp; damped trend models for now (as of 4/20/2020). Recall that we can convert a multiplicative model into additive by power transform, so this is not a problem. . Using statespace (ETS) can often work better than Holt Winter because of how the parameters are initialized and optimized. This can make a huge difference in the results as we will see. ETS framework also returns 95% predictive interval which HW does not. . Refer to ch.7 of Hyndman&#39;s book for a quick refrence on ETS models and this for more detailed explanation. . For this series, we already know that trend is exponential so I will use logged version of the training set and model ETS(AAA) and ETS(AAdA) . ETS(A,Ad,A) model with Log . #https://www.statsmodels.org/stable/statespace.html# ets_LAdA=sm.tsa.statespace.ExponentialSmoothing(train_log, trend=True, initialization_method= &#39;heuristic&#39;, seasonal=4, damped_trend=True).fit() fc_LAdA = np.exp(ets_LAdA.forecast(6)) #inverting the Log predictions[&quot;LAdA&quot;]=fc_LAdA . #Plotting train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) np.exp(ets_LAdA.fittedvalues).plot(color=&quot;b&quot;, legend=True, label=&quot;Log-AAdA_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) predictions[&quot;LAdA&quot;].plot(color=&quot;b&quot;, legend=True, label=&quot;Log-AAdA_Forecast&quot;); . Accuracy . accuracy(predictions[&quot;Sales&quot;],predictions[&quot;LAdA&quot;]) . %MAPE RMSE . 0 5.1 | 40.9 | . Residual Check . residcheck(ets_LAdA.resid,12); . ** Mean of the residuals: 0.0 ** Ljung Box Test, p-value: 0.346 (&gt;0.05, Uncorrelated) ** Jarque Bera Normality Test, p_value: 0.266 (&gt;0.05, Normal) ** AD Fuller, p_value: 0.712 (&gt;0.05, Non-stationary) . Observations . ETS model performed significantly better than Holt-Winter&#39;s, despite the fact that they both perform exponential smoothing. RSME is 40.9 compared to 86 for HOlt Winter&#39;s. | The primary reason why ETS performs better is the parameter initialization &amp; optimization. Holt Winter&#39;s minimizes the residual error whereas ETS optimizes likelihood. ETS is more likely to obtain global minima faster &amp; more accurately. This doesnt mean ETS will always be better, but in general it should perform better. | Residuals are uncorrelated, normally distributed with 0 mean. | The model struggled initially with a poor fit but it learned the systematic components very well and does an excellent job on the test set | Our observations from EDA and HW model informed the choice to take a log of the training set. In general, it&#39;s always a good idea to try log/BoxCox transform to stablize the variance. | ETS(A,Ad,A) . ets_AAdA=sm.tsa.statespace.ExponentialSmoothing(train, trend=True, initialization_method= &#39;concentrated&#39;, seasonal=4, damped_trend=True).fit() fc_AAdA=ets_AAdA.forecast(len(test)) predictions[&quot;AAdA&quot;]=fc_AAdA . #Plotting train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) ets_AAdA.fittedvalues.plot(color=&quot;b&quot;, legend=True, label=&quot;AAdA_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) predictions[&quot;AAdA&quot;].plot(color=&quot;b&quot;, legend=True, label=&quot;AAdA_Forecast&quot;); . Accuracy . accuracy(predictions[&quot;Sales&quot;],predictions[&quot;AAdA&quot;]) . %MAPE RMSE . 0 4.9 | 43.4 | . Residual Check . residcheck(ets_AAdA.resid,12); . ** Mean of the residuals: 0.4 ** Ljung Box Test, p-value: 0.165 (&gt;0.05, Uncorrelated) ** Jarque Bera Normality Test, p_value: 0.582 (&gt;0.05, Normal) ** AD Fuller, p_value: 1.0 (&gt;0.05, Non-stationary) . Observations: . (A,Ad,A) model without taking log also does very well, slightly worse than with log | Residuals are uncorrelated, look more normal than log version but are slightly biased (0.4 mean) | Fit looks better than logged model | Since we are using the damped model, notice how the trend has slowed down and Q3 forecast is lower than actual sales. This was the main reason I chose this model and the reason will be apparent later as we explore more models. | SARIMA (Seasonal, Auto-Regressive, Integrated, Moving Average Model) . SARIMA (Seasonal ARIMA) is a classical, statistical forecasting method that predicts the forecast values based on past values, i.e lagged values (AR) and lagged errors (MA). Unlike Holt-Winter&#39;s (or ETS), it needs the time series to be stationary before it can be used. That&#39;s where the &quot;Integrated&quot; part comes from. &quot;Integration&quot; means differecning the series to remove trend and make it stationary. You can learn amore about the fundamentals by watching below video. Prof. Shmueli also has an excellent book on Forecasting that I highly recommend. . I also recommend this free Coursera course Practical Time Series Analysis if you want to gain practical and intutitive understanding of ARIMA models. . I will share my main take-aways from these and other resources I have used. . . Auto-regression, AR(p): . As the name suggests, it&#39;s the linear regression with its past values | AR (p) =&gt; Current value = mean + fraction (phi) of yesterday&#39;s value + fraction (phi) of day before yesterday&#39;s value +......+ fraction of pth day&#39;s value + noise | If phi is negaitive =&gt; mean inversion, i.e today&#39;s value will likely go down after yesterday&#39;s peak. | If phi is positive =&gt; Momentum | If phi = 0 =&gt; white noise | If phi = 1 =&gt; random walk | phi has to be between [-1,1] for process to be stationary | If the PACF plot cuts off sharply at lag k, while there is a gradual decay in ACF, it&#39;s a AR(p) process. [Note: I keep PACF and AR(P) mnemonic in mind to know which plot to use for identifying AR process) | An AR(1) model is equivalent to MA(infinity) model, (practially q&gt;&gt;50) | Below video explain AR process really well . . Moving Average ,MA(q): . MA process is not the same as taking moving average of a series | MA process is made up of white noise at different times. In MA(q), q tells us how far back along the sequence of white noise we have to loo for weighted average | For example, in our case if the series is an MA(q) process, the forecast is not affected by the previous sales but rather errors in past forecast. | MA processes are not common but when combined with AR, can produce very accurate forecasts | For an MA(q) model, the forecast beyond 1 period will be the same for rest of the forecast horizon | To identify MA(q) process, plot the ACF. If it sharply cuts off at qth lag, it&#39;s an MA(q) process | Thus, ARIMA (p,d,q) = constant + (weighted sum of last p values) + (weighted sum of last q values of forecast errors) after d differencing . Below are the simulated MA and AR processes. If you run this cell a few times and observe the plots, you will not that it&#39;s not possible to distinguish an AR from MA by just looking at the plots. You need to study the ACF &amp; PACF to know the difference. . #Simulating AR process from statsmodels.tsa.arima_process import ArmaProcess ar = np.array([1,-0.9]) ma = np.array([1, 0.9]) AR = ArmaProcess(ar=ar, ma=None) MA = ArmaProcess(ar=None, ma=ma) simulated_AR= AR.generate_sample(125) simulated_MA= MA.generate_sample(125) fig, (ax1, ax2) = plt.subplots(1, 2) # fig = plt.figure(figsize=(10,8)) fig.suptitle(&#39;Simulated AR &amp; MA Processes&#39;) ax1.plot(simulated_AR); ax2.plot(simulated_MA); . Finding the parameters of the ARIMA process (p,d,q) is an art and science. Generally, p+q &lt;=3. Similar to ARIMA, Seasonal ARIMA (SARIMA) has (P,D,Q) parameters, so SARIMA is (p,d,q)(P,D,Q). p+d+q+P+D+Q &lt;=6 (generally) . Instead of finding the above parameters manually by studying the ACF, PACF, we usually use grid searching just like HW method above. pmdarima is a great library for SARIMA forecasting in Python. It returns the parameters that minimizes AICc and also has cross-validation tools.statsmodels has arma_order_select_ic() for identifying order of the ARMA model but not for SARIMA. . Let&#39;s first take a look at the ACF and PACF to identify potential order of the SARIMA model . ACF plot shows that 1st lag is significant (outside the blue band), the ACFs and PACF both decrease gradually. We will need at least 1 differencing to make the series stationary. When ACF and PACF plots do not have sharp cut offs and significant lags at higher orders, its a indication of ARMA process with seasonality. . plot_acf(train[&quot;Sales&quot;]); plot_pacf(train[&quot;Sales&quot;]); . Using pmdarima() to find the SARIMA order with lowest aicc. This may take a few minutes to run. . (auto_arima(train[&quot;Sales&quot;], seasonal=True, m=4, #seasonality_order 4 d=1, #ACF plot showed we need at least 1 differencing information_criterion=&#39;aicc&#39;). #You can choose AIC, BIC. AICc is corrected AIC summary()) . SARIMAX Results Dep. Variable: y | No. Observations: 18 | . Model: SARIMAX(0, 1, 1)x(1, 1, [], 4) | Log Likelihood -55.434 | . Date: Mon, 20 Apr 2020 | AIC 118.867 | . Time: 12:57:38 | BIC 121.127 | . Sample: 0 | HQIC 118.403 | . - 18 | | . Covariance Type: opg | | . | coef std err z P&gt;|z| [0.025 0.975] . intercept 7.0286 | 1.992 | 3.529 | 0.000 | 3.125 | 10.932 | . ma.L1 -0.9992 | 158.481 | -0.006 | 0.995 | -311.617 | 309.618 | . ar.S.L4 -0.7492 | 0.305 | -2.457 | 0.014 | -1.347 | -0.152 | . sigma2 176.2645 | 2.78e+04 | 0.006 | 0.995 | -5.43e+04 | 5.47e+04 | . Ljung-Box (Q): 8.66 | Jarque-Bera (JB): 1.55 | . Prob(Q): 0.73 | Prob(JB): 0.46 | . Heteroskedasticity (H): 1.17 | Skew: -0.84 | . Prob(H) (two-sided): 0.88 | Kurtosis: 3.24 | . Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step). Observations . pmdarima() has identified the training set as (0, 1, 1)x(1, 1, 0, 4) process. It&#39;s a seasonal AR(1) with d=D=1. Summary also shows that Ljung Box p value (Prob(Q) and JB p value (Prob(JB) are &gt; 0.05 thus residuals are uncorrelated and normally distributed. Summary also shows MA is significant at lag 1, seasonal AR is significant at lag 4. . #Creating SARIMA model in Python using statsmodels sarima_model=(SARIMAX(endog=train[&quot;Sales&quot;], order=(0,1,1), seasonal_order=(1,1,0,4), trend=&#39;c&#39;, enforce_invertibility=False)) sarima_fit=sarima_model.fit() start = len(train) end = len(train) +len(test) -1 sarima_fitted = sarima_fit.fittedvalues sarima_resid = sarima_fit.resid py_sarima = sarima_fit.predict(start, end, dynamic=False) predictions[&quot;py_sarima&quot;] = py_sarima sarima_fit.plot_diagnostics(); . Residual Check . SARIMAX() has its own residual diagnostics (shown above). It shows the residuals are normally distributed, uncorrelated. Q-Q plot shows an outlier in the lower left but otherwise everyting looks good. . Accuracy . accuracy(predictions.Sales,py_sarima) . %MAPE RMSE . 0 12.6 | 94.4 | . Observations: . SARIMA did worse than Holt-Winter&#39;s method. RMSE for HW was 85.7 | SARIMA2 - (Using Logged value) . Recall that one of the observations from the HW method was log models performed better, so I will try log of the trainining set. The forecast will be logged values so I will inverse it with np.exp() . #Fitting model to log of train (auto_arima(np.log(train[&quot;Sales&quot;]), seasonal=True, m=4, #seasonality_order 4 information_criterion=&#39;aicc&#39;). #You can choose AIC, BIC. AICc is corrected AIC summary()) . SARIMAX Results Dep. Variable: y | No. Observations: 18 | . Model: SARIMAX(1, 1, 0, 4) | Log Likelihood 26.993 | . Date: Mon, 20 Apr 2020 | AIC -47.986 | . Time: 12:57:49 | BIC -46.068 | . Sample: 0 | HQIC -48.163 | . - 18 | | . Covariance Type: opg | | . | coef std err z P&gt;|z| [0.025 0.975] . intercept 0.2848 | 0.032 | 8.979 | 0.000 | 0.223 | 0.347 | . ar.S.L4 -0.8106 | 0.163 | -4.966 | 0.000 | -1.131 | -0.491 | . sigma2 0.0009 | 0.001 | 1.357 | 0.175 | -0.000 | 0.002 | . Ljung-Box (Q): 20.24 | Jarque-Bera (JB): 0.47 | . Prob(Q): 0.09 | Prob(JB): 0.79 | . Heteroskedasticity (H): 0.40 | Skew: -0.24 | . Prob(H) (two-sided): 0.34 | Kurtosis: 2.23 | . Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step). It&#39;s a SARIMA model with no (p,d,q). (P,D,Q,m) = (1,1,0,4) . Prob(Q) &gt; 0.05 and Prob(JB) &gt; 0.05 Thus, residuals are uncorrelated and normal ! . sarima_logmodel=(SARIMAX(np.log(train[&quot;Sales&quot;]), order=(0,0,0), seasonal_order=(1,1,0,4), trend=&#39;c&#39;, enforce_invertibility=False)).fit() sarima_log = np.exp(sarima_logmodel.predict(start, end)) predictions[&quot;sarima_log&quot;] = sarima_log slog_fitted = np.exp(sarima_logmodel.fittedvalues) . Residual Check . sarima_logmodel.plot_diagnostics(); . Observations . Residuals are not stationary but are normally distributed and are uncorrelated | There is an outlier in the left tail in the Q-Q plot. This is expected, since we took 1 seasonal differencing so model did not fit well to the early data. | Accuracy . accuracy(predictions.Sales,sarima_log ) . %MAPE RMSE . 0 11.1 | 81.7 | . RMSE &amp; %MAPE are now slightly better than the HW model !!! Plot below shows model did not fit well in the early data but performs well on validation set. . #Plotting train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) slog_fitted.plot(color=&quot;b&quot;, legend=True, label=&quot;HW_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) predictions[&quot;sarima_log&quot;].plot(color=&quot;b&quot;, legend=True, label=&quot;LogSARIMA_forecast&quot;); . . Note: There is a misconception that ARIMA is a more accurate method that ETS/Holt-Winters. That&#8217;s not accurate. In this example, ARIMA worked better but that may not always be the case and you won&#8217;t know until you experiment. . Facebook Prophet . Facebook described Prophet library as below in their documentation: . &quot; Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.&quot; . Video below gives great overview of this package. . . My main take aways are: . Prophet was built for high frequency data like daily, hourly, minute etc.. It may not work very well on monthly, quarterly data, but you won&#39;t know until you try. | In addition to forecasting, it also provide changepoints, anomalies which are great for detecting sudden changes in the time series | Prof. Kourentzes tested Prophet along with other methods (ETS, SARIMA) on M3 competition data and found that Prophet performed poorly. | ETS/HW &amp; SARIMA cannot work with multiple seasonalities &amp; high frequency data. Prophet can also include effect of holidays. | Prophet requires the data to be in specific format. Dataframe must have time column ds and time series observations in column y | Though Prophet is designed mainly for high frequency data, it can be used for monthly/quarterly/yearly data with some tweaks. | from fbprophet import Prophet data_fb = data.reset_index() data_fb.columns=[&#39;ds&#39;,&#39;y&#39;] #create new df with columns ds &amp; y train_fb, test_fb = data_fb.iloc[:-len(test)], data_fb.iloc[-len(test):] #create train &amp; test df . #Fit the model to train fb1_model=Prophet(weekly_seasonality=False, daily_seasonality=False, n_changepoints=10, seasonality_mode=&quot;multiplicative&quot;).fit(train_fb) #I tried &quot;additive too&quot;, it was slightly worse #Prophet results are saved to a dataframe using make_future_dataframe() fb1_df=fb1_model.make_future_dataframe(6, freq=&#39;Q&#39;) #set the freq argument to &#39;Q&#39; for quarterly data #We only need &quot;ds&quot; and &quot;yhat&quot; columns.. &quot;ds&quot; is the date column and &quot;yhat&quot; are predictions fb1_fc_df=fb1_model.predict(fb1_df)[[&quot;ds&quot;,&quot;yhat&quot;]] fb1_fc__=fb1_model.predict(fb1_df) #Residuals fb1_resid = train[&quot;Sales&quot;].values - fb1_fc_df[&#39;yhat&#39;].iloc[:len(train)] fb1_fc = fb1_fc_df.iloc[-len(test):] predictions[&quot;fb1&quot;] = fb1_fc[&quot;yhat&quot;].values . fb1_fc_df.head() . ds yhat . 0 2012-03-31 | 362.0 | . 1 2012-06-30 | 385.0 | . 2 2012-09-30 | 432.0 | . 3 2012-12-31 | 341.0 | . 4 2013-03-31 | 382.0 | . Accuracy . accuracy(test[&quot;Sales&quot;],fb1_fc[&quot;yhat&quot;].values) . %MAPE RMSE . 0 8.3 | 65.8 | . Prophet performed significantly better than HW &amp; SARIMA, that too on quarterly data ! I didn&#39;t expect that given how extensively it&#39;s been proven that Prophet does not work well on low frequency data. This is still not as good as the ETS models. . train[&quot;Sales&quot;].plot(figsize=(12,8), style=&quot;--&quot;, color=&quot;gray&quot;, legend=True, label=&quot;Train&quot;) fb1_fc_df.set_index(&#39;ds&#39;)[&quot;yhat&quot;].iloc[:-len(test)].plot(color=&quot;b&quot;, legend=True, label=&quot;Prophet_Fitted&quot;) predictions[&quot;Sales&quot;].plot(style=&quot;--&quot;,color=&quot;r&quot;, legend=True, label=&quot;Test&quot;) fb1_fc_df.set_index(&#39;ds&#39;)[&quot;yhat&quot;].iloc[-len(test):].plot(color=&quot;b&quot;, legend=True, label=&quot;Prophet_forecast&quot;); . Prophet has its own plotting method. As you can see the main drawback with this model is how wide the confidence interval is. . fb1_model.plot(fb1_fc__); . residcheck(fb1_resid,12); . ** Mean of the residuals: -0.0 ** Ljung Box Test, p-value: 0.402 (&gt;0.05, Uncorrelated) ** Jarque Bera Normality Test, p_value: 0.003 (&lt;0.05, Not-normal) ** AD Fuller, p_value: 0.969 (&gt;0.05, Non-stationary) . Observations: . Prophet under forecasts while other methods were over the actual values in the validation set | Prophet captured the overall trend and seasonality well | Above plot also shows that Prophet overfitted the training set. I am sure there are hyperparameters that could be tuned but most the parameters are for high frequency data. Also, Prophet has a cross-validation method built-in, but it only accepts daily or sub-daily data. That&#39;s a limitation. | Residuals are uncorrelated but not-normal, which is ok. | Uncertainty band is very wide. | Forecasts . We evaluated four different forecasting methods: . Seasonal Naive | Holt-Winter&#39;s (Triple Exponential Smoothing) | ETS (Log-AAdA, AAdA) | SARIMA | Prophet | . ETS gave the most accurate forecast, followed by Prophet.. Here&#39;s how the point forecasts compare with each other and the test set: . predictions.round(0) . Sales py_snaive py_hw LAdA AAdA py_sarima sarima_log fb1 . Date . 2016-09-30 773.0 | 681.0 | 813.0 | 761.0 | 725.0 | 783.0 | 797.0 | 717.0 | . 2016-12-31 592.0 | 557.0 | 650.0 | 620.0 | 613.0 | 678.0 | 650.0 | 518.0 | . 2017-03-31 627.0 | 628.0 | 751.0 | 699.0 | 687.0 | 755.0 | 743.0 | 654.0 | . 2017-06-30 725.0 | 707.0 | 813.0 | 781.0 | 724.0 | 811.0 | 803.0 | 716.0 | . 2017-09-30 854.0 | 681.0 | 941.0 | 845.0 | 785.0 | 911.0 | 933.0 | 762.0 | . 2017-12-31 661.0 | 557.0 | 753.0 | 688.0 | 671.0 | 799.0 | 762.0 | 572.0 | . forecasts = predictions.copy() fc_melt=pd.melt(forecasts.reset_index(), id_vars=&#39;Date&#39;, value_vars=forecasts.columns, var_name=&quot;Model&quot;, value_name=&quot;Forecasts&quot;).round(0) fc_melt.head() . Date Model Forecasts . 0 2016-09-30 | Sales | 773.0 | . 1 2016-12-31 | Sales | 592.0 | . 2 2017-03-31 | Sales | 627.0 | . 3 2017-06-30 | Sales | 725.0 | . 4 2017-09-30 | Sales | 854.0 | . Interactive Chart . #Ref:https://altair-viz.github.io/gallery/multiline_tooltip.html # Create a selection that chooses the nearest point &amp; selects based on x-value nearest = alt.selection(type=&#39;single&#39;, nearest=True, on=&#39;mouseover&#39;, fields=[&#39;Date&#39;], empty=&#39;none&#39;) # The basic line line = alt.Chart(fc_melt).mark_line(point=True).encode( x=&#39;Date&#39;, y=alt.Y(&#39;Forecasts:Q&#39;,scale=alt.Scale(domain=[500,1000], clamp=True)), color=&#39;Model:N&#39;, tooltip=[&#39;Date&#39;,&#39;Forecasts&#39;,&#39;Model&#39;] ) # Transparent selectors across the chart. This is what tells us # the x-value of the cursor selectors = alt.Chart(fc_melt).mark_point().encode( x=&#39;Date&#39;, opacity=alt.value(0), ).add_selection( nearest ) # Draw points on the line, and highlight based on selection points = line.mark_point().encode( opacity=alt.condition(nearest, alt.value(1), alt.value(0)) ) # Draw text labels near the points, and highlight based on selection text = line.mark_text(align=&#39;left&#39;, baseline=&#39;top&#39;, dx=5, dy=-5).encode( text=alt.condition(nearest, &#39;Forecasts:Q&#39;, alt.value(&#39; &#39;)) ) text2 = line.mark_text(align=&#39;left&#39;, baseline=&#39;bottom&#39;, dx=5, dy=-5).encode( text=alt.condition(nearest, &#39;Model:N&#39;, alt.value(&#39; &#39;)) ) # Draw a rule at the location of the selection rules = alt.Chart(fc_melt).mark_rule(color=&#39;gray&#39;).encode( x=&#39;Date&#39;, ).transform_filter( nearest ) # Put the five layers into a chart and bind the data alt.layer( line, selectors, points, rules, text, text2 ).properties( width=800, height=500, title=&quot;Comaprison of various Forecasting Models&quot; ).interactive() . In the above chart, red line is the test set and rest are forecasts. You can zoom-in/out, pan to inspect the fit over the test set. . SNaive &amp; Prophet are below the test while SARIMA and HW are above. AAdA is also below actual in Q3. | HW &amp; SARIMA are almost identical | If we had to pick a single best model, we would use the Log AAdA model which has the lowest RMSE and tracks the actual values very closely. | SARIMA did well on the test set but not so good on the early part of the training set. That&#39;s acceptable but the point is that all these methods did well in training or test. Thus, a better approach is to create an ensemble forecast that combines all or some of these forecasts together. | Many research studies have shown that forecast combination often provides a more robust, accurate forecast that&#39;s less susceptible to overfitting. You can read more here | Ensemble Forecasts . Although there are many different ways to combine forecasts, simple averaging often works as good as a more complex methods and is easier to implement/monitor/debug. As we saw above, some forecasts are above the test and some are below. So hopefully averaging will bring it closer to the actual values. . We have 7 different models with more than 200 possible combinations. Let&#39;s compare their RSME with fc_combo() function I wrote. This function averages the forecasts and calculates the RSME. . . Note: This function uses mean() to average the forecasts but you should try median() as well. Depending on the data and the skewness, meadian() might work better in some cases. . forecasts . Sales py_snaive py_hw LAdA AAdA py_sarima sarima_log fb1 . Date . 2016-09-30 773.0 | 681.0 | 812.646500 | 760.681119 | 724.566985 | 782.950175 | 797.175115 | 716.954330 | . 2016-12-31 592.0 | 557.0 | 649.896932 | 620.038936 | 612.542359 | 677.965781 | 649.770583 | 517.539078 | . 2017-03-31 627.0 | 628.0 | 750.899727 | 698.970622 | 687.029723 | 755.245430 | 743.222703 | 654.222708 | . 2017-06-30 725.0 | 707.0 | 812.897039 | 780.999108 | 724.246686 | 810.558120 | 802.853193 | 715.815974 | . 2017-09-30 854.0 | 681.0 | 941.221318 | 845.246326 | 784.520604 | 910.805446 | 932.851318 | 761.781268 | . 2017-12-31 661.0 | 557.0 | 752.695160 | 687.517889 | 671.296876 | 798.603878 | 762.493309 | 571.860845 | . fc_combo(forecasts.iloc[:,1:]) . (&#39;py_snaive&#39;,) RMSE ==&gt; 92.0 (&#39;py_hw&#39;,) RMSE ==&gt; 85.7 (&#39;LAdA&#39;,) RMSE ==&gt; 40.9 (&#39;AAdA&#39;,) RMSE ==&gt; 43.4 (&#39;py_sarima&#39;,) RMSE ==&gt; 94.4 (&#39;sarima_log&#39;,) RMSE ==&gt; 81.7 (&#39;fb1&#39;,) RMSE ==&gt; 65.8 (&#39;py_snaive&#39;, &#39;py_hw&#39;) RMSE ==&gt; 36.1 (&#39;py_snaive&#39;, &#39;LAdA&#39;) RMSE ==&gt; 48.6 (&#39;py_snaive&#39;, &#39;AAdA&#39;) RMSE ==&gt; 61.8 (&#39;py_snaive&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 43.4 (&#39;py_snaive&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 36.1 (&#39;py_snaive&#39;, &#39;fb1&#39;) RMSE ==&gt; 77.2 (&#39;py_hw&#39;, &#39;LAdA&#39;) RMSE ==&gt; 60.3 (&#39;py_hw&#39;, &#39;AAdA&#39;) RMSE ==&gt; 49.3 (&#39;py_hw&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 89.0 (&#39;py_hw&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 83.5 (&#39;py_hw&#39;, &#39;fb1&#39;) RMSE ==&gt; 35.1 (&#39;LAdA&#39;, &#39;AAdA&#39;) RMSE ==&gt; 37.6 (&#39;LAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 65.3 (&#39;LAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 58.5 (&#39;LAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 37.1 (&#39;AAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 56.8 (&#39;AAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 48.4 (&#39;AAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 47.4 (&#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 87.5 (&#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 38.7 (&#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 33.5 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;) RMSE ==&gt; 36.1 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;AAdA&#39;) RMSE ==&gt; 37.3 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 46.8 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 42.0 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;fb1&#39;) RMSE ==&gt; 39.3 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;) RMSE ==&gt; 45.2 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 40.9 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 35.8 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 52.0 (&#39;py_snaive&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 42.8 (&#39;py_snaive&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 37.8 (&#39;py_snaive&#39;, &#39;AAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 60.9 (&#39;py_snaive&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 46.4 (&#39;py_snaive&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 41.7 (&#39;py_snaive&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 39.6 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;) RMSE ==&gt; 46.0 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 71.0 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 67.1 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.0 (&#39;py_hw&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 63.4 (&#39;py_hw&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 58.8 (&#39;py_hw&#39;, &#39;AAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 33.5 (&#39;py_hw&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 86.5 (&#39;py_hw&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 49.5 (&#39;py_hw&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 46.0 (&#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 50.8 (&#39;LAdA&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 45.4 (&#39;LAdA&#39;, &#39;AAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.9 (&#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 70.0 (&#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 38.9 (&#39;LAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 35.0 (&#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 62.8 (&#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 37.7 (&#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 33.3 (&#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 48.5 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;) RMSE ==&gt; 36.4 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 45.3 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 41.7 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 35.3 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 42.7 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 38.7 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;AAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 38.7 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 53.3 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.4 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 33.6 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 40.4 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 36.5 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 47.5 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 44.9 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 37.6 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 35.3 (&#39;py_snaive&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 42.7 (&#39;py_snaive&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 41.4 (&#39;py_snaive&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 39.1 (&#39;py_snaive&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.2 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 57.0 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 53.5 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 34.4 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 73.5 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 46.9 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 44.1 (&#39;py_hw&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 67.4 (&#39;py_hw&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 42.8 (&#39;py_hw&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 39.5 (&#39;py_hw&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 56.6 (&#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 56.5 (&#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 37.5 (&#39;LAdA&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 34.1 (&#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 46.2 (&#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 42.4 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;) RMSE ==&gt; 42.1 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 39.0 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;fb1&#39;) RMSE ==&gt; 35.8 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 50.5 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.7 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 34.3 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 47.2 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.5 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 34.0 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 40.2 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 42.0 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 38.2 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.0 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.5 (&#39;py_snaive&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.6 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 61.3 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 42.3 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 39.6 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 52.8 (&#39;py_hw&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 48.7 (&#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 41.9 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;) RMSE ==&gt; 46.0 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.5 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 34.3 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;LAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 40.2 (&#39;py_snaive&#39;, &#39;py_hw&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 38.5 (&#39;py_snaive&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 36.5 (&#39;py_hw&#39;, &#39;LAdA&#39;, &#39;AAdA&#39;, &#39;py_sarima&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) RMSE ==&gt; 47.1 . Observations: . The best performing model is (&#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) with RMSE = 33.3 which is better than the best solo model LogAAdA (RSME:40) but not by very much. | Notice how some of the individual models that gave very poor forecasts (Seasonal Naive, HW, SARIMA) but when combined with other forecasts, performed very well. Similarly the models that performed the best LogAAdA, SARIMA_Log did not create the best combination forecast. | Diversity of models in forecast combination makes the forecast more robust, typically produce normally distributed errors and narrower prediction intervals. Follow the research work by Prof. Kourentzes for theory and practical advise on forecast combination. | More models in combination does not mean better results. When selecting combinations, focus should be on divertsity of the models selected (different class of models). Some researchers (Armstrong et al) have suggested using 5 models. | This shows the value of experimentation in any statistical /Machine Learning project. | Final Model . For the final model, we use all the data and not just the training set. If we want a single best model we will choose the Log ETS(A,Ad,A). The best combination model is (&#39;AAdA&#39;, &#39;sarima_log&#39;, &#39;fb1&#39;) which has the lowest RMSE. The difference between them is not significant when accuracy is compared. In the next blog, I am going to demonstrate how to deploy this forecasting model in PowerBI &amp; AzureML so to keep things simple, I will use the Log ETS(A,Ad,A) model. Also, ETS allows simulations which can be used for aggregate forecasting and communicating uncertainty. Forecast combination is a fascinating topic and deserves a seperate blog post. . ETS Model . ets_model=sm.tsa.statespace.ExponentialSmoothing(np.log(data), trend=True, initialization_method= &#39;heuristic&#39;, seasonal=4, damped_trend=True).fit() results = ets_model.get_forecast(4) results_df=results.summary_frame(alpha=0.05).apply(np.exp)[[&quot;mean&quot;,&quot;mean_ci_lower&quot;,&quot;mean_ci_upper&quot;]] results_df.round(1) . Sales mean mean_ci_lower mean_ci_upper . 2018-03-31 692.5 | 599.8 | 799.5 | . 2018-06-30 799.1 | 692.1 | 922.6 | . 2018-09-30 939.4 | 813.6 | 1084.6 | . 2018-12-31 725.6 | 628.5 | 837.8 | . fig, ax = plt.subplots() results_df[&#39;mean&#39;].plot(ax=ax, legend=True, label=&quot;Forecast&quot; ) ax.fill_between(results_df.index, results_df[&#39;mean_ci_lower&#39;], results_df[&#39;mean_ci_upper&#39;], alpha=0.2, color=&#39;gray&#39;) data[&quot;Sales&quot;].plot(legend=True, label=&quot;Data&quot;, figsize=(12,8)); . Forecast Uncertainty . Forecasts are based on history and are probabilistic. When creating and communicating the forecasts, it is important that the stakeholders who will be using the forecasts understand the uncertainty in the estimates. Forecasts have four sources of uncertainties: . Random error | Parameters used in the model | Model choice | Data uncertainty | While it&#39;s not possible to estimate all sources of uncertainties, the forecaster should provide enough information/tools to the stakeholders to understand and gauge the risk in using the forecast. Stochastic simulation can be used for risk modeling. . Simulation . Imagine that the stakeholders are interested in the aggregate sales for 2018, i.e the total sales and not just the quarterly sales. The obvious way is to calculate quarterly forecasts, like we did, and then summing them up to get annual sales. While this may provide a good estimate, it will not be correct. Since the quarterly forecasts are distributions, we need to account for the variance in those distributions when calculating the aggregates. If the forecasts were based on median or percentile, we cannot simply add them up. Also, it&#39;s possible that not all forecasts are from normal distributions and thus cannot be added. . Monte Carlo simulation can be used to overcome these challenges. Below we will generate 5000 future forecasts based on the ETS model we have and then calculate the aggregates, prediction intervals and cumulative distribution function for it. . #Simulating 5000 futures sim_frame=ets_model.simulate(4, anchor=&#39;2018-03-31&#39;, repetitions=5000 ).T.reset_index().iloc[:,2:].apply(np.exp).round(1) sim_frame[&quot;aggregate&quot;] = sim_frame.sum(axis=1) sim_frame.tail() . 2018-03-31 00:00:00 2018-06-30 00:00:00 2018-09-30 00:00:00 2018-12-31 00:00:00 aggregate . 4995 632.1 | 858.9 | 868.6 | 804.4 | 3164.0 | . 4996 732.0 | 741.2 | 1019.0 | 698.6 | 3190.8 | . 4997 721.6 | 914.4 | 938.7 | 785.9 | 3360.6 | . 4998 627.2 | 868.6 | 882.2 | 728.8 | 3106.8 | . 4999 585.7 | 905.0 | 873.0 | 765.6 | 3129.3 | . mean_2018_sales = np.mean(sim_frame[&quot;aggregate&quot;]) print(&quot;25th percntile Sales for FY2018:&quot;,np.quantile(sim_frame[&quot;aggregate&quot;], 0.25).round(0)) print(&quot; nAvg Sales for FY2018, Simulation:&quot;,mean_2018_sales.round(0)) print(&quot; n75th percntile Sales for FY2018:&quot;,np.quantile(sim_frame[&quot;aggregate&quot;], 0.75).round(0)) print(&quot; nFY2018 Based on Quarterly Forecasts:&quot;,results_df[&quot;mean&quot;].sum().round(0)) . 25th percntile Sales for FY2018: 3084.0 Avg Sales for FY2018, Simulation: 3163.0 75th percntile Sales for FY2018: 3241.0 FY2018 Based on Quarterly Forecasts: 3157.0 . As you can see, if we just summed up the quarterly forecasts, the aggregate for 2018 would be 3157, and if we used the simulations it would be 3163. In this case the difference is not much and is within the forecast error but that may not always be the case. . When communicating forecasts (or any other uncertainty), providing range is a better solution than providing point estimates. e.g. we calculated the 25% &amp; 75% percentile for the aggregate forecast. Thus, we could say that we are 50% confident that the FY2018 sales would be between 3083 and 3240.i.e P50 [3083 , 3240]. This can also be interpreted as there is only 25% chance that Sales would be &gt;3240 next year. This conveys the uncertainty far better than point estimates. . Confidence Interval vs. Prediction Interval . It&#39;s common to calculate and show 95% confidence around the mean forecast. 95% is a conventional measure and doesn&#39;t have any particular significance. Confidence interval (CI)is not the same as prediction interval (PI). CI is often confused as a measure of uncertainty and is used to show the upper and lower bound for the estimate - it&#39;s not! . Confidence interval is around the populatio point estimate. e.g. if we calculate the mean, to convey the error in calculating it we compute the CI. In our case the mean for Q1 is 692.5 and lower and upper CI bound are [599.8,799.5]. We are 95% confident that the mean will be within this bound. This doesn&#39;t say the forecast will be between these bounds with 95% confidence. Think of CI as error around the population point estimate because of limited sample size. CI will be lower if increase the sample size or lower the alpha. Point estimate means calculating a single number e.g. mean, median, percentile etc. about the population/sample. Also make a note if the CI is calculated using parameters of the models (residuals). . To calculate the upper and lower bound on the forecast, we need prediction interval. PI can be calculated using the forecast errors or simulations. PI gives us the upper and lower bounds based on the model we have created. By definition PI &gt; CI. In our examle, the 97.5th percentile is 923 and 2.5% percentils is 691. Now we can be 95% confident that the forecast for Q1 2018 will be between 923 and 691 ! . Many years ago at the end of my Business Simulations class in grad school, Prof.Nelson said the main take-away from his class should be the difference between PI and CI and correct use/interpretation of CI. I hope after reading this blog, among other things, you will also learn to interprete CI correctly, especially in the context of forecasting. CI is useless for forecasting. . q1_975=np.quantile(sim_frame.iloc[:,1],0.975) q1_025=np.quantile(sim_frame.iloc[:,1],0.025) print(&quot;PI_95%:&quot;, q1_95.round(1)) print(&quot;PI_5%:&quot;, q1_05.round(1)) . PI_95%: 923.3 PI_5%: 691.3 . Cumulative Distribution Function . #Reference: http://stanford.edu/~raejoon/blog/2017/05/16/python-recipes-for-cdfs.html num_bins = 100 counts, bin_edges = np.histogram (sim_frame[&quot;aggregate&quot;], bins=num_bins, normed=True) cdf = np.cumsum (counts) cdf_x = bin_edges[1:] cdf_y = cdf/cdf[-1] cdf_df = pd.DataFrame({&#39;Sales&#39;:cdf_x, &#39;Percentile&#39;:cdf_y}) cdf_df.head() . Sales Percentile . 0 2798.317 | 0.0002 | . 1 2806.334 | 0.0002 | . 2 2814.351 | 0.0006 | . 3 2822.368 | 0.0008 | . 4 2830.385 | 0.0010 | . #collapse-hide # Create a selection that chooses the nearest point &amp; selects based on x-value nearest = alt.selection(type=&#39;single&#39;, nearest=True, on=&#39;mouseover&#39;, fields=[&#39;Sales&#39;], empty=&#39;none&#39;) # The basic line line = alt.Chart(cdf_df).mark_line(interpolate=&#39;basis&#39;).encode( x=&#39;Sales:Q&#39;, y=&#39;Percentile:Q&#39;, tooltip = [&quot;Sales&quot;, &quot;Percentile&quot;]) # Transparent selectors across the chart. This is what tells us # the x-value of the cursor selectors = alt.Chart(cdf_df).mark_point().encode( x=&#39;Sales:Q&#39;, opacity=alt.value(0), ).add_selection( nearest ) # Draw points on the line, and highlight based on selection points = line.mark_point().encode( opacity=alt.condition(nearest, alt.value(1), alt.value(0)) ) # Draw text labels near the points, and highlight based on selection text1 = line.mark_text(align=&#39;left&#39;, baseline=&quot;bottom&quot;,dx=5, dy=-5).encode( text=alt.condition(nearest, &#39;Percentile:Q&#39;, alt.value(&#39; &#39;)) ) text2 = line.mark_text(align=&#39;left&#39;, baseline=&quot;top&quot;, dx=5, dy=-5).encode( text=alt.condition(nearest, &#39;Sales:Q&#39;, alt.value(&#39; &#39;)) ) # Draw a rule at the location of the selection rules = alt.Chart(cdf_df).mark_rule(color=&#39;gray&#39;).encode( x=&#39;Sales:Q&#39;, ).transform_filter( nearest ) # Put the five layers into a chart and bind the data alt.layer( line, selectors, points, rules, text1, text2 ).properties( width=600, height=300, title =&quot;FY2018 Sales Probability &quot; ).configure_axis(grid=False) . . This is a dynamic and interactive way for the stakeholders to understand forecast uncertainty. For example, if this French retailer has a Revenue target at least 3300, they have only 12% (1-0.88) chance of achiveing that based on this forecating model. The stakeholders will find this more beneficial for decision making than the typical static forecast chart. You could create similar chart for all quarters and make it more interactive. Power BI can be used for that. . Forecast Using R . Import rpy2 . Use pandas2ri.activate() to convert pandas to R dataframe . %load_ext rpy2.ipython to use %%R magic command to run R in a cell . import rpy2 import warnings warnings.filterwarnings(&#39;ignore&#39;) from rpy2.robjects import pandas2ri import rpy2.rinterface as rinterface pandas2ri.activate() %load_ext rpy2.ipython . R-ETS . ets() package in R uses a state space approach rather than Holt-Winter&#39;s (which can be modeled in ETS form). You can read more here . %%R -i data -o fets,fc_ets library(fpp2) r_train &lt;- ts(data$Sales, start=c(2012,01), frequency=4) fets &lt;- r_train %&gt;% ets() fc_ets&lt;-r_train %&gt;% ets() %&gt;% forecast(h=4) %&gt;% summary() r_train %&gt;% ets() %&gt;% forecast(h=4) %&gt;% autoplot() . print(fets,end=&quot;&quot;) . ETS(M,A,M) Call: ets(y = .) Smoothing parameters: alpha = 0.1564 beta = 1e-04 gamma = 2e-04 Initial states: l = 339.3571 b = 16.5403 s = 0.8781 1.1272 1.0301 0.9646 sigma: 0.05 AIC AICc BIC 241.8271 254.6843 252.4296 . ets() returned ETS(M,A,M) as the best model based on AICc, i.e &quot;multiplicative&quot; error, &quot;additive&quot; trend and &quot;multiplicative&quot; error. We also obtained same results in Python. But because we used a log transform the multiplicative term became additive. . fc_ets . Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 . 0 737.458963 | 690.235833 | 784.682093 | 665.237417 | 809.680508 | . 1 804.569111 | 752.445770 | 856.692452 | 724.853340 | 884.284882 | . 2 899.154431 | 840.263229 | 958.045633 | 809.088110 | 989.220753 | . 3 714.953180 | 667.641498 | 762.264862 | 642.596206 | 787.310153 | . R-SARIMA . %%R -i train -o fsarima r_train &lt;- ts(train$Sales, start=c(2012,01), frequency=4) fsarima &lt;- r_train %&gt;% auto.arima(stepwise=FALSE) . %%R -i data -o fc_arima r_data &lt;- ts(data$Sales, start=c(2012,01), frequency=4) fc_arima&lt;- r_data %&gt;% auto.arima(stepwise=FALSE) %&gt;% forecast(h=4) %&gt;% summary() r_data %&gt;% auto.arima(stepwise=FALSE) %&gt;% forecast(h=4) %&gt;% autoplot() . print(fsarima, end=&quot;&quot;) . Series: . ARIMA(0,1,0)(1,1,0)[4] Coefficients: sar1 -0.6495 s.e. 0.2501 sigma^2 estimated as 433: log likelihood=-58.48 AIC=120.96 AICc=122.16 BIC=122.09 . We obtained the same SARIMA order as Python ARIMA(0,1,0)(1,1,0)[4] . Conclusion . We explored various time series forecasting methods, model selection procedures and created an ensemble forecast that can be used to improve the forecast accuracy. We also briefly covered communicating uncertainty in forecasts. I did not include any deep-learning methods as they generally do well on large dataset with multiple features. Classical methods such as HW/ETS &amp; ARIMA provide more interpretable results which can prove useful for understanding the behaviour of the time time series and make business decisions. In the next blog I will cover deploying this model in PowerBI. . References: . Forecasting: Principles and Practice, by Prof. Hyndman | Time Series Analysis and its Applications, by Robert Shumway | Time Series Analysis and Forecasting, by Montgomery &amp; Jennings | Introduction to Time Series and Analysis, by Brockwell | Practial Time Series Forecasting with R, by Galit Shmueli 6. https://homepage.univie.ac.at/robert.kunst/pres09_prog_turyna_hrdina.pdf |",
            "url": "https://pawarbi.github.io/blog/forecasting/r/python/rpy2/altair/fbprophet/ensemble_forecast/uncertainty/simulation/2020/04/21/timeseries-part2.html",
            "relUrl": "/forecasting/r/python/rpy2/altair/fbprophet/ensemble_forecast/uncertainty/simulation/2020/04/21/timeseries-part2.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post44": {
            "title": "Time series Forecasting in Python & R, Part 1 (EDA)",
            "content": "Overview . This is a quarterly sales data of a French retail company from Prof. Rob Hyndman&#39;s &quot;Forecasting Methods &amp; Applications&quot; book. I have uploaded the data to my github.The goals for this first part are: . Exploratory data analysis of the time series | Explain the time series behaviour in qualitative and quantitative terms to build intuition for model selection | Identify the candidate models and possible model parameters that can be used based on the findings in the EDA | Importing libraries . #collapse-hide #Author: Sandeep Pawar #Version: 1.0 #Date Mar 27, 2020 import pandas as pd import numpy as np import itertools #Plotting libraries import matplotlib.pyplot as plt import seaborn as sns import altair as alt plt.style.use(&#39;seaborn-white&#39;) %matplotlib inline #statistics libraries import statsmodels.api as sm import scipy from scipy.stats import anderson from statsmodels.tools.eval_measures import rmse from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import month_plot, seasonal_plot, plot_acf, plot_pacf, quarter_plot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing from statsmodels.stats.diagnostic import acorr_ljungbox as ljung #from nimbusml.timeseries import SsaForecaster from statsmodels.tsa.statespace.tools import diff as diff import pmdarima as pm from pmdarima import ARIMA, auto_arima from scipy import signal from scipy.stats import shapiro from scipy.stats import boxcox from sklearn.preprocessing import StandardScaler #library to use R in Python import rpy2 from rpy2.robjects import pandas2ri pandas2ri.activate() import warnings warnings.filterwarnings(&quot;ignore&quot;) np.random.seed(786) . . . Note: I have found that results could be significanlty different if you use different versions of the libraries, especially with statsmodels. If you want to reproduce these results, be sure to use the same versions of these libraries. For this project, I created a conda virtual environment as rpy2 requires specific versions of Pandas &amp; certain R libraries . #Printing library versions print(&#39;Pandas:&#39;, pd.__version__) print(&#39;Statsmodels:&#39;, sm.__version__) print(&#39;Scipy:&#39;, scipy.__version__) print(&#39;Rpy2:&#39;, rpy2.__version__) . Pandas: 0.25.0 Statsmodels: 0.11.0 Scipy: 1.4.1 Rpy2: 2.9.4 . #collapse-hide # Define some custom functions to help the analysis def MAPE(y_true, y_pred): &quot;&quot;&quot; %Error compares true value with predicted value. Lower the better. Use this along with rmse(). If the series has outliers, compare/select model using MAPE instead of rmse() &quot;&quot;&quot; y_true, y_pred = np.array(y_true), np.array(y_pred) return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 def residcheck(residuals, lags): &quot;&quot;&quot; Function to check if the residuals are white noise. Ideally the residuals should be uncorrelated, zero mean, constant variance and normally distributed. First two are must, while last two are good to have. If the first two are not met, we have not fully captured the information from the data for prediction. Consider different model and/or add exogenous variable. If Ljung Box test shows p&gt; 0.05, the residuals as a group are white noise. Some lags might still be significant. Lags should be min(2*seasonal_period, T/5) plots from: https://tomaugspurger.github.io/modern-7-timeseries.html &quot;&quot;&quot; resid_mean = np.mean(residuals) lj_p_val = np.mean(ljung(x=residuals, lags=lags)[1]) norm_p_val = jb(residuals)[1] adfuller_p = adfuller(residuals)[1] fig = plt.figure(figsize=(10,8)) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2); acf_ax = plt.subplot2grid(layout, (1, 0)); kde_ax = plt.subplot2grid(layout, (1, 1)); residuals.plot(ax=ts_ax) plot_acf(residuals, lags=lags, ax=acf_ax); sns.kdeplot(residuals); #[ax.set_xlim(1.5) for ax in [acf_ax, kde_ax]] sns.despine() plt.tight_layout(); print(&quot;** Mean of the residuals: &quot;, np.around(resid_mean,2)) print(&quot; n** Ljung Box Test, p-value:&quot;, np.around(lj_p_val,3), &quot;(&gt;0.05, Uncorrelated)&quot; if (lj_p_val &gt; 0.05) else &quot;(&lt;0.05, Correlated)&quot;) print(&quot; n** Jarque Bera Normality Test, p_value:&quot;, np.around(norm_p_val,3), &quot;(&gt;0.05, Normal)&quot; if (norm_p_val&gt;0.05) else &quot;(&lt;0.05, Not-normal)&quot;) print(&quot; n** AD Fuller, p_value:&quot;, np.around(adfuller_p,3), &quot;(&gt;0.05, Non-stationary)&quot; if (adfuller_p &gt; 0.05) else &quot;(&lt;0.05, Stationary)&quot;) return ts_ax, acf_ax, kde_ax def accuracy(y1,y2): accuracy_df=pd.DataFrame() rms_error = np.round(rmse(y1, y2),1) map_error = np.round(np.mean(np.abs((np.array(y1) - np.array(y2)) / np.array(y1))) * 100,1) accuracy_df=accuracy_df.append({&quot;RMSE&quot;:rms_error, &quot;%MAPE&quot;: map_error}, ignore_index=True) return accuracy_df def plot_pgram(series,diff_order): &quot;&quot;&quot; This function plots thd Power Spectral Density of a de-trended series. PSD should also be calculated for a de-trended time series. Enter the order of differencing needed Output is a plot with PSD on Y and Time period on X axis Series: Pandas time series or np array differencing_order: int. Typically 1 &quot;&quot;&quot; #from scipy import signal de_trended = series.diff(diff_order).dropna() f, fx = signal.periodogram(de_trended) freq=f.reshape(len(f),1) #reshape the array to a column psd = fx.reshape(len(f),1) # plt.figure(figsize=(5, 4) plt.plot(1/freq, psd ) plt.title(&quot;Periodogram&quot;) plt.xlabel(&quot;Time Period&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.tight_layout() . . Importing Data . path = &#39;https://raw.githubusercontent.com/pawarbi/datasets/master/timeseries/ts_frenchretail.csv&#39; #Sales numbers are in thousands, so I am dividing by 1000 to make it easier to work with numbers, especially squared errors data = pd.read_csv(path, parse_dates=True, index_col=&quot;Date&quot;).div(1_000) data.index.freq=&#39;Q&#39; data.head() . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . . Note: I have explicitly set the index frequency to quarterly. This makes plotting and analyzing data with pandas and statsmodels easier. Many methods in Statsmodels have freq argument. Setting the frequency explicitly will pass the value automatically. More date offsets can be found in Pandas documentation here. freq=&amp;#8217;Q-DEC&amp;#8217; in the index.freq below shows quarterly data ending in December. Other advantage of setting the .freq value is that if the dates are not continous, Pandas will throw an error, which can be used to fix the data quality error and make the series continuos. Other common date offsets are: - Monthly Start: &#39;MS&#39; . Quarterly Start: &#39;QS&#39; | Weekly: &#39;W&#39; | Bi Weekly: &#39;2W&#39; | Business/ Weekday: &#39;B&#39; | Hourly: &#39;H&#39; | . data.index . DatetimeIndex([&#39;2012-03-31&#39;, &#39;2012-06-30&#39;, &#39;2012-09-30&#39;, &#39;2012-12-31&#39;, &#39;2013-03-31&#39;, &#39;2013-06-30&#39;, &#39;2013-09-30&#39;, &#39;2013-12-31&#39;, &#39;2014-03-31&#39;, &#39;2014-06-30&#39;, &#39;2014-09-30&#39;, &#39;2014-12-31&#39;, &#39;2015-03-31&#39;, &#39;2015-06-30&#39;, &#39;2015-09-30&#39;, &#39;2015-12-31&#39;, &#39;2016-03-31&#39;, &#39;2016-06-30&#39;, &#39;2016-09-30&#39;, &#39;2016-12-31&#39;, &#39;2017-03-31&#39;, &#39;2017-06-30&#39;, &#39;2017-09-30&#39;, &#39;2017-12-31&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, freq=&#39;Q-DEC&#39;) . Train Test Split: . Before analyzing the data, first split it into train and test (hold-out) for model evaluation. All the EDA and model fitting/selection should be done first using train data. Never look at the test sample until later to avoid any bias. Typically we want at least 3-4 full seasonal cycles for training, and test set length should be no less than the forecast horizon. . In this example, we have 24 observations of the quarterly data, which means 6 full cycles (24/4). Our forecast horizon is 4 quarters. So training set should be more than 16 and less than 20. I will use first 18 observations for training and keep last 6 for validation.Note that I am always selecting the last 6 values for test by using .iloc[:-6]. As we get more data, this will ensure that last 6 values are always for validation. Unlike typical train/test split, we can not shuffle the data before splitting to retain the temporal structure. . Cross-validation: . Data can be split using the above method or using cross-validation where the series is split into number of successive segments and the model is tested using one-step ahead forecast. Model accuracy in that case is based on the mean of the cross-validation errors over the number of splits used. This minimizes chances of overfitting. Be sure to include at least 1-2 seasonal periods to capture the seasonality. e.g. in this case, the first training set of the CV should be min 8 values so the model has captured seasonal behaviour from 2 years. This is the preferred method when the time series is short. . Our series has 24 obervations so I can use last 6-8 for validation. When the typical train/test split is used, always check the sensisitivity of the model performance and model parameters to train/test size. If AIC or AICc is used for model evaluation, it approximatley approaches cross-validation error asymptotically. I will cover this in Part 2 with the code and example. . . #Split into train and test train = data.iloc[:-6] test = data.iloc[-6:] #forecast horizon h = 6 train_length = len(train) print(&#39;train_length:&#39;,train_length, &#39; n test_length:&#39;, len(test) ) . train_length: 18 test_length: 6 . train.head() . Sales . Date . 2012-03-31 362.0 | . 2012-06-30 385.0 | . 2012-09-30 432.0 | . 2012-12-31 341.0 | . 2013-03-31 382.0 | . Exploratory Data Analysis &amp; Modeling Implications . These are some of the questions I ask at various stages of model building. . Are there any null values? how many? best way to impute the null data? If null/NaNs are present, first identify why the data is missing and if NaNs mean anything. Missing values can be filled by interpolation, forward-fill or backward-fill depending on the data and context. Also make sure null doesnt mean 0, which is acceptable but has modeling implications. | It&#39;s important to understand how the data was generated (manual entry, ERP system), any transformations, assumptions were made before providing the data. | . | Are the data/dates continuous? In this exmaple I am only looking at continous time-series. There other methods that deal with non-continuous data. ETS &amp; ARIMA require the data to be continuous. If the series is not continuous, we can add dummy data or use interpolation. | . | Are there any duplicate dates, data? Remove the duplicates or aggregate the data (e.g. average or mean) to treat duplicates | . | Any &#39;potential&#39; outliers? . Outliers are defined as observations that differ significantly from the general observations. Identify if the data is susceptible to outliers/spikes, if outliers mean anything and how to define outliers. While &#39;Outlier Detection&#39; is a topic in itself, in forecasting context we want to treat outliers before the data is used for fitting the model. Both ETS and ARIMA class of models (especially ARIMA) are not robust to outliers and can provide erroneous forecasts. Data should be analyzed while keeping seasonality in mind. e.g. a sudden spike could be because of the seasonal behaviour and not be outlier. Do not confuse outlier with &#39;influential data&#39;. . | Few ways to treat outliers: . Winsorization: Use Box and whiskers and clip the values tha exceed 1 &amp; 99th percentile (not preferred) | Use residual standard deviation and compare against observed values (preferred but can&#39;t do a priori) | Use moving average to check spikes/troughs (iterative and not robust) | . | Another important reason to pay close attention to outliers is that we will choose the appropriate error metric based on that. There are many error metrics used to assess accuracy of forecasts, viz. MAE, MSE, RMSE, %MAPE, %sMAPE. If outliers are present, don&#39;t use RMSE because the squaring the error at the outlier value can inflate the RMSE. In that case model should be selected/assessed using %MAPE or %sMAPE. More on that in part 2. . | . | Visually any trend, seasonality, cyclic behaviour? This will help us choose the appropriate model (Single, Double, Triple Exponential Smoothing, ARIMA/SARIMA) | If cyclic behiour is present (seasonality is short-order variation e.g. month/quarter, cyclicity occurs over 10-20 years e.g. recession) we will need to use different type of decomposition (X11, STL). Depending on the context and purpose of analysis, seasoanlity adjustment may also be needed. | If multiple seasonalities are present, ETS or ARIMA cannot be used. SSA, TBATS, harmonic regression are more appropriate in that case. FB Prophet can also help with multiple seasonalities. | Frequency of seasonality is important. ETS &amp; SARIMAX are not appropriate for high frequency data such as hourly, daily, sub-daily and even weekly. Consider using SSA,TBTAS, FB Prophet, deep learning models. | . | How does the data change from season to season for each period and period to period and compared to the level? Does it increas/decrease with the trend? Changes slowly, rapidly or remains constant. This is an important observation to be made, especially for ETS model, as it can determine the parametrs to be used &amp; if any preprocessing will be needed. | De-compose the series into level, trend, seasonal components and residual error. Observe the patterns in the decomposed series. | Is the trend constant, growing/slowing linearly or exponentially or some other non-linear function? | Is the seasonal pattern repetitive? | How is the seasonal pattern changing relative to level? If it is constant relative to level, it shows &quot;additive&quot; seasonality, whereas if it is growing, it&#39;s &quot;multiplicative&quot;.(Part 2 covers this in detail) | . | Distribution of the data? will we need any transformations? While normally distributed data is not a requirement for forecasting and doesnt necessarily improve point forecast accuracy, it can help stablize the variance and narrow the prediction interval. | Plot the histogram/KDE for each time period (e.g. each year and each seasona) to get gauge peakedness, spread in the data. It can also help compare different periods and track trends over time. | If the data is severely skewed, consider normalizing the data before training the model. Be sure to apply inverse transformation on the forecasts. Use the same transformation parameters on the train and test sets. Stabilizing the variance by using Box Cox transformation (special case being log &amp; inverse transform), power law etc can help more than normalizing the data. | Watch out for outliers before transformation as it will affect the transformation | Plottng distribution also helps track &quot;concept-drift&quot; in the data, i.e. does the underlying temporal structure / assumption change over time. If the drift is significant, refit the model or at least re-evaluate. This can be tricky in time series analysis. | Uncertainty in the training data will lead to higher uncertainty in the forecast. If the data is highly volatile/uncertain (seen by spread in the distribution, standard deviation, non-constant variance etc), ETS and ARIMA models will not be suitable. Consider GARCH and other methods. | . | Is the data stationary? Is this a white noise, random walk process? . Perhaps the most important concept to keep in mind when doing time series analysis and forecasting is that, time series is a probabilistic / stochastic process, and the time series we are analyzing is a &#39;realization of a stochastic process&#39;. A time signal could be deterministic or stochastic/probabilistic. In a deterministic process, the future values can be predicted exactly with a mathematical function e.g. y = sin(2$ pi$ft). In our case, the future values can only be expressed in terms of probability distribution. The point estimates are mean/median of the distribution. By definition, the mean has a distribution around it and as such the stakeholders should be made aware of the probabilistic nature of the forecast through uncertainty estimates. . | Stationarity: Statistical stationarity means the time series has constant mean, variance and autocorrelation is insignificant at all lags. Autocorrelation is a mouthful, all it means is the correlation with its past self. e.g. to check if two variables are linearly correlated with each other, we calculate their coeff of correlation (Pearson correlation). Similarly, autocorrelation does the same thing but with its past values (i.e lags). More on that later. For a stationary time series, the properties are the same no matter which part of the series (w.r.t time) we look at. This is a core concept of the ARIMA methods, as only stationary processes can be modeled using ARIMA. ETS can handle non-stationary processes. . | White Noise: If a time series has zero mean and a constant variance , i.e. N(0,$ sigma^2$), it&#39;s a white noise. The variables in this case are independent and identically distributed (i.i.d) and are uncorrelated. We want the residuals left after fitting the model to be a white noise. White noise can be identified by using ADFuller test and plotting autocorrelation function (ACF) plots. In an ACF plot, the autocorrelation should be insignificant (inside the 95% CI band) at all lags. | Random Walk: Random walks are non-stationary. Its mean or variance or both change over time. Random walk cannot be forecast because we have more unknowns than the data so we will end up having way too many parameters in the model. In essence, random walk has no pattern to it, it&#39;s last data point plus some random signal (drift). Thus, if the first difference of the time series results in a white noise, it&#39;s an indication of a Random Walk. e.g. most equity stocks are random walk but by looking at percent difference (%growth over time) we can study the white noise. `Next Data point = Previous Data Point + Random Noise` `Random Noise = Next Data Point - Previous Data Point` . | . | . . Note: It&#8217;s easy to mistake randomness for seasonality. In the random walk chart below, it can appear that the data has some seasonality but does not! . #collapse-hide #create white noise with N(0,1.5), 500 points np.random.seed(578) steps = np.random.normal(0,1,500) noise = pd.DataFrame({&quot;x&quot;:steps}) wnoise_chart = alt.Chart(noise.reset_index()).mark_line().encode( x=&#39;index&#39;, y=&#39;x&#39;).properties( title=&quot;White Noise&quot;) #Create random walk with N(0,1.5), 500 points steps[0]=0 rwalk = pd.DataFrame({&quot;x&quot;:100 + np.cumsum(steps)}).reset_index() rwalk_chart = alt.Chart(rwalk).mark_line().encode( x=&#39;index&#39;, y=alt.Y(&#39;x&#39;, scale=alt.Scale(domain=(80,150)))).properties( title=&quot;Random Walk&quot;) wnoise_chart | rwalk_chart . . Auto-correlation? at what lag? Study the second order properties (autocorrelation and power spectral density) of the time series along with mean, standard deviation, distribution. More details below. | . | If trend is present, momentum or mean-reversing? . Time series with momentum indicates the value tends to keep going up or down (relative to trend) depending on the immediate past. Series with mean-reversion indicates it will go up (or down) if it has gone down (or up) in the immediate past. This can be found by examining the coefficients of the ARIMA model. This provides more insight into the process and builds intuition. This doesnt not directly help with forecasting. | . | Break-points in the series? . Are there any structural breaks (shifts) in the series. Structural breaks are abrupt changes in the trend. Gather more information about the sudden changes. If the breaks are valid, ETS/ARIMA models wont work. FB Prophet, dynamic regression, deep learning models, adding more features might help. Identify the possible reasons for change, e.g. change in macros, price change, change in customer preferenaces etc. Note structural change persists for some time, while outliers do not. Break points are different from non-stationarity. Read more here for examples &amp; explanations. In case of structural break-points, consider modeling the segments of the series separately. | . | Intermittent demand? Time series is said to be intermittent when there are several 0 and small values (not nulls) in the series. ETS and ARIMA are not appropriate for this type of time series. It&#39;s a common pattern with inventory time series, especially for new items. Croston&#39;s method is one approach to use for forecasting intermittent demand. | When demand is intermittent, use RMSE rather than %MAPE as the evaluation metric. With %MAPE, the denominator would be 0 leading to erroneous results. | . | #collapse-hide #creating intermittent demand plot demand = [10, 12, 0, 3,50,0,0,18,0,4, 12,0,0,8,0,3] demanddf = pd.DataFrame({&#39;y&#39;: demand, &#39;x&#39;: np.arange(2000, 2016) } ) alt.Chart(demanddf).mark_bar().encode( x=&#39;x&#39;, y=&#39;y&#39;).properties( title=&quot;Example: Intermittent Demand&quot;, width = 700) . . Do we need any exogenous variables/external regressors? It may be necessary to include additional features/variables to accurately capture the time series behaviour. For example, the sales for a retailer might be higher on weekends, holidays, when it&#39;s warmer etc. This is different from seasonal pattern. In such cases, using the &#39;day of the week&#39; or &#39;is_holiday&#39; feature might provide better forecast. ETS models cannot use exogenous variable. SARIMAX (X is for exogenous), deep learning, XGB models are more suited. | Always inspect the residuals after fitting the model. If the residuals are correlated (use ACF/PACF plots, Ljung Box test on residuals), it&#39;s an indication that we are not capturing the time series behaviour accurately and could try adding exogenous behaviour. | . | Are the stakeholders interested in forecast for invidiuals periods or hierarchical forecast? Typically forecasts are made for individual periods, e.g in this example, we are interested in the forecasts for next 4 quarters. But it&#39;s possible that the business leaders might be more interested in the 1 year forecast rather than 4 quarters. We could combine the forecasts from 4 quarters to calculate the forecast for 1 year, but that would be incorrect. As mentioned above, time series is a statistical process with probability distribution. We can get reasonable value by summing the mean forecasts but the uncertainty around those forecasts cannot be added. Also, if the individual forecasts are based on the median (rather than mean), forecasts cannot be added. We will need to calculate &quot;hierarchical forecast* by simulating the future paths and then adding the distributions to get prediction interval. | . | Are forecast explainability &amp; interpretability important? Many traditional and statistical forecasting methods such as ETS, SARIMA are easy to apply, interprete and model parameters/results can help explain the time series behaviour. This can be important in scenarios where such insights can help make business decisions. e.g if an ETS model with damped trend fits better, it can be an indication of slowing growth etc. However, many other models such deep learning, RNN, S2S, LSTM etc are blackbox approaches that may lead to higher accuracy but provide little-to-no explainability. | . | EDA in Pyhton . Data Integrity / Quality . #Any missing data? print(&quot;missing_data:&quot;, train.isna().sum()) print(&quot;unique dates:&quot;, train.index.nunique()) . missing_data: Sales 0 dtype: int64 unique dates: 18 . #Counting number of values for each quarter and Year. Columsn are quarters. #Here each qquarter and year has 1 value, thus no duplicates pd.crosstab(index=train.index.year, columns=train.index.quarter) . col_0 1 2 3 4 . row_0 . 2012 1 | 1 | 1 | 1 | . 2013 1 | 1 | 1 | 1 | . 2014 1 | 1 | 1 | 1 | . 2015 1 | 1 | 1 | 1 | . 2016 1 | 1 | 0 | 0 | . Observations: . No null values | Length of the train set is 18 and we have 12 unique dates/quarters so no duplicate dates | Each year and quarter has 1 observation, so no duplicates and data is continuous | Time Series . Plotting the time series and the 4 quarter rolling mean using Altair. . . Tip: Matplotlib and Seaborn create static charts, whereas plots created with Altair are interactive. You can hover over the data points to read tooltips. The most useful feature is the ability to zoom-in and out. Time series data can be dense and it&#8217;s important to check each time period to get insights. With zoom-in/out, it can be done iteractively without slicing the time series. Altair&#8217;s documentation and example library is great. . #collapse-hide #Create line chart for Training data. index is reset to use Date column train_chart=alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;Date&#39;, y=&#39;Sales&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]) #Create Rolling mean. This centered rolling mean rolling_mean = alt.Chart(train.reset_index()).mark_trail( color=&#39;orange&#39;, size=1 ).transform_window( rolling_mean=&#39;mean(Sales)&#39;, frame=[-4,4] ).encode( x=&#39;Date:T&#39;, y=&#39;rolling_mean:Q&#39;, size=&#39;Sales&#39; ) #Add data labels text = train_chart.mark_text( align=&#39;left&#39;, baseline=&#39;top&#39;, dx=5 # Moves text to right so it doesn&#39;t appear on top of the bar ).encode( text=&#39;Sales:Q&#39; ) #Add zoom-in/out scales = alt.selection_interval(bind=&#39;scales&#39;) #Combine everything (train_chart + rolling_mean +text).properties( width=600, title=&quot;French Retail Sales &amp; 4Q Rolling mean ( in &#39;000)&quot;).add_selection( scales ) . . Sub-series plot . Sub-series plot to show how the series behaves each year in all seasons (quarterly or monthly) . #collapse-hide alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;quarter(Date)&#39;, y=&#39;Sales&#39;, column=&#39;year(Date)&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]).properties( title=&quot;Sales: Yearly Subseries plot&quot;, width=100).configure_header( titleColor=&#39;black&#39;, titleFontSize=14, labelColor=&#39;blue&#39;, labelFontSize=14 ) . . #box plot to see distribution of sales in each year fig, ax = plt.subplots(figsize = (12,8)) sns.boxplot(data=train, x=train.index.year, y = &#39;Sales&#39;, ax = ax, boxprops=dict(alpha=.3)); sns.swarmplot(data=train, x=train.index.year, y = &#39;Sales&#39;); . #%Growth each year. Excluding 2016 since we have only 2 quarters growth = train[:&#39;2015&#39;].groupby(train[:&#39;2015&#39;].index.year)[&quot;Sales&quot;].sum().pct_change() growth*100 . Date 2012 NaN 2013 10.263158 2014 21.837709 2015 15.768854 Name: Sales, dtype: float64 . Observations: . Sales has gone up each year from 2012-2015 =&gt;Positive Trend present. | Typically, Sales goes up from Q1 to Q3, peaks in Q3, drops in Q4. Definitely a seasoanl pattern. =&gt; Model should capture seasonality and trend. | Just comparing Q4 peaks, sales has gone up from $432K to $582K =&gt; Trend exists, Model should capture trend. No cyclic behaviour | Overall data looks clean, no observations outside of IQR =&gt; Clean data, no outliers | No structural breaks, intermittent pattern =&gt; ETS and SARIMA may be used | Notice that the length of the bar in box plot increases from 2012-2015. =&gt; Mean &amp; variance increasing, we will need to stabilize the variance by taking log or using Box Cox transform | Quarterly trends &amp; distrbution . Quarterly sub-series plot to see how the series behaves in each quarter across alll years. . #collapse-hide alt.Chart(train.reset_index()).mark_line(point=True).encode( x=&#39;year(Date)&#39;, y=&#39;Sales&#39;, column=&#39;quarter(Date)&#39;, tooltip=[&#39;Date&#39;, &#39;Sales&#39;]).properties( title=&quot;Sales: Quarterly Subseries plot&quot;, width=100).configure_header( titleColor=&#39;black&#39;, titleFontSize=14, labelColor=&#39;blue&#39;, labelFontSize=14 ) . . . Tip: Statsmodels has a quarter_plot() method that can be used to create similar chart easily. . #Quarterly plot: Shows trend for Q1-Q4 for each of the years. Red line shows mean quarter_plot(train); . Distribution of Sales in each year . #collapse-hide #Distribution plot of each year compared with overall distribution sns.distplot(train, label=&#39;Train&#39;, hist=False, kde_kws={&quot;color&quot;: &quot;g&quot;, &quot;lw&quot;: 3, &quot;label&quot;: &quot;Train&quot;,&quot;shade&quot;:True}) sns.distplot(train[&#39;2012&#39;], label=&#39;2012&#39;, hist=False) sns.distplot(train[&#39;2013&#39;], label=&#39;2013&#39;, hist=False) sns.distplot(train[&#39;2014&#39;], label=&#39;2014&#39;, hist=False) sns.distplot(train[&#39;2015&#39;], label=&#39;2015&#39;, hist=False); . . In this case the heatmap feels redundant but when the series is long, heatmap can reveal more patterns . #collapse-hide sns.heatmap(pd.pivot_table(data=train, index=train.index.year, columns=train.index.quarter), square=True, cmap=&#39;Blues&#39;, xticklabels=[&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;]); . . Visualizing the quarterly sales for each year as % . #collapse-hide #As stacked bar chart, in % values. stack1= alt.Chart(train[:&#39;2015&#39;].reset_index()).mark_bar().encode( x=alt.X(&#39;sum(Sales)&#39;), y=&#39;year(Date):N&#39;, color=alt.Color( &#39;quarter(Date)&#39;, scale=alt.Scale(scheme=&#39;category10&#39;)), tooltip=[&quot;Date&quot;, &quot;Sales&quot;]).properties( height=100, width = 300, title = &quot;Sum of Sales by each Quarter&quot;) stack2= alt.Chart(train[:&#39;2015&#39;].reset_index()).mark_bar().encode( x=alt.X(&#39;sum(Sales)&#39;, stack=&#39;normalize&#39;), y=&#39;year(Date):N&#39;, color=alt.Color( &#39;quarter(Date)&#39;, scale=alt.Scale(scheme=&#39;category10&#39;)), tooltip=[&quot;Date&quot;, &quot;Sales&quot;] ).properties( height=100, width = 300, title = &quot;Sum of Sales as % by each Quarter&quot;) stack1 | stack2 . . pie= train[:&#39;2015&#39;].groupby(train[:&#39;2015&#39;].index.quarter)[&quot;Sales&quot;].sum().plot.bar( title=&quot;Total Sales by Quarter 2012-2015&quot;, legend=True, label=&quot;Sales each Quarter&quot;) . Seasonality Factor . This will help us understand how much each quarter contributes relative to the average demand. Note that this should be done on a de-trended series(taking first difference) but because we don&#39;t have enough data and for a quick demonstration, I am using the series as is. . #Groupby Sales by Quarter #Only use upto 2015 because we have partial data for 2016 train_2015=train[:&#39;2015&#39;] avg_2015= np.int(train[:&#39;2015&#39;].mean()) #Avg sales per quarter qrt_avg=train_2015.groupby(train_2015.index.quarter)[&quot;Sales&quot;].mean() #Groupby quarter qrt_table = pd.pivot_table(train_2015, index=train_2015.index.quarter, columns=train_2015.index.year) #add qrt_avg to qrt_table qrt_table[&quot;avg&quot;] = qrt_avg #Additive Seasonality Factor: Subtract mean from avg column qrt_table[&quot;additive&quot;] = qrt_table[&quot;avg&quot;]-avg_2015 #Multiplicative Seasonality Factor: Subtract mean from avg column qrt_table[&quot;multiplicative&quot;] = (qrt_table[&quot;avg&quot;]/avg_2015).round(2) qrt_table.index.name=&quot;Quarters&quot; qrt_table . Sales avg additive multiplicative . Date 2012 2013 2014 2015 . Quarters . 1 362.0 | 382.0 | 473.0 | 544.0 | 440.25 | -34.75 | 0.93 | . 2 385.0 | 409.0 | 513.0 | 582.0 | 472.25 | -2.75 | 0.99 | . 3 432.0 | 498.0 | 582.0 | 681.0 | 548.25 | 73.25 | 1.15 | . 4 341.0 | 387.0 | 474.0 | 557.0 | 439.75 | -35.25 | 0.93 | . Observations: . Quarter plot &amp; heatmap confirm peak in Q3, drop in Q4. | For each of the years the upward trend observed in all quarters | Kenel Density plot shows data looks normally distributed, bi-modal distribution in quarters is because of small sample size. Peaks shift right from 2012 to 2015 indicating increase in average. | Distribution becomes fatter as the years progress, indicating higher spread/variation (as seen above in boxplot too) | Though The sales peak in Q3 each year, as a % of annual sales, all quarters contribute roughly the same | Seasonal factor analysis shows that in 3rd quarter we see that sales jump up by 15% (or $73K) relative to average while in other three quarters it drops by 1-7%. This is great for intuitive understanding of series behaviour. Another key takeaway from this analysis is that sales is not stable, as is evident from the charts above, multiplicative seasonality would capture the pattern better than additive seasonality. This insight will come handy when we create HW/ETS model (part 2). We could also reduce the variance by taking the log so errors are additive. | Decomposition . We will de-compose the time series into trend, seasonal and residuals . . Tip: Always use a semicolon (;) after plotting any results from statsmodels. For some reason if you don&#8217;t, it will print the plots twice. Also, by default the statsmodels plots are small and do not have a figsize() argument. Use rcParams() to define the plot size . decompose = seasonal_decompose(train[&quot;Sales&quot;]) decompose.plot(); plt.rcParams[&#39;figure.figsize&#39;] = (12, 8); . Observations: . Trend is more than linear, notice a small upward take off after 2013-07. Also notice that trend is projecting upward . | Seasonal pattern is consistent | Resduals are whetever is left after fitting the trend and seasonal components to the observed data. It&#39;s the component we cannot explain. We want the residuals to be i.i.d (i.e uncorrelated). If the residuals have a pattern, it means there is still some structural information left to be captured. Residuals are showing some wavy pattern, which is not good. Let&#39;s perform Ljung Box test to confirm if they are i.i.d as a group. | We do not want to see any recognizable patterns in the residuals, e.g. waves, upward/downward slope, funnel pattern etc. | ljung_p = np.mean(ljung(x=decompose.resid.dropna())[1]).round(3) print(&quot;Ljung Box, p value:&quot;, ljung_p, &quot;, Residuals are uncorrelated&quot; if ljung_p&gt;0.05 else &quot;, Residuals are correlated&quot;) . Ljung Box, p value: 0.184 , Residuals are uncorrelated . Residuals are uncorrelated. If the residuals are correlated, we can perform transformations to see if it stabilizes the variance. It&#39;s also an indication that we may need to use exogenous variable to fully explain the time series behaviour or use higher order models. In this case, the residuals are uncorrelated so that&#39;s good. . . Note: Ljung Box test tests the residuals as a group. Some residuals may have significant lag but as a group, we want to make sure they are uncorrelated. . Second Order Properties of the time series . We study the second order properties to understand - . is the data stationary | is the data white noise, random walk? i.e are the lags correlated? | quantify seasonal/cyclic behviour | . Stationarity: . For the series to be stationary, it must have: . constant mean | constant variance | constant covariance (uncorrelated) | . We verify this by observing change in mean, variance, autocorrelation and with a statistical test (ADFuller test) . Is the mean constant? . train.plot(figsize=(12,6), legend=True, label=&quot;Train&quot;, cmap=&#39;gray&#39;) train[&quot;Sales&quot;].rolling(4, center=False).mean().plot(legend=True, label=&quot;Rolling Mean 4Q&quot;); print(&quot;Mean is:&quot;, train[&quot;Sales&quot;].mean()) . Mean is: 496.5 . Notice that each year, the difference between the mean and the max in Q3 increases. This can potentially mean multiplicative seasonality. . Is the variance constant? . train[&quot;Sales&quot;].rolling(4).std().plot(legend=True, label=&quot;Rolling Std Deviation 4Q&quot;); print(&quot;S.D is:&quot;, train[&quot;Sales&quot;].std().round(1)) . S.D is: 110.9 . Both mean and standard deviation are increasing, thus not stationary. . Coefficient of Variation: . Coefficient of variation gives us an idea about the variability in the process, especially when looking at sales and demand. Note that this should be used for relative comparison and does not have a strict statistical defition. It&#39;s very common measure in demand planning and inventory analytics. . c.v = s.d/mean . If C.V&lt;0.75 =&gt; Low Variability . If 0.75&lt;C.V&lt;1.3 =&gt; Medium Variability . If C.V&gt;1.3 =&gt; High Variability . cv = train[&quot;Sales&quot;].std()/train[&quot;Sales&quot;].mean() cv.round(2) . 0.22 . This is a low-variability process. . Is the covariance constant? . #Plot ACF and PACF using statsmodels plot_acf(train); plot_pacf(train); . ADFuller Test for stationarity . Augmented Dicky Fuller test is a statistical test for stionarity. If the p value is less than 0.05, the series is stationary, otherwise non-stationary. Use adfuller() from statsmodels . #Calculate ad fuller statistic adf = adfuller(train[&quot;Sales&quot;])[1] print(f&quot;p value:{adf.round(4)}&quot;, &quot;, Series is Stationary&quot; if adf &lt;0.05 else &quot;, Series is Non-Stationary&quot;) . p value:0.999 , Series is Non-Stationary . Observations: . ACF: ACF plot shows autocorrelation coeff is insignificant at all lag values (within the blue 95%CI band), except lag 1. When studying ACF plots, we look at these 4 things Are any lags significant, i.e outside the blue band of 95% CI. If they are, the series is correlated with itself at those lags. Note there is 5% chance that the lag shown as insignificant (ACF=0) is shows as significant. In our case, 1st lag is barely significant, indicating sales last quarter affect the sales this quarter. | How quickly do the bar lenghts change: If the bars are taping down, that shows presence of trend. Our series has a trend | Pattern: If the ACF shows up/down repeating pattern, it means seasonality with size equal to length of repetition. | Sign of ACF: Alternating signs in ACF shows mean-reversing process whereas if all the ACs are positive (or negative), it shows momentum process. | . | Properties of ACF help us determine the order of the MA process. More on that in part 2. . PACF: Partial autocorrelation, similar to partial correlation, shows correlation after &#39;partialing out&#39; previous lags. If a series has PACF significant at lag k, it means controlling for other lags &lt;k, lag k has a significant correlation. PACF plot is used to determine order of AR process. . | ADFuller test shows that the series is not stationary. We can try to make it stationary by differencing it. . | #De-trending de_trended = train.diff(1).dropna() adf2 = adfuller(de_trended)[1] print(f&quot;p value:{adf2}&quot;, &quot;, Series is Stationary&quot; if adf2 &lt;0.05 else &quot;, Series is Non-Stationary&quot;) de_trended.plot(); . p value:1.544700848240804e-18 , Series is Stationary . By taking the first difference we de-trended the series and it has become stationary! . Autocovariance function vs autocorrelation fucntion The autocovariance measures the linear dependence between two points on the same series observed at different times. Very smooth series exhibit autocovariance functions that stay large even when the t and s are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations. autocorrelation functions (ACF) measures the predictability (linear), and is the normalized autocovariance. ACF, just like a correlation coeff, is between [-1,1] and is easier to interprete. Both measure linear dependence between random variables. . For example, the scatterplot below shows the train[&quot;Sales&quot;] plotted against it&#39;s first lag. We can see a linear, although weak, relationship between them. Use pandas.plotting.lag_plot() . Lag plot, 1st lag . pd.plotting.lag_plot(train[&quot;Sales&quot;],1); . Lag plot, 2nd lag : Weak relationship . pd.plotting.lag_plot(train[&quot;Sales&quot;],2); . . Note: If you wish to study the lags, you can obtain it by .shift() method. . sns.scatterplot(train[&quot;Sales&quot;], train[&quot;Sales&quot;].shift(-1)); . . Important: Use statsmodels for calculating ACF and NOT pandas pd.Series.autocorr() . Statsmodels and R use mean differencing, i.e subtract the mean of the entire series before summing it up, whereas Pandas uses Pearson correlation to calculate the ACF. Pearson correlation uses mean of the subseries rather than the entire series. For a long time series, the difference between the two should be negligible but for a short series, the diffrenece could be significant. In most cases, we are more interested in the pattern in the ACF than the actual values so, in a practical sense either would work. But, to be consistent and accurate use statsmodels to calculate and plot the ACF. . Which frequencies are prominent? . We typically look at the time series in the time domain. But, we can also analyze the time series in the frequency domain. It&#39;s based on the assumption that it is made up of sine and cosine waves of different frequencies. This helps us detect periodic component of known/unknown frequencies. It can show additional details of the time series that can be easily missed. We do it with a Periodogram and Power Spectral Density plot. . Periodogram: We analyze frequency and associated intensity of frequency. Note that below I have invrted the frequency to obtain periods Period,T = 1/frequency. For example, a monthly time series has 12 seasonal periods, so we would obtain frequency = 1/12 = 0.0833. In our example, we expect to see the intensity to be high at period=4 . . Tip: Periodogram should be plotted for a de-trended time series. Time series can be obtained by differencing the series . plot_pgram(train[&quot;Sales&quot;],1); . Power Spectral Density: Periodogram assumes the frequencies to be harmonics of the fundamental frequency, whereas PSD allows the frequency to vary contunuously. PSD is calculated using autocovariance function (ACF seen above). Spectral density is the amount of variance per frequency interval. PSD shows the eaxct same information of the time series as the ACF, just in the frequency domain. We rarely use PSD for business time series analysis. Plot below shows that lower frequency content dominates the time series. If it had another bump at higher frequency, that would indicate cyclic behaviour. Sometime it can be easier to figure out MA vs AR process by looking at the PSD plot. . #Plot PSD plt.psd(train[&quot;Sales&quot;], detrend=&#39;linear&#39;); plt.title(&quot;PSD Plot&quot;); . Is the series Normal? . As mentioned above, series does not have to be Gaussian for accurate forecasting but if the data is highly skewed it can affect the model selection and forecast uncertainty. In general, if the series is non-gaussian, it should be normalized before any further transformations (differencing, log, Box Cox) at least to check if normalization helps. Normalization will also help if decide to use regression, tree-based models, NN models later. Note that with normalization, we make the z score between [0,1], with standardization on the other hand , we center the distribution with mean =0, s.d.=1. . Normality can be checked visually by plotting the density plot, q-q plot and Shapiro-Wilk test. . #Distribution Plot sns.distplot(train[&quot;Sales&quot;]); . #Q-Q Plot sm.qqplot(train[&quot;Sales&quot;], fit=True, line=&#39;45&#39;, alpha=0.5, dist=&#39;norm&#39; ); . #Jarque Bera Stastical Test for Normality from scipy.stats import jarque_bera as jb is_norm=jb(train[&quot;Sales&quot;])[1] print(f&quot;p value:{is_norm.round(2)}&quot;, &quot;, Series is Normal&quot; if is_norm &gt;0.05 else &quot;, Series is Non-Normal&quot;) . p value:0.59 , Series is Normal . Observations: . Q-Q plot shows the data follows the 45deg line very closely, deviates slightly in the left tail. | Jarque Bera test shows the data is from Normal distribution | Summary . Model explainability is as important as model accuracy. We will keep above insights in mind when choosing, fitting, evaluating and selecting various models. We will choose the model that we can explain based on above insights. It is important to be able to explain the time series behavour in qualitative and quantitative terms. . In Part 2, I will cover model fitting, selection and ensemble forecasting . EDA in R . Forecasting Principles and Practice by Prof. Hyndmand and Prof. Athanasapoulos is the best and most practical book on time series analysis. Most of the concepts discussed in this blog are from this book. Below is code to run the forecast() and fpp2() libraries in Python notebook using rpy2 . import rpy2 import warnings warnings.filterwarnings(&#39;ignore&#39;) from rpy2.robjects import pandas2ri import rpy2.rinterface as rinterface pandas2ri.activate() %load_ext rpy2.ipython . %%R -i data,train -o r_train library(fpp2) r_train &lt;- ts(train$Sales, start=c(2012,01), frequency=4) r_train %&gt;% autoplot() + ggtitle(&quot;French Retail&quot;) +xlab(&quot;Year-Quarter&quot;)+ylab(&quot;Retail Sales&quot;) . %Rpush r_train . %%R -i r_train r_train %&gt;% ggsubseriesplot() . Lag Plots . %%R r_train %&gt;% gglagplot() . ACF Plots . %%R r_train %&gt;% ggAcf() . R shows 3 lags to be significant, whereas in Python we saw only the first lag to be significant. I am not sure why. Just to confirm, I am did the analysis in JMP statistical software which I use at work for statistical analysis. Below are the results. It matches with Python&#39;s results - 1st lag to be significant, spectral density plot matches too. . . Outlier detection . %%R -o olier olier &lt;- r_train %&gt;% tsoutliers() . print(olier, end=&quot;&quot;) . $index integer(0) $replacements numeric(0) . using tsoutliers() does not return any results, showing there are no statistical outliers . Summary . Here is a summary of what we have learned about this time series: . There are no null values, outliers or duplicate values in the series. Series is continuous, non-intermittent. No structural breaks. We don&#39;t have to do any cleaning. | Series has a trend. Sales have increased every year. It looks more than linear but less than exponential. We might need to try lof or BoxCox transform. | Series has seasonality with seasonal periods = 4. Peak in Q3, drops in Q4, ramps up from Q1 to Q3. No other dominant periods. No cyclic behaviour. | Avg sales per quarter is 497, and S.D is 111, low variability. We will use this to guage the model error relative to mean and S.D. | Since it is not a high-frequency data and has fixed seasonality, we can use ETS and SARIMA class of models | SARIMA could be used. We will need at least 1 differencing as de-trended series was stationary | Mean, variance, covariance are not constant. Series is not stationary, not white noise and not random-walk. | Variance increasing with time. Highs also look to be increasing relative to mean (rolling avg). &#39;multiplicative&#39; seasonality might fit better in ETS model | Series is normally distributed | 1st lag was significant, though barely (ACF~0.5) | We do not have any information on exogenous variables | As there are no outliers and series is not intermittent, we can use RSME for model evaluation | We never looked at the test data. All the EDA and model building must be done using the training set. Intuitively, given what we have observed in the training set, we expect the forecast to be on the upward trend with slightly mupltiplicative seasonality. | References: . Forecasting: Principles and Practice, by Prof. Hyndman | Time Series Analysis and its Applications, by Robert Shumway | Time Series Analysis and Forecasting, by Montgomery &amp; Jennings | Introduction to Time Series and Analysis, by Brockwell | Practial Time Series Forecasting with R, by Galit Shmueli |",
            "url": "https://pawarbi.github.io/blog/forecasting/r/python/rpy2/altair/2020/04/21/timeseries-part1.html",
            "relUrl": "/forecasting/r/python/rpy2/altair/2020/04/21/timeseries-part1.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post45": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pawarbi.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "(Last Updated August 2022) . My name is Sandeep Pawar. I am a Data Analytics &amp; Data Science professional, currently working at Hitachi Solutions as Sr Power BI Architect. I use advanced data analytics, data visualization, Statistics, Machine Learning to create solutions, primarily using Microsoft data products &amp; tools. . You can watch my chat with Ben Ferry to learn about me, my work and data : . . Few things about me: . I focus on creating solutions not reports and models | I enjoy collaborating with others, learn from them and share what I know | I have a process-focussed view of analytics, i.e analytics can only be successful if it aligns with business processes (new or existing). Hence, you will often find me talk about the business processes that generate the data, decision frameworks to consume the data/reports/models, and data governance. | I love exploring and learning new technologies. Currently I am learning various frameworks and packages to explore high volume, high dimensional data effectively at scale using Python and how to integrate that with Power BI projects. | . I enjoy watching cricket and making espressos &amp; latte art. I can make a heart, tulip &amp; rose. Learning to make a swan :) (If you have a favorite coffee blend or local espresso bar, send me a note). Since you asked, my favorite beans are Epic Espresso by 49th Parallel and favorite cafe is Espresso Cielo in Santa Monica, CA. I am a die-hard Sachin Tendulkar fan. . . Volunteering: I volunteer at Elmhurst Art Museum, a local non-profit that showcases artwork of local and international artists and helps educate local community about art. I help them with their data analytics needs to improve community outreach and marketing. I am always looking for volunteering opportunities so feel free to get in touch with me if I can be of any help. . Education: . Masters in Engineering Management, Business Analytics, Northwestern University, IL | M.S, Mechanical Engineering, Illinois Institute of Technology, Chicago IL | B.E, Mechanical Engineering, Pune, India | . Certifications: . Machine Learning Engineer with Microsoft Azure - Udacity Nanodegree | Microsoft Certified Enterprise Data Analyst (DP-500) | Microsoft Certified Azure Data Scientist (DP-100) | Microsoft Certified Data Analyst (PL-300) | Azure AI Fundamentals Certification (AI-900) | JMP: Modern Screening Designs (Statistical Design of Experiments) | JMP: ANOVA and Regression | JMP: Data Exploration | . Some Bragging . Won Navigant Business Strategy Competition | Won Microsoft-Udacity Azure Machine Learning Scholarship | Won 1st Prize for my project on Time Series Forecasting in Azure ML | Finalist, AECOM Global Business Innovation &amp; Strategy Competition | 4 Patents: US D750317, US 20150345715, US 20150316249, US 20140268730. Related to thermal design of lighting fixtures | 2X Employee Recognition Award for using Data Analytics to help create business solutions | . Speaking Engagements . Introduction to Power BI for Educators, January 31st, 2020. Microsoft Technology Event, Chicago | Propelling Power BI with Python &amp; R (English), May 16th, 2020, Bangalore Power BI Days | Propelling Power BI with Python &amp; R (Hindi), May 17th, 2020, Bangalore Power BI Days | Introduction to Classical Time Series Forecasting, Aug 17th, 2020, Udacity Azure ML Group | Key Influencer Deep Dive, Sept 15th, 2020, Power BI Days Munich | Deploying Azure ML forecast Model in Power BI, Dec 08th, 2020, Washington DC Power BI User Group | Forecasting in Power BI, Dec 09th, 2020, Austin,TX Power BI User Group | Key Influencer Deep Dive, Dec 10th, 2020, Lexington,KY Power BI User Group | Time Series Forecasting in Power BI, Dec 15th, 2020, Power BI Days Hamburg | Risk Modeling in Power BI using Monte Carlo Simulation, Dec 22nd, 2020, Dublin Power BI User Group | Model Interpretability using Azure ML SDK, Jan 16 2021, Global AI Bootcamp Singapore | Time Series Forecasting in Power BI, Jan 17 2021, Global AI Bootcamp Singapore | Introduction to Azure ML SDK,Feb 20 2021, Azure Saturday Hamburg | Enterprise Risk Modeling in Power BI, Feb 27th, 2021, Scottish Summit 2021 | Operationalizing Machine Learning Models With Azure ML &amp; Power BI, Mar 22nd, 2021, Power BI Days | Enterprise Risk Modeling in Power BI, Apr 21, 2021, Global Power BI Summit 2021 | Power BI Datasets &amp; Dataflow, Apr 21, 2021, Cincinnati Data and AI User Group | Power Up Power BI with Jupyter Notebook - Use Cases, May 15, 2021, DataWeekender | Power BI AutoML Deep Dive, May 26, 2021, Portalnd Power BI user Group Portland PBUG | Power Up Power BI with Jupyter Notebook - Use Cases (Lightning Talk), June 25, 2021, Data Saturdays ANZ 2021 | Power Up Power BI with Jupyter Notebook - Use Cases, Sept 28, 2021, Hampton Roads Power BI User Group | Excel for Data Science, Oct 06, 2021, AI42 | Power BI for advanced analytics - Part 1, Oct 20, 2021, AI42 | Power BI for advanced analytics - Part 2, Nov 3, 2021, AI42 | Clustering and Segmentation In Power BI, Nov 8-12, 2021, PASS Data Community Summit | Demystifying Power BI AI Visuals, November 1 2021, Atlanta Power BI User Group | Power BI Scatterplot Deep Dive, Apr 19, 2022, Minnesota SQL Server UG | Query Folding in Power BI,May 18 2022, Austin TX Power BI User Group | Serialization To Optimize Spark Notebooks in Synapse Analytics,July 9th 2022, Data Toboggan Cool Runnings | . Upcoming Presentations: . Data Insights Summit - Chicago,Sept 13-14 2022 | Portland Power BI User Group,Sept 28 2022 | . You can connect with me on LinkedIn and Twitter at: . LinkedIn | Twitter | YouTube | Email | . If you would like to provide anonymous feedback or send a quick note to me, please use this typeform. Thanks ! .",
          "url": "https://pawarbi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

}